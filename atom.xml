<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>王小二客栈</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.wenchong.top/"/>
  <updated>2018-05-16T08:45:18.459Z</updated>
  <id>http://www.wenchong.top/</id>
  
  <author>
    <name>王彪</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python——基础数据类型</title>
    <link href="http://www.wenchong.top/2018/05/16/Python%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <id>http://www.wenchong.top/2018/05/16/Python学习（1）——基本数据类型/</id>
    <published>2018-05-16T08:22:57.304Z</published>
    <updated>2018-05-16T08:45:18.459Z</updated>
    
    <content type="html"><![CDATA[<p>** Python 中的变量不需要声明。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。<br>在 Python 中，变量就是变量，它没有类型，我们所说的”类型”是变量所指的内存中对象的类型。</p><p>等号（=）用来给变量赋值。</p><p>等号（=）运算符左边是一个变量名,等号（=）运算符右边是存储在变量中的值。例如： **</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"> </span><br><span class="line">counter = 100          # 整型变量</span><br><span class="line">miles   = 1000.0       # 浮点型变量</span><br><span class="line">name    = &quot;小二&quot;     # 字符串</span><br><span class="line"> </span><br><span class="line">print (counter)</span><br><span class="line">print (miles)</span><br><span class="line">print (name)</span><br></pre></td></tr></table></figure><p>运行实例 »<br>执行以上程序会输出如下结果：</p><p>100<br>1000.0<br>小二</p><ul><li>多个变量赋值<br>Python允许你同时为多个变量赋值。例如：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = b = c = 1</span><br></pre></td></tr></table></figure></li></ul><p>以上实例，创建一个整型对象，值为 1，三个变量都指向同一个内存位置。</p><p>您也可以为多个对象指定多个变量。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, b, c = 1, 2, &quot;小二&quot;</span><br></pre></td></tr></table></figure></p><p>以上实例，两个整型对象 1 和 2 的分配给变量 a 和 b，字符串对象 “小二” 分配给变量 c。</p><ul><li>标准数据类型<br>Python3 中有六个标准的数据类型：</li></ul><p>Number（数字）<br>String（字符串）<br>List（列表）<br>Tuple（元组）<br>Sets（集合）<br>Dictionary（字典）<br>Python3 的六个标准数据类型中：</p><p>不可变数据（四个）：Number（数字）、String（字符串）、Tuple（元组）、Sets（集合）；<br>可变数据（两个）：List（列表）、Dictionary（字典）。</p><ul><li>Number（数字）<br>Python3 支持 int、float、bool、complex（复数）。</li></ul><p>在Python 3里，只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。</p><p>像大多数语言一样，数值类型的赋值和计算都是很直观的。</p><p>内置的 type() 函数可以用来查询变量所指的对象类型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a, b, c, d = 20, 5.5, True, 4+3j</span><br><span class="line">&gt;&gt;&gt; print(type(a), type(b), type(c), type(d))</span><br><span class="line">&lt;class &apos;int&apos;&gt; &lt;class &apos;float&apos;&gt; &lt;class &apos;bool&apos;&gt; &lt;class &apos;complex&apos;&gt;</span><br></pre></td></tr></table></figure></p><p>此外还可以用 isinstance 来判断：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;a = 111</span><br><span class="line">&gt;&gt;&gt; isinstance(a, int)</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p><p>您也可以使用del语句删除一些对象引用。<br>del语句的语法是：</p><p>del var1[,var2[,var3[….,varN]]]]</p><ul><li>数值运算<br>实例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;5 + 4  # 加法</span><br><span class="line">9</span><br><span class="line">&gt;&gt;&gt; 4.3 - 2 # 减法</span><br><span class="line">2.3</span><br><span class="line">&gt;&gt;&gt; 3 * 7  # 乘法</span><br><span class="line">21</span><br><span class="line">&gt;&gt;&gt; 2 / 4  # 除法，得到一个浮点数</span><br><span class="line">0.5</span><br><span class="line">&gt;&gt;&gt; 2 // 4 # 除法，得到一个整数</span><br><span class="line">0</span><br><span class="line">&gt;&gt;&gt; 17 % 3 # 取余 </span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; 2 ** 5 # 乘方</span><br><span class="line">32</span><br></pre></td></tr></table></figure></li></ul><p>1、Python可以同时为多个变量赋值，如a, b = 1, 2。<br>2、一个变量可以通过赋值指向不同类型的对象。<br>3、数值的除法包含两个运算符：/ 返回一个浮点数，// 返回一个整数。<br>4、在混合计算时，Python会把整型转换成为浮点数。</p><ul><li>String（字符串）<br>Python中的字符串用单引号(‘)或双引号(“)括起来，同时使用反斜杠()转义特殊字符。</li></ul><p>字符串的截取的语法格式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">变量[头下标:尾下标]</span><br></pre></td></tr></table></figure></p><p>索引值以 0 为开始值，-1 为从末尾的开始位置。</p><p>加号 (+) 是字符串的连接符， 星号 (*) 表示复制当前字符串，紧跟的数字为复制的次数。实例如下：</p><p>实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"> </span><br><span class="line">str = &apos;xiaoer&apos;</span><br><span class="line"> </span><br><span class="line">print (str)          # 输出字符串</span><br><span class="line">print (str[0:-1])    # 输出第一个到倒数第二个的所有字符</span><br><span class="line">print (str[0])       # 输出字符串第一个字符</span><br><span class="line">print (str[2:5])     # 输出从第三个开始到第五个的字符</span><br><span class="line">print (str[2:])      # 输出从第三个开始的后的所有字符</span><br><span class="line">print (str * 2)      # 输出字符串两次</span><br><span class="line">print (str + &quot;TEST&quot;) # 连接字符串</span><br></pre></td></tr></table></figure></p><p>执行以上程序会输出如下结果：</p><p>xiaoer<br>xiaoe<br>x<br>aor<br>aoer<br>xiaoerxiaoer<br>xiaoerTEST</p><p>Python 使用反斜杠()转义特殊字符，如果你不想让反斜杠发生转义，可以在字符串前面添加一个 r，表示原始字符串：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; print(&apos;xi\aoer&apos;)</span><br><span class="line">xi</span><br><span class="line">aoer</span><br><span class="line">&gt;&gt;&gt; print(r&apos;xi\aoer&apos;)</span><br><span class="line">xi\aoer</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">`</span><br></pre></td></tr></table></figure></p><p>另外，反斜杠()可以作为续行符，表示下一行是上一行的延续。也可以使用 “””…””” 或者 ‘’’…’’’ 跨越多行。</p><p>注意，Python 没有单独的字符类型，一个字符就是长度为1的字符串。</p><p>实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;word = &apos;Python&apos;</span><br><span class="line">&gt;&gt;&gt; print(word[0], word[5])</span><br><span class="line">P n</span><br><span class="line">&gt;&gt;&gt; print(word[-1], word[-6])</span><br><span class="line">n P</span><br></pre></td></tr></table></figure></p><p>与 C 字符串不同的是，Python 字符串不能被改变。向一个索引位置赋值，比如word[0] = ‘m’会导致错误。</p><p>注意：</p><p>1、反斜杠可以用来转义，使用r可以让反斜杠不发生转义。<br>2、字符串可以用+运算符连接在一起，用*运算符重复。<br>3、Python中的字符串有两种索引方式，从左往右以0开始，从右往左以-1开始。<br>4、Python中的字符串不能改变。</p><ul><li>List（列表）<br>List（列表） 是 Python 中使用最频繁的数据类型。</li></ul><p>列表可以完成大多数集合类的数据结构实现。列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。</p><p>列表是写在方括号([])之间、用逗号分隔开的元素列表。</p><p>和字符串一样，列表同样可以被索引和截取，列表被截取后返回一个包含所需元素的新列表。</p><p>列表截取的语法格式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">变量[头下标:尾下标]</span><br></pre></td></tr></table></figure></p><p>索引值以 0 为开始值，-1 为从末尾的开始位置。</p><p>加号（+）是列表连接运算符，星号（*）是重复操作。如下实例：</p><p>实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"> </span><br><span class="line">list = [ &apos;abcd&apos;, 786 , 2.23, &apos;xiaoer&apos;, 70.2 ]</span><br><span class="line">tinylist = [123, &apos;xiaoer&apos;]</span><br><span class="line"> </span><br><span class="line">print (list)            # 输出完整列表</span><br><span class="line">print (list[0])         # 输出列表第一个元素</span><br><span class="line">print (list[1:3])       # 从第二个开始输出到第三个元素</span><br><span class="line">print (list[2:])        # 输出从第三个元素开始的所有元素</span><br><span class="line">print (tinylist * 2)    # 输出两次列表</span><br><span class="line">print (list + tinylist) # 连接列表</span><br></pre></td></tr></table></figure></p><p>以上实例输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[&apos;abcd&apos;, 786, 2.23, &apos;xiaoer&apos;, 70.2]</span><br><span class="line">abcd</span><br><span class="line">[786, 2.23]</span><br><span class="line">[2.23, &apos;xiaoer&apos;, 70.2]</span><br><span class="line">[123, &apos;xiaoer&apos;, 123, &apos;xiaoer&apos;]</span><br><span class="line">[&apos;abcd&apos;, 786, 2.23, &apos;xiaoer&apos;, 70.2, 123, &apos;xiaoer&apos;]</span><br></pre></td></tr></table></figure></p><p>与Python字符串不一样的是，列表中的元素是可以改变的：</p><p>实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;a = [1, 2, 3, 4, 5, 6]</span><br><span class="line">&gt;&gt;&gt; a[0] = 9</span><br><span class="line">&gt;&gt;&gt; a[2:5] = [13, 14, 15]</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">[9, 2, 13, 14, 15, 6]</span><br><span class="line">&gt;&gt;&gt; a[2:5] = []   # 将对应的元素值设置为 [] </span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">[9, 2, 6]</span><br></pre></td></tr></table></figure></p><p>List内置了有很多方法，例如append()、pop()等等。</p><p>注意：</p><p>1、List写在方括号之间，元素用逗号隔开。<br>2、和字符串一样，list可以被索引和切片。<br>3、List可以使用+操作符进行拼接。<br>4、List中的元素是可以改变的。</p><ul><li>元祖<br>元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号 () 里，元素之间用逗号隔开。</li></ul><p>元组中的元素类型也可以不相同：</p><p>实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"> </span><br><span class="line">tuple = ( &apos;abcd&apos;, 786 , 2.23, &apos;xiaoer&apos;, 70.2  )</span><br><span class="line">tinytuple = (123, &apos;xiaoer&apos;)</span><br><span class="line"> </span><br><span class="line">print (tuple)             # 输出完整元组</span><br><span class="line">print (tuple[0])          # 输出元组的第一个元素</span><br><span class="line">print (tuple[1:3])        # 输出从第二个元素开始到第三个元素</span><br><span class="line">print (tuple[2:])         # 输出从第三个元素开始的所有元素</span><br><span class="line">print (tinytuple * 2)     # 输出两次元组</span><br><span class="line">print (tuple + tinytuple) # 连接元组</span><br></pre></td></tr></table></figure></p><p>以上实例输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(&apos;abcd&apos;, 786, 2.23, &apos;xiaoer&apos;, 70.2)</span><br><span class="line">abcd</span><br><span class="line">(786, 2.23)</span><br><span class="line">(2.23, &apos;xiaoer&apos;, 70.2)</span><br><span class="line">(123, &apos;xiaoer&apos;, 123, &apos;xiaoer&apos;)</span><br><span class="line">(&apos;abcd&apos;, 786, 2.23, &apos;xiaoer&apos;, 70.2, 123, &apos;xiaoer&apos;)</span><br></pre></td></tr></table></figure></p><p>元组与字符串类似，可以被索引且下标索引从0开始，-1 为从末尾开始的位置。也可以进行截取（看上面，这里不再赘述）。</p><p>其实，可以把字符串看作一种特殊的元组。</p><p>实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;tup = (1, 2, 3, 4, 5, 6)</span><br><span class="line">&gt;&gt;&gt; print(tup[0])</span><br><span class="line">1</span><br><span class="line">&gt;&gt;&gt; print(tup[1:5])</span><br><span class="line">(2, 3, 4, 5)</span><br><span class="line">&gt;&gt;&gt; tup[0] = 11  # 修改元组元素的操作是非法的</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">TypeError: &apos;tuple&apos; object does not support item assignment</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p><p>虽然tuple的元素不可改变，但它可以包含可变的对象，比如list列表。</p><p>构造包含 0 个或 1 个元素的元组比较特殊，所以有一些额外的语法规则：</p><p>tup1 = ()    # 空元组<br>tup2 = (20,) # 一个元素，需要在元素后添加逗号<br>string、list和tuple都属于sequence（序列）。</p><p>注意：</p><p>1、与字符串一样，元组的元素不能修改。<br>2、元组也可以被索引和切片，方法一样。<br>3、注意构造包含0或1个元素的元组的特殊语法规则。<br>4、元组也可以使用+操作符进行拼接。</p><ul><li>Set（集合）<br>集合（set）是一个无序不重复元素的序列。</li></ul><p>基本功能是进行成员关系测试和删除重复元素。</p><p>可以使用大括号 { } 或者 set() 函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。</p><p>创建格式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parame = &#123;value01,value02,...&#125;</span><br></pre></td></tr></table></figure></p><p>或者<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set(value)</span><br></pre></td></tr></table></figure></p><p>实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"> </span><br><span class="line">student = &#123;&apos;Tom&apos;, &apos;Jim&apos;, &apos;Mary&apos;, &apos;Tom&apos;, &apos;Jack&apos;, &apos;Rose&apos;&#125;</span><br><span class="line"> </span><br><span class="line">print(student)   # 输出集合，重复的元素被自动去掉</span><br><span class="line"> </span><br><span class="line"># 成员测试</span><br><span class="line">if(&apos;Rose&apos; in student) :</span><br><span class="line">    print(&apos;Rose 在集合中&apos;)</span><br><span class="line">else :</span><br><span class="line">    print(&apos;Rose 不在集合中&apos;)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># set可以进行集合运算</span><br><span class="line">a = set(&apos;abracadabra&apos;)</span><br><span class="line">b = set(&apos;alacazam&apos;)</span><br><span class="line"> </span><br><span class="line">print(a)</span><br><span class="line"> </span><br><span class="line">print(a - b)     # a和b的差集</span><br><span class="line"> </span><br><span class="line">print(a | b)     # a和b的并集</span><br><span class="line"> </span><br><span class="line">print(a &amp; b)     # a和b的交集</span><br><span class="line"> </span><br><span class="line">print(a ^ b)     # a和b中不同时存在的元素</span><br></pre></td></tr></table></figure></p><p>以上实例输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;&apos;Mary&apos;, &apos;Jim&apos;, &apos;Rose&apos;, &apos;Jack&apos;, &apos;Tom&apos;&#125;</span><br><span class="line">Rose 在集合中</span><br><span class="line">&#123;&apos;b&apos;, &apos;a&apos;, &apos;c&apos;, &apos;r&apos;, &apos;d&apos;&#125;</span><br><span class="line">&#123;&apos;b&apos;, &apos;d&apos;, &apos;r&apos;&#125;</span><br><span class="line">&#123;&apos;l&apos;, &apos;r&apos;, &apos;a&apos;, &apos;c&apos;, &apos;z&apos;, &apos;m&apos;, &apos;b&apos;, &apos;d&apos;&#125;</span><br><span class="line">&#123;&apos;a&apos;, &apos;c&apos;&#125;</span><br><span class="line">&#123;&apos;l&apos;, &apos;r&apos;, &apos;z&apos;, &apos;m&apos;, &apos;b&apos;, &apos;d&apos;&#125;</span><br></pre></td></tr></table></figure></p><ul><li>Dictionary（字典）<br>字典（dictionary）是Python中另一个非常有用的内置数据类型。</li></ul><p>列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。</p><p>字典是一种映射类型，字典用”{ }”标识，它是一个无序的键(key) : 值(value)对集合。</p><p>键(key)必须使用不可变类型。</p><p>在同一个字典中，键(key)必须是唯一的。</p><p>实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"> </span><br><span class="line">dict = &#123;&#125;</span><br><span class="line">dict[&apos;one&apos;] = &quot;1 - 小二客栈&quot;</span><br><span class="line">dict[2]     = &quot;2 - 小二最牛&quot;</span><br><span class="line"> </span><br><span class="line">tinydict = &#123;&apos;name&apos;: &apos;runoob&apos;,&apos;code&apos;:1, &apos;site&apos;: &apos;www.runoob.com&apos;&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">print (dict[&apos;one&apos;])       # 输出键为 &apos;one&apos; 的值</span><br><span class="line">print (dict[2])           # 输出键为 2 的值</span><br><span class="line">print (tinydict)          # 输出完整的字典</span><br><span class="line">print (tinydict.keys())   # 输出所有键</span><br><span class="line">print (tinydict.values()) # 输出所有值</span><br></pre></td></tr></table></figure></p><p>以上实例输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1 - 小二客栈</span><br><span class="line">2 - 小二最牛</span><br><span class="line">&#123;&apos;name&apos;: &apos;runoob&apos;, &apos;site&apos;: &apos;www.runoob.com&apos;, &apos;code&apos;: 1&#125;</span><br><span class="line">dict_keys([&apos;name&apos;, &apos;site&apos;, &apos;code&apos;])</span><br><span class="line">dict_values([&apos;runoob&apos;, &apos;www.runoob.com&apos;, 1])</span><br></pre></td></tr></table></figure></p><p>构造函数 dict() 可以直接从键值对序列中构建字典如下：</p><p>实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;dict([(&apos;xiaoer&apos;, 1), (&apos;Google&apos;, 2), (&apos;Taobao&apos;, 3)])</span><br><span class="line">&#123;&apos;Taobao&apos;: 3, &apos;xiaoer&apos;: 1, &apos;Google&apos;: 2&#125;</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; &#123;x: x**2 for x in (2, 4, 6)&#125;</span><br><span class="line">&#123;2: 4, 4: 16, 6: 36&#125;</span><br><span class="line"> </span><br><span class="line">&gt;&gt;&gt; dict(xiaoer=1, Google=2, Taobao=3)</span><br><span class="line">&#123;&apos;Taobao&apos;: 3, &apos;xiaoer&apos;: 1, &apos;Google&apos;: 2&#125;</span><br></pre></td></tr></table></figure></p><p>另外，字典类型也有一些内置的函数，例如clear()、keys()、values()等。</p><p>注意：</p><p>1、字典是一种映射类型，它的元素是键值对。<br>2、字典的关键字必须为不可变类型，且不能重复。<br>3、创建空字典使用 { }。</p><ul><li>Python数据类型转换<br>有时候，我们需要对数据内置的类型进行转换，数据类型的转换，你只需要将数据类型作为函数名即可。</li></ul><p>以下几个内置的函数可以执行数据类型之间的转换。这些函数返回一个新的对象，表示转换的值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">函数描述</span><br><span class="line">int(x [,base])</span><br><span class="line"></span><br><span class="line">将x转换为一个整数</span><br><span class="line"></span><br><span class="line">float(x)</span><br><span class="line"></span><br><span class="line">将x转换到一个浮点数</span><br><span class="line"></span><br><span class="line">complex(real [,imag])</span><br><span class="line"></span><br><span class="line">创建一个复数</span><br><span class="line"></span><br><span class="line">str(x)</span><br><span class="line"></span><br><span class="line">将对象 x 转换为字符串</span><br><span class="line"></span><br><span class="line">repr(x)</span><br><span class="line"></span><br><span class="line">将对象 x 转换为表达式字符串</span><br><span class="line"></span><br><span class="line">eval(str)</span><br><span class="line"></span><br><span class="line">用来计算在字符串中的有效Python表达式,并返回一个对象</span><br><span class="line"></span><br><span class="line">tuple(s)</span><br><span class="line"></span><br><span class="line">将序列 s 转换为一个元组</span><br><span class="line"></span><br><span class="line">list(s)</span><br><span class="line"></span><br><span class="line">将序列 s 转换为一个列表</span><br><span class="line"></span><br><span class="line">set(s)</span><br><span class="line"></span><br><span class="line">转换为可变集合</span><br><span class="line"></span><br><span class="line">dict(d)</span><br><span class="line"></span><br><span class="line">创建一个字典。d 必须是一个序列 (key,value)元组。</span><br><span class="line"></span><br><span class="line">frozenset(s)</span><br><span class="line"></span><br><span class="line">转换为不可变集合</span><br><span class="line"></span><br><span class="line">chr(x)</span><br><span class="line"></span><br><span class="line">将一个整数转换为一个字符</span><br><span class="line"></span><br><span class="line">ord(x)</span><br><span class="line"></span><br><span class="line">将一个字符转换为它的整数值</span><br><span class="line"></span><br><span class="line">hex(x)</span><br><span class="line"></span><br><span class="line">将一个整数转换为一个十六进制字符串</span><br><span class="line"></span><br><span class="line">oct(x)</span><br><span class="line"></span><br><span class="line">将一个整数转换为一个八进制字符串</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;** Python 中的变量不需要声明。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。&lt;br&gt;在 Python 中，变量就是变量，它没有类型，我们所说的”类型”是变量所指的内存中对象的类型。&lt;/p&gt;
&lt;p&gt;等号（=）用来给变量赋值。&lt;/p&gt;
&lt;p&gt;等号（=）运算
      
    
    </summary>
    
    
      <category term="Python" scheme="http://www.wenchong.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python基础语法</title>
    <link href="http://www.wenchong.top/2018/05/16/Python%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/"/>
    <id>http://www.wenchong.top/2018/05/16/Python学习（1）——基础语法/</id>
    <published>2018-05-16T07:49:50.720Z</published>
    <updated>2018-05-16T08:20:05.723Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>编码<br>Python3源码文件以UTF-8编码，所有字符串都是Unicode字符串。</p></li><li><p>标识符<br>第一个字符必须是字母表中字母或下划线_。<br>标识符的其他部分有字母、数字和下划线组成。<br>标识符对大小写敏感。</p></li><li><p>Python保留字<br>保留字即关键字。不能把它们用作任何标识符名称。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; keyword.kwlist</span><br><span class="line">[&apos;False&apos;, &apos;None&apos;, &apos;True&apos;, &apos;and&apos;, &apos;as&apos;, &apos;assert&apos;, &apos;break&apos;, &apos;class&apos;, &apos;continue&apos;, &apos;def&apos;, &apos;del&apos;, &apos;elif&apos;, &apos;else&apos;, &apos;except&apos;, &apos;finally&apos;, &apos;for&apos;, &apos;from&apos;, &apos;global&apos;, &apos;if&apos;, &apos;import&apos;, &apos;in&apos;, &apos;is&apos;, &apos;lambda&apos;, &apos;nonlocal&apos;, &apos;not&apos;, &apos;or&apos;, &apos;pass&apos;, &apos;raise&apos;, &apos;return&apos;, &apos;try&apos;, &apos;while&apos;, &apos;with&apos;, &apos;yield&apos;]</span><br></pre></td></tr></table></figure></li><li><p>注释<br>Python中单行注释以#开头<br>多行注释可以用多个#或者’’’和”””:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line"># 第一个注释</span><br><span class="line"># 第二个注释</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">第三注释</span><br><span class="line">第四注释</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">第五注释</span><br><span class="line">第六注释</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure></li><li><p>行与缩进<br>Python最具特色的就是使用缩进来表示代码块。<br>缩进的空格数是可变的，但是同一个代码块语句biubiu包含相同的缩进空格数。例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if True:</span><br><span class="line">    print (&quot;True&quot;)</span><br><span class="line">else:</span><br><span class="line">    print (&quot;False&quot;)</span><br></pre></td></tr></table></figure></li></ul><p>** 缩进不一致，会导致运行错误。错误类型如下：<br>IndentationError: unindent does not match any outer indentation level</p><ul><li>多行语句<br>Python通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠（\）来实现多行语句，例如：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">total = item_one + \</span><br><span class="line">        item_two + \</span><br><span class="line">        item_three</span><br></pre></td></tr></table></figure></li></ul><p>** 在 [], {}, 或 () 中的多行语句，不需要使用反斜杠()。</p><ul><li><p>数字类型<br>Python中有是四种类型：整数、布尔型、浮点数和复数。<br>int 整数<br>bool 布尔<br>float 浮点数<br>complex 复数</p></li><li><p>字符串<br>Python中单引号和双引号使用完全相同。<br>使用三引号可以指定一个多行字符串。<br>转义符’\’<br>反斜杠可以用来转义，但r可以让反斜杠不发生转义。如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r&quot;this is a line with \n&quot; 则\n会显示，并不是换行。</span><br></pre></td></tr></table></figure></li></ul><p>按字面意义级联字符串，如”this””is””string”会被自动转换成this id string。<br>字符串可以用+运算符连接诶在一起，用*运算符重复。<br>Python中的字符串有两种索引方式，从左往右以0开始，从右往左以-1开始。<br>Python中的字符串不能改变。<br>Python没有单独的字符类型，一个字符就是长度为1的字符串。<br>字符串的截取的语法格式如下：变量[头下标:尾下标]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word = &apos;字符串&apos;</span><br><span class="line">sentence = &quot;这是一个句子。&quot;</span><br><span class="line">paragraph = &quot;&quot;&quot;这是一个段落，</span><br><span class="line">可以由多行组成&quot;&quot;&quot;</span><br></pre></td></tr></table></figure></p><p>实例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line">str=&apos;Runoob&apos;</span><br><span class="line"></span><br><span class="line">print(str)                 # 输出字符串</span><br><span class="line">print(str[0:-1])           # 输出第一个到倒数第二个的所有字符</span><br><span class="line">print(str[0])              # 输出字符串第一个字符</span><br><span class="line">print(str[2:5])            # 输出从第三个开始到第五个的字符</span><br><span class="line">print(str[2:])             # 输出从第三个开始的后的所有字符</span><br><span class="line">print(str * 2)             # 输出字符串两次</span><br><span class="line">print(str + &apos;你好&apos;)        # 连接字符串</span><br><span class="line"></span><br><span class="line">print(&apos;------------------------------&apos;)</span><br><span class="line"></span><br><span class="line">print(&apos;hello\nrunoob&apos;)      # 使用反斜杠(\)+n转义特殊字符</span><br><span class="line">print(r&apos;hello\nrunoob&apos;)     # 在字符串前面添加一个 r，表示原始字符串，不会发生转义</span><br><span class="line">输出结果为：</span><br><span class="line"></span><br><span class="line">Runoob</span><br><span class="line">Runoo</span><br><span class="line">R</span><br><span class="line">noo</span><br><span class="line">noob</span><br><span class="line">RunoobRunoob</span><br><span class="line">Runoob你好</span><br><span class="line">------------------------------</span><br><span class="line">hello</span><br><span class="line">runoob</span><br><span class="line">hello\nrunoob</span><br></pre></td></tr></table></figure></p><ul><li>空行<br>函数、类的方法之间有空行分隔。</li></ul><p>*等待用户输入<br>执行下面的程序在按回车键后就会等待用户输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line">input(&quot;\n\n按下 enter 键后退出。&quot;)</span><br></pre></td></tr></table></figure></p><p>以上代码中 ，”\n\n”在结果输出前会输出两个新的空行。一旦用户按下 enter 键时，程序将退出。</p><ul><li><p>同一行显示多条语句<br>Python可以在同一行中使用多条语句，语句之间使用分号(;)分割，以下是一个简单的实例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line">import sys; x = &apos;小二&apos;; sys.stdout.write(x + &apos;\n&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">执行以上代码，输出结果为：</span><br><span class="line"></span><br><span class="line">小二</span><br></pre></td></tr></table></figure></li><li><p>多个语句构成代码组<br>缩进相同的一组语句构成一个代码块，我们称之代码组。</p></li></ul><p>像if、while、def和class这样的复合语句，首行以关键字开始，以冒号( : )结束，该行之后的一行或多行代码构成代码组。</p><p>我们将首行及后面的代码组称为一个子句(clause)。</p><p>如下实例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if expression : </span><br><span class="line">   suite</span><br><span class="line">elif expression : </span><br><span class="line">   suite </span><br><span class="line">else : </span><br><span class="line">   suite</span><br></pre></td></tr></table></figure></p><ul><li><p>print输出<br>print 默认输出是换行的，如果要实现不换行需要在变量末尾加上 end=””：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line">x=&quot;a&quot;</span><br><span class="line">y=&quot;b&quot;</span><br><span class="line"># 换行输出</span><br><span class="line">print( x )</span><br><span class="line">print( y )</span><br><span class="line"></span><br><span class="line">print(&apos;---------&apos;)</span><br><span class="line"># 不换行输出</span><br><span class="line">print( x, end=&quot; &quot; )</span><br><span class="line">print( y, end=&quot; &quot; )</span><br><span class="line">print()</span><br><span class="line">以上实例执行结果为：</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">---------</span><br><span class="line">a b</span><br></pre></td></tr></table></figure></li><li><p>import 与 from…import<br>在 python 用 import 或者 from…import 来导入相应的模块。</p></li></ul><p>将整个模块(somemodule)导入，格式为： import somemodule</p><p>从某个模块中导入某个函数,格式为： from somemodule import somefunction</p><p>从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc</p><p>将某个模块中的全部函数导入，格式为： from somemodule import *</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">导入 sys 模块</span><br><span class="line">import sys</span><br><span class="line">print(&apos;================Python import mode==========================&apos;);</span><br><span class="line">print (&apos;命令行参数为:&apos;)</span><br><span class="line">for i in sys.argv:</span><br><span class="line">    print (i)</span><br><span class="line">print (&apos;\n python 路径为&apos;,sys.path)</span><br><span class="line">导入 sys 模块的 argv,path 成员</span><br><span class="line">from sys import argv,path  #  导入特定的成员</span><br><span class="line"> </span><br><span class="line">print(&apos;================python from import===================================&apos;)</span><br><span class="line">print(&apos;path:&apos;,path) # 因为已经导入path成员，所以此处引用时不需要加sys.path</span><br></pre></td></tr></table></figure><ul><li>命令行参数<br>很多程序可以执行一些操作来查看一些基本信息，Python可以使用-h参数查看各参数帮助信息：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ python -h</span><br><span class="line">usage: python [option] ... [-c cmd | -m mod | file | -] [arg] ...</span><br><span class="line">Options and arguments (and corresponding environment variables):</span><br><span class="line">-c cmd : program passed in as string (terminates option list)</span><br><span class="line">-d     : debug output from parser (also PYTHONDEBUG=x)</span><br><span class="line">-E     : ignore environment variables (such as PYTHONPATH)</span><br><span class="line">-h     : print this help message and exit</span><br><span class="line"></span><br><span class="line">[ etc. ]</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;编码&lt;br&gt;Python3源码文件以UTF-8编码，所有字符串都是Unicode字符串。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;标识符&lt;br&gt;第一个字符必须是字母表中字母或下划线_。&lt;br&gt;标识符的其他部分有字母、数字和下划线组成。&lt;br&gt;标识符对大小写敏感。
      
    
    </summary>
    
    
      <category term="Python" scheme="http://www.wenchong.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Linux常用命令</title>
    <link href="http://www.wenchong.top/2018/05/14/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>http://www.wenchong.top/2018/05/14/Linux常用命令/</id>
    <published>2018-05-14T03:08:03.323Z</published>
    <updated>2018-05-14T06:16:03.670Z</updated>
    
    <content type="html"><![CDATA[<p>** 使用Linux时，会有大量的命令，而一般常用的命令并不是很多（当然Linux系统管理员除外）。<br>1.cd命令<br>这是一个非常基本，也是大家经常需要使用的命令，它用于切换当前目录，它的参数是要切换到的目录的路径，可以是绝对路径，也可以是相对路径。如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /root/Docements # 切换到目录/root/Docements  </span><br><span class="line">cd ./path          # 切换到当前目录下的path目录中，“.”表示当前目录    </span><br><span class="line">cd ../path         # 切换到上层目录中的path目录中，“..”表示上一层目录</span><br></pre></td></tr></table></figure></p><p>2、ls命令<br>这是一个非常有用的查看文件与目录的命令，list之意，它的参数非常多，下面就列出一些我常用的参数吧，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-l ：列出长数据串，包含文件的属性与权限数据等  </span><br><span class="line">-a ：列出全部的文件，连同隐藏文件（开头为.的文件）一起列出来（常用）  </span><br><span class="line">-d ：仅列出目录本身，而不是列出目录的文件数据  </span><br><span class="line">-h ：将文件容量以较易读的方式（GB，kB等）列出来  </span><br><span class="line">-R ：连同子目录的内容一起列出（递归列出），等于该目录下的所有文件都会显示出来  </span><br><span class="line">注：这些参数也可以组合使用，下面举两个例子：</span><br><span class="line"></span><br><span class="line">ls -l #以长数据串的形式列出当前目录下的数据文件和目录  </span><br><span class="line">ls -lR #以长数据串的形式列出当前目录下的所有文件</span><br></pre></td></tr></table></figure></p><p>3、grep命令<br>该命令常用于分析一行的信息，若当中有我们所需要的信息，就将该行显示出来，该命令通常与管道命令一起使用，用于对一些命令的输出进行筛选加工等等，它的简单语法为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">grep [-acinv] [--color=auto] &apos;查找字符串&apos; filename  </span><br><span class="line">它的常用参数如下：</span><br><span class="line"></span><br><span class="line">-a ：将binary文件以text文件的方式查找数据  </span><br><span class="line">-c ：计算找到‘查找字符串’的次数  </span><br><span class="line">-i ：忽略大小写的区别，即把大小写视为相同  </span><br><span class="line">-v ：反向选择，即显示出没有‘查找字符串’内容的那一行  </span><br><span class="line"># 例如：  </span><br><span class="line"># 取出文件/etc/man.config中包含MANPATH的行，并把找到的关键字加上颜色  </span><br><span class="line">grep --color=auto &apos;MANPATH&apos; /etc/man.config  </span><br><span class="line"># 把ls -l的输出中包含字母file（不区分大小写）的内容输出  </span><br><span class="line">ls -l | grep -i file</span><br></pre></td></tr></table></figure></p><p>4、find命令<br>find是一个基于查找的功能非常强大的命令，相对而言，它的使用也相对较为复杂，参数也比较多，所以在这里将给把它们分类列出，它的基本语法如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">find [PATH] [option] [action]  </span><br><span class="line">  </span><br><span class="line">** 与时间有关的参数：  </span><br><span class="line">-mtime n : n为数字，意思为在n天之前的“一天内”被更改过的文件；  </span><br><span class="line">-mtime +n : 列出在n天之前（不含n天本身）被更改过的文件名；  </span><br><span class="line">-mtime -n : 列出在n天之内（含n天本身）被更改过的文件名；  </span><br><span class="line">-newer file : 列出比file还要新的文件名  </span><br><span class="line"># 例如：  </span><br><span class="line">find /root -mtime 0 # 在当前目录下查找今天之内有改动的文件</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">与用户或用户组名有关的参数：  </span><br><span class="line">-user name : 列出文件所有者为name的文件  </span><br><span class="line">-group name : 列出文件所属用户组为name的文件  </span><br><span class="line">-uid n : 列出文件所有者为用户ID为n的文件  </span><br><span class="line">-gid n : 列出文件所属用户组为用户组ID为n的文件  </span><br><span class="line"># 例如：  </span><br><span class="line">find /home/ljianhui -user ljianhui # 在目录/home/ljianhui中找出所有者为ljianhui的文件</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 与文件权限及名称有关的参数：  </span><br><span class="line">-name filename ：找出文件名为filename的文件  </span><br><span class="line">-size [+-]SIZE ：找出比SIZE还要大（+）或小（-）的文件  </span><br><span class="line">-tpye TYPE ：查找文件的类型为TYPE的文件，TYPE的值主要有：一般文件（f)、设备文件（b、c）、  </span><br><span class="line">             目录（d）、连接文件（l）、socket（s）、FIFO管道文件（p）；  </span><br><span class="line">-perm mode ：查找文件权限刚好等于mode的文件，mode用数字表示，如0755；  </span><br><span class="line">-perm -mode ：查找文件权限必须要全部包括mode权限的文件，mode用数字表示  </span><br><span class="line">-perm +mode ：查找文件权限包含任一mode的权限的文件，mode用数字表示  </span><br><span class="line"># 例如：  </span><br><span class="line">find / -name passwd # 查找文件名为passwd的文件  </span><br><span class="line">find . -perm 0755 # 查找当前目录中文件权限的0755的文件  </span><br><span class="line">find . -size +12k # 查找当前目录中大于12KB的文件，注意c表示byte</span><br></pre></td></tr></table></figure><p>5、cp命令<br>该命令用于复制文件，copy之意，它还可以把多个文件一次性地复制到一个目录下，它的常用参数如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-a ：将文件的特性一起复制  </span><br><span class="line">-p ：连同文件的属性一起复制，而非使用默认方式，与-a相似，常用于备份  </span><br><span class="line">-i ：若目标文件已经存在时，在覆盖时会先询问操作的进行  </span><br><span class="line">-r ：递归持续复制，用于目录的复制行为  </span><br><span class="line">-u ：目标文件与源文件有差异时才会复制  </span><br><span class="line">例如 ：</span><br><span class="line"></span><br><span class="line">cp -a file1 file2 #连同文件的所有特性把文件file1复制成文件file2  </span><br><span class="line">cp file1 file2 file3 dir #把文件file1、file2、file3复制到目录dir中</span><br></pre></td></tr></table></figure></p><p>6、mv命令<br>该命令用于移动文件、目录或更名，move之意，它的常用参数如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-f ：force强制的意思，如果目标文件已经存在，不会询问而直接覆盖  </span><br><span class="line">-i ：若目标文件已经存在，就会询问是否覆盖  </span><br><span class="line">-u ：若目标文件已经存在，且比目标文件新，才会更新  </span><br><span class="line">注：该命令可以把一个文件或多个文件一次移动一个文件夹中，但是最后一个目标文件一定要是“目录”。</span><br><span class="line"></span><br><span class="line">例如：</span><br><span class="line">[plain] view plain copy</span><br><span class="line">mv file1 file2 file3 dir # 把文件file1、file2、file3移动到目录dir中  </span><br><span class="line">mv file1 file2 # 把文件file1重命名为file2</span><br></pre></td></tr></table></figure></p><p>7、rm命令<br>该命令用于删除文件或目录，remove之间，它的常用参数如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-f ：就是force的意思，忽略不存在的文件，不会出现警告消息  </span><br><span class="line">-i ：互动模式，在删除前会询问用户是否操作  </span><br><span class="line">-r ：递归删除，最常用于目录删除，它是一个非常危险的参数  </span><br><span class="line">例如：</span><br><span class="line">[plain] view plain copy</span><br><span class="line">rm -i file # 删除文件file，在删除之前会询问是否进行该操作  </span><br><span class="line">rm -fr dir # 强制删除目录dir中的所有文件 </span><br><span class="line">``` </span><br><span class="line">8、ps命令</span><br><span class="line">该命令用于将某个时间点的进程运行情况选取下来并输出，process之意，它的常用参数如下：</span><br></pre></td></tr></table></figure></p><p>-A ：所有的进程均显示出来<br>-a ：不与terminal有关的所有进程<br>-u ：有效用户的相关进程<br>-x ：一般与a参数一起使用，可列出较完整的信息<br>-l ：较长，较详细地将PID的信息列出<br>其实我们只要记住ps一般使用的命令参数搭配即可，它们并不多，如下：<br>[plain] view plain copy<br>ps aux # 查看系统所有的进程数据<br>ps ax # 查看不与terminal有关的所有进程<br>ps -lA # 查看系统所有的进程数据<br>ps axjf # 查看连同一部分进程树状态<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">9、kill命令</span><br><span class="line">该命令用于向某个工作（%jobnumber）或者是某个PID（数字）传送一个信号，它通常与ps和jobs命令一起使用，它的基本语法如下：</span><br></pre></td></tr></table></figure></p><p>kill -signal PID<br>signal的常用参数如下：<br>注：最前面的数字为信号的代号，使用时可以用代号代替相应的信号。<br>[plain] view plain copy<br>1：SIGHUP，启动被终止的进程<br>2：SIGINT，相当于输入ctrl+c，中断一个程序的进行<br>9：SIGKILL，强制中断一个进程的进行<br>15：SIGTERM，以正常的结束进程方式来终止进程<br>17：SIGSTOP，相当于输入ctrl+z，暂停一个进程的进行<br>例如：<br>[plain] view plain copy</p><h1 id="以正常的结束进程方式来终于第一个后台工作，可用jobs命令查看后台中的第一个工作进程"><a href="#以正常的结束进程方式来终于第一个后台工作，可用jobs命令查看后台中的第一个工作进程" class="headerlink" title="以正常的结束进程方式来终于第一个后台工作，可用jobs命令查看后台中的第一个工作进程"></a>以正常的结束进程方式来终于第一个后台工作，可用jobs命令查看后台中的第一个工作进程</h1><p>kill -SIGTERM %1   </p><h1 id="重新改动进程ID为PID的进程，PID可用ps命令通过管道命令加上grep命令进行筛选获得"><a href="#重新改动进程ID为PID的进程，PID可用ps命令通过管道命令加上grep命令进行筛选获得" class="headerlink" title="重新改动进程ID为PID的进程，PID可用ps命令通过管道命令加上grep命令进行筛选获得"></a>重新改动进程ID为PID的进程，PID可用ps命令通过管道命令加上grep命令进行筛选获得</h1><p>kill -SIGHUP PID<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10、killall命令</span><br><span class="line">该命令用于向一个命令启动的进程发送一个信号，它的一般语法如下：</span><br></pre></td></tr></table></figure></p><p>killall [-iIe] [command name]<br>它的参数如下：<br>[plain] view plain copy<br>-i ：交互式的意思，若需要删除时，会询问用户<br>-e ：表示后面接的command name要一致，但command name不能超过15个字符<br>-I ：命令名称忽略大小写  </p><h1 id="例如："><a href="#例如：" class="headerlink" title="例如："></a>例如：</h1><p>killall -SIGHUP syslogd # 重新启动syslogd<br>11、file命令<br>该命令用于判断接在file命令后的文件的基本数据，因为在Linux下文件的类型并不是以后缀为分的，所以这个命令对我们来说就很有用了，它的用法非常简单，基本语法如下：<br>[plain] view plain copy<br>file filename  </p><p>#例如：<br>file ./test<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">12、tar命令</span><br><span class="line">该命令用于对文件进行打包，默认情况并不会压缩，如果指定了相应的参数，它还会调用相应的压缩程序（如gzip和bzip等）进行压缩和解压。它的常用参数如下：</span><br></pre></td></tr></table></figure></p><p>-c ：新建打包文件<br>-t ：查看打包文件的内容含有哪些文件名<br>-x ：解打包或解压缩的功能，可以搭配-C（大写）指定解压的目录，注意-c,-t,-x不能同时出现在同一条命令中<br>-j ：通过bzip2的支持进行压缩/解压缩<br>-z ：通过gzip的支持进行压缩/解压缩<br>-v ：在压缩/解压缩过程中，将正在处理的文件名显示出来<br>-f filename ：filename为要处理的文件<br>-C dir ：指定压缩/解压缩的目录dir<br>上面的解说可以已经让你晕过去了，但是通常我们只需要记住下面三条命令即可：<br>[plain] view plain copy<br>压缩：tar -jcv -f filename.tar.bz2 要被处理的文件或目录名称<br>查询：tar -jtv -f filename.tar.bz2<br>解压：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录<br>注：文件名并不定要以后缀tar.bz2结尾，这里主要是为了说明使用的压缩程序为bzip2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">13、cat命令</span><br><span class="line">该命令用于查看文本文件的内容，后接要查看的文件名，通常可用管道与more和less一起使用，从而可以一页页地查看数据。例如：</span><br></pre></td></tr></table></figure></p><p>cat text | less # 查看text文件中的内容  </p><h1 id="注：这条命令也可以使用less-text来代替"><a href="#注：这条命令也可以使用less-text来代替" class="headerlink" title="注：这条命令也可以使用less text来代替"></a>注：这条命令也可以使用less text来代替</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">14、chgrp命令</span><br><span class="line">该命令用于改变文件所属用户组，它的使用非常简单，它的基本用法如下：</span><br></pre></td></tr></table></figure><p>chgrp [-R] dirname/filename<br>-R ：进行递归的持续对所有文件和子目录更改  </p><h1 id="例如：-1"><a href="#例如：-1" class="headerlink" title="例如："></a>例如：</h1><p>chgrp users -R ./dir # 递归地把dir目录下中的所有文件和子目录下所有文件的用户组修改为users<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">15、chown命令</span><br><span class="line">该命令用于改变文件的所有者，与chgrp命令的使用方法相同，只是修改的文件属性不同，不再详述。</span><br><span class="line"></span><br><span class="line">16、chmod命令</span><br><span class="line">该命令用于改变文件的权限，一般的用法如下：</span><br></pre></td></tr></table></figure></p><p>chmod [-R] xyz 文件或目录<br>-R：进行递归的持续更改，即连同子目录下的所有文件都会更改<br>同时，chmod还可以使用u（user）、g（group）、o（other）、a（all）和+（加入）、-（删除）、=（设置）跟rwx搭配来对文件的权限进行更改。</p><p>[plain] view plain copy</p><h1 id="例如：-2"><a href="#例如：-2" class="headerlink" title="例如："></a>例如：</h1><p>chmod 0755 file # 把file的文件权限改变为-rxwr-xr-x<br>chmod g+w file # 向file的文件权限中加入用户组可写权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">18、vim命令</span><br><span class="line">该命令主要用于文本编辑，它接一个或多个文件名作为参数，如果文件存在就打开，如果文件不存在就以该文件名创建一个文件。vim是一个非常好用的文本编辑器，它里面有很多非常好用的命令，在这里不再多说。你可以从这里下载vim常用操作的详细说明。</span><br><span class="line"></span><br><span class="line">19、gcc命令</span><br><span class="line">对于一个用Linux开发C程序的人来说，这个命令就非常重要了，它用于把C语言的源程序文件，编译成可执行程序，由于g++的很多参数跟它非常相似，所以这里只介绍gcc的参数，它的常用参数如下：</span><br></pre></td></tr></table></figure></p><p>-o ：output之意，用于指定生成一个可执行文件的文件名<br>-c ：用于把源文件生成目标文件（.o)，并阻止编译器创建一个完整的程序<br>-I ：增加编译时搜索头文件的路径<br>-L ：增加编译时搜索静态连接库的路径<br>-S ：把源文件生成汇编代码文件<br>-lm：表示标准库的目录中名为libm.a的函数库<br>-lpthread ：连接NPTL实现的线程库<br>-std= ：用于指定把使用的C语言的版本  </p><h1 id="例如：-3"><a href="#例如：-3" class="headerlink" title="例如："></a>例如：</h1><h1 id="把源文件test-c按照c99标准编译成可执行程序test"><a href="#把源文件test-c按照c99标准编译成可执行程序test" class="headerlink" title="把源文件test.c按照c99标准编译成可执行程序test"></a>把源文件test.c按照c99标准编译成可执行程序test</h1><p>gcc -o test test.c -lm -std=c99  </p><p>#把源文件test.c转换为相应的汇编程序源文件test.s<br>gcc -S test.c<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">20、time命令</span><br><span class="line">该命令用于测算一个命令（即程序）的执行时间。它的使用非常简单，就像平时输入命令一样，不过在命令的前面加入一个time即可，例如：</span><br></pre></td></tr></table></figure></p><p>time ./process<br>time ps aux<br>在程序或命令运行结束后，在最后输出了三个时间，它们分别是：<br>user：用户CPU时间，命令执行完成花费的用户CPU时间，即命令在用户态中执行时间总和；<br>system：系统CPU时间，命令执行完成花费的系统CPU时间，即命令在核心态中执行时间总和；<br>real：实际时间，从command命令行开始执行到运行终止的消逝时间；<br>```<br>注：用户CPU时间和系统CPU时间之和为CPU时间，即命令占用CPU执行的时间总和。实际时间要大于CPU时间，因为Linux是多任务操作系统，往往在执行一条命令时，系统还要处理其它任务。另一个需要注意的问题是即使每次执行相同命令，但所花费的时间也是不一样，其花费时间是与系统运行相关的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;** 使用Linux时，会有大量的命令，而一般常用的命令并不是很多（当然Linux系统管理员除外）。&lt;br&gt;1.cd命令&lt;br&gt;这是一个非常基本，也是大家经常需要使用的命令，它用于切换当前目录，它的参数是要切换到的目录的路径，可以是绝对路径，也可以是相对路径。如：&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="运维" scheme="http://www.wenchong.top/categories/operation/"/>
    
    
      <category term="Linux" scheme="http://www.wenchong.top/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>kafka介绍</title>
    <link href="http://www.wenchong.top/2018/05/11/kafka%E4%BB%8B%E7%BB%8D/"/>
    <id>http://www.wenchong.top/2018/05/11/kafka介绍/</id>
    <published>2018-05-10T16:00:00.000Z</published>
    <updated>2018-06-27T08:15:35.021Z</updated>
    
    <content type="html"><![CDATA[<h3 id="生产者发送消息到broker的流程"><a href="#生产者发送消息到broker的流程" class="headerlink" title="生产者发送消息到broker的流程"></a>生产者发送消息到broker的流程</h3><ul><li>1.ProducerIntercptor对消息进行拦截</li><li>2.Serialzer对key和values进行序列化</li><li>3.Partitioner对消息选择合适分区</li><li>4.RecordAccumulator收集消息，实现批量发送</li><li>5.Sender从RecordAccumulator获取消息</li><li>6.构造ClientRequest</li><li>7.将ClientRequest交给Network,准备发送</li><li>8.Network将请求放在KafkaChannel的缓存</li><li>9.发送请求</li><li>10.收到响应，发送ClientRequest</li><li>11.调用RecordBath的回调函数，最终调用到每一个消息上的回调函数<br><strong> 在这里主要涉及到两个线程：</strong><br>主线程主要负责封装消息成ProducerRecord对象，之后调用send方法将消息放入RecordAccumulator中暂存<br>Sender线程负责将消息构造成请求，并从RecordAccumulator取出消息消息并批量发送</li></ul><h3 id="核心字段"><a href="#核心字段" class="headerlink" title="核心字段"></a>核心字段</h3><p>String clientId：该生产者的唯一标示</p><p>AtomicInteger PRODUCER_CLIENT_ID_SEQUENCE: clientId生成器</p><p>Partitioner: 分区选择器，根据一定策略将消息路由到合适的分区</p><p>int maxRequestSize: 消息的最大长度</p><p>long totalMemorySize: 发送单个消息的缓冲区的大小</p><p>Metadata: 整个kafka集群的元数据</p><p>RecordAccumulator accumulator: 用于收集并缓存消息，等待sender线程获取</p><p>Sender:发送消息的sender任务</p><p>Thread ioThread: 执行sender任务发送消息的线程</p><p>CompressionType: 压缩算法，针对RecordAccumulator中多条消息进行的压缩，消息越多效果越好</p><p>Serializer<k> keySerializer: key的序列化器</k></p><p>Serializer<v> valueSerializer: value的序列化器</v></p><p>long maxBlockTimeMs: 等待更新kafka集群元数据的最大时长</p><p>int requestTimeoutMs: 消息超时时长</p><p>ProducerInterceptors<k, v=""> interceptors: 拦截record，可以对record进行进一步处理再发送到服务器</k,></p><h3 id="重要的方法"><a href="#重要的方法" class="headerlink" title="重要的方法"></a>重要的方法</h3><p> 调用ProducerInterceptors的onSend方法，对消息进行拦截<br> 调用doSend方法，然后就调用waitOnMetadata方法获取kafka集群元数据信息，底层会唤醒Sender线程更新Metadata保存的kafka元数据<br> 调用Serializer的serialize方法对消息的key和value进行序列化<br> 调用partition方法为消息选择合适的分区<br> 调用RecordAccumulator的append方法将消息追加到RecordAccumulator中<br> 唤醒Sender线程，由Sender线程将RecordAccumulator中缓存的消息发送出去<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line">public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123;</span><br><span class="line">    // 在发送消息之前，对消息进行拦截，然后可以对消息进行修改</span><br><span class="line">    ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors == null ? record : this.interceptors.onSend(record);</span><br><span class="line">    return doSend(interceptedRecord, callback);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">/** 异步发送record 到topic */</span><br><span class="line">private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123;</span><br><span class="line">    TopicPartition tp = null;</span><br><span class="line">    try &#123;</span><br><span class="line">        // 首先获取集群信息和加载元数据的时间</span><br><span class="line">        ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);</span><br><span class="line">        // 计算剩余的等待时间，还可以用于等待添加数据到队列（总的等待时间-获取元数据信息的时间）</span><br><span class="line">        long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);</span><br><span class="line">        Cluster cluster = clusterAndWaitTime.cluster;</span><br><span class="line">        // 序列化key &amp; value</span><br><span class="line">        byte[] serializedKey;</span><br><span class="line">        try &#123;</span><br><span class="line">            serializedKey = keySerializer.serialize(record.topic(), record.key());</span><br><span class="line">        &#125; catch (ClassCastException cce) &#123;</span><br><span class="line">            throw new SerializationException(&quot;Can&apos;t convert key of class &quot; + record.key().getClass().getName() +</span><br><span class="line">                    &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                    &quot; specified in key.serializer&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        byte[] serializedValue;</span><br><span class="line">        try &#123;</span><br><span class="line">            serializedValue = valueSerializer.serialize(record.topic(), record.value());</span><br><span class="line">        &#125; catch (ClassCastException cce) &#123;</span><br><span class="line">            throw new SerializationException(&quot;Can&apos;t convert value of class &quot; + record.value().getClass().getName() +</span><br><span class="line">                    &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                    &quot; specified in value.serializer&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        // 计算record的分区，默认是0</span><br><span class="line">        int partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">        // record的总长度</span><br><span class="line">        // size(4)+offset(8)+crc(4)+ magic(1)+attribute(1)+timestamp(8)+key(4)+keysize+value(4)+valuesize</span><br><span class="line">        int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);</span><br><span class="line">        // 但是record的总长度不能大于maxRequestSize和totalMemorySize</span><br><span class="line">        ensureValidRecordSize(serializedSize);</span><br><span class="line">        tp = new TopicPartition(record.topic(), partition);</span><br><span class="line">        long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp();</span><br><span class="line">        log.trace(&quot;Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;&quot;, record, callback, record.topic(), partition);</span><br><span class="line">        Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp);</span><br><span class="line">        // 将record放入RecordAccumulator队列（相当于消息缓冲区），供Sender线程去读取数据，然后发给broker</span><br><span class="line">        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs);</span><br><span class="line">        // 如果满了或者是新创建的，必须满上唤醒sender线程</span><br><span class="line">        if (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">            log.trace(&quot;Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch&quot;, record.topic(), partition);</span><br><span class="line">            this.sender.wakeup();</span><br><span class="line">        &#125;</span><br><span class="line">        // 返回结果</span><br><span class="line">        return result.future;</span><br><span class="line">        // handling exceptions and record the errors;</span><br><span class="line">        // for API exceptions return them in the future,</span><br><span class="line">        // for other exceptions throw directly</span><br><span class="line">    &#125; catch (ApiException e) &#123;</span><br><span class="line">        log.debug(&quot;Exception occurred during message send:&quot;, e);</span><br><span class="line">        if (callback != null)</span><br><span class="line">            callback.onCompletion(null, e);</span><br><span class="line">        this.errors.record();</span><br><span class="line">        if (this.interceptors != null)</span><br><span class="line">            this.interceptors.onSendError(record, tp, e);</span><br><span class="line">        return new FutureFailure(e);</span><br><span class="line">    &#125; catch (InterruptedException e) &#123;</span><br><span class="line">        this.errors.record();</span><br><span class="line">        if (this.interceptors != null)</span><br><span class="line">            this.interceptors.onSendError(record, tp, e);</span><br><span class="line">        throw new InterruptException(e);</span><br><span class="line">    &#125; catch (BufferExhaustedException e) &#123;</span><br><span class="line">        this.errors.record();</span><br><span class="line">        this.metrics.sensor(&quot;buffer-exhausted-records&quot;).record();</span><br><span class="line">        if (this.interceptors != null)</span><br><span class="line">            this.interceptors.onSendError(record, tp, e);</span><br><span class="line">        throw e;</span><br><span class="line">    &#125; catch (KafkaException e) &#123;</span><br><span class="line">        this.errors.record();</span><br><span class="line">        if (this.interceptors != null)</span><br><span class="line">            this.interceptors.onSendError(record, tp, e);</span><br><span class="line">        throw e;</span><br><span class="line">    &#125; catch (Exception e) &#123;</span><br><span class="line">        // we notify interceptor about all exceptions, since onSend is called before anything else in this method</span><br><span class="line">        if (this.interceptors != null)</span><br><span class="line">            this.interceptors.onSendError(record, tp, e);</span><br><span class="line">        throw e;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">private ClusterAndWaitTime waitOnMetadata(String topic, Integer partition, long maxWaitMs) throws InterruptedException &#123;</span><br><span class="line">    // add topic to metadata topic list if it is not there already and reset expiry</span><br><span class="line">    // 如果元数据不存在这个topic，则添加到元数据的topic集合中</span><br><span class="line">    metadata.add(topic);</span><br><span class="line">    // 根据元数据获取集群信息</span><br><span class="line">    Cluster cluster = metadata.fetch();</span><br><span class="line">    // 获取指定topic的partition数量</span><br><span class="line">    Integer partitionsCount = cluster.partitionCountForTopic(topic);</span><br><span class="line">    // Return cached metadata if we have it, and if the record&apos;s partition is either undefined</span><br><span class="line">    // or within the known partition range</span><br><span class="line">    // 如果partition数量不为空，直接返回</span><br><span class="line">    if (partitionsCount != null &amp;&amp; (partition == null || partition &lt; partitionsCount))</span><br><span class="line">        return new ClusterAndWaitTime(cluster, 0);</span><br><span class="line"></span><br><span class="line">    long begin = time.milliseconds();</span><br><span class="line">    // 最大的等待时间</span><br><span class="line">    long remainingWaitMs = maxWaitMs;</span><br><span class="line">    long elapsed;</span><br><span class="line">    // Issue metadata requests until we have metadata for the topic or maxWaitTimeMs is exceeded.</span><br><span class="line">    // In case we already have cached metadata for the topic, but the requested partition is greater</span><br><span class="line">    // than expected, issue an update request only once. This is necessary in case the metadata</span><br><span class="line">    // is stale and the number of partitions for this topic has increased in the meantime.</span><br><span class="line">    do &#123;</span><br><span class="line">        log.trace(&quot;Requesting metadata update for topic &#123;&#125;.&quot;, topic);</span><br><span class="line">        // 请求更新当前的集群元数据信息，在更新之前返回当前版本</span><br><span class="line">        int version = metadata.requestUpdate();</span><br><span class="line">        // 唤醒sender线程</span><br><span class="line">        sender.wakeup();</span><br><span class="line">        try &#123;</span><br><span class="line">            // 等待元数据更新，直到当前版本大于我们所知道的最新版本</span><br><span class="line">            metadata.awaitUpdate(version, remainingWaitMs);</span><br><span class="line">        &#125; catch (TimeoutException ex) &#123;</span><br><span class="line">            // Rethrow with original maxWaitMs to prevent logging exception with remainingWaitMs</span><br><span class="line">            throw new TimeoutException(&quot;Failed to update metadata after &quot; + maxWaitMs + &quot; ms.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        // metadata更新完了在获取一次集群信息</span><br><span class="line">        cluster = metadata.fetch();</span><br><span class="line">        elapsed = time.milliseconds() - begin;</span><br><span class="line">        // 如果时间超过最大等待时间，抛出更新元数据失败异常</span><br><span class="line">        if (elapsed &gt;= maxWaitMs)</span><br><span class="line">            throw new TimeoutException(&quot;Failed to update metadata after &quot; + maxWaitMs + &quot; ms.&quot;);</span><br><span class="line">        // 如果集群未授权topics包含这个topic，也会抛出异常</span><br><span class="line">        if (cluster.unauthorizedTopics().contains(topic))</span><br><span class="line">            throw new TopicAuthorizationException(topic);</span><br><span class="line">        remainingWaitMs = maxWaitMs - elapsed;</span><br><span class="line">        // 在此获取该topic的partition数量</span><br><span class="line">        partitionsCount = cluster.partitionCountForTopic(topic);</span><br><span class="line">    &#125; while (partitionsCount == null);// 直到topic的partition数量不为空</span><br><span class="line"></span><br><span class="line">    if (partition != null &amp;&amp; partition &gt;= partitionsCount) &#123;</span><br><span class="line">        throw new KafkaException(</span><br><span class="line">                String.format(&quot;Invalid partition given with record: %d is not in the range [0...%d).&quot;, partition, partitionsCount));</span><br><span class="line">    &#125;</span><br><span class="line">    // 返回ClusterAndWaitTime</span><br><span class="line">    return new ClusterAndWaitTime(cluster, elapsed);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;生产者发送消息到broker的流程&quot;&gt;&lt;a href=&quot;#生产者发送消息到broker的流程&quot; class=&quot;headerlink&quot; title=&quot;生产者发送消息到broker的流程&quot;&gt;&lt;/a&gt;生产者发送消息到broker的流程&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1.P
      
    
    </summary>
    
      <category term="kafka" scheme="http://www.wenchong.top/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://www.wenchong.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>nginx安装</title>
    <link href="http://www.wenchong.top/2018/05/11/nginx%E5%AE%89%E8%A3%85/"/>
    <id>http://www.wenchong.top/2018/05/11/nginx安装/</id>
    <published>2018-05-10T16:00:00.000Z</published>
    <updated>2018-05-14T06:13:40.805Z</updated>
    
    <content type="html"><![CDATA[<p>** Nginx(“engine x”)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。</p><p>** 在高连接并发的情况下，Nginx是Apache服务器不错的替代品。</p><h3 id="Nginx-安装"><a href="#Nginx-安装" class="headerlink" title="Nginx 安装"></a>Nginx 安装</h3><p>系统平台：CentOS release 6.6 (Final) 64位。</p><h3 id="一、安装编译工具及库文件"><a href="#一、安装编译工具及库文件" class="headerlink" title="一、安装编译工具及库文件"></a>一、安装编译工具及库文件</h3><p>yum -y install make zlib zlib-devel gcc-c++ libtool  openssl openssl-devel</p><h3 id="二、首先要安装-PCRE"><a href="#二、首先要安装-PCRE" class="headerlink" title="二、首先要安装 PCRE"></a>二、首先要安装 PCRE</h3><p>PCRE 作用是让 Nginx 支持 Rewrite 功能。</p><p>1、下载 PCRE 安装包，下载地址： <a href="http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz" target="_blank" rel="noopener">http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz</a></p><p>[root@bogon src]# wget <a href="http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz" target="_blank" rel="noopener">http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz</a></p><p>2、解压安装包:</p><p>[root@bogon src]# tar zxvf pcre-8.35.tar.gz<br>3、进入安装包目录</p><p>[root@bogon src]# cd pcre-8.35<br>4、编译安装 </p><p>[root@bogon pcre-8.35]# ./configure<br>[root@bogon pcre-8.35]# make &amp;&amp; make install<br>5、查看pcre版本</p><p>[root@bogon pcre-8.35]# pcre-config –version</p><h3 id="安装-Nginx"><a href="#安装-Nginx" class="headerlink" title="安装 Nginx"></a>安装 Nginx</h3><p>1、下载 Nginx，下载地址：<a href="http://nginx.org/download/nginx-1.6.2.tar.gz" target="_blank" rel="noopener">http://nginx.org/download/nginx-1.6.2.tar.gz</a></p><p>[root@bogon src]# wget <a href="http://nginx.org/download/nginx-1.6.2.tar.gz" target="_blank" rel="noopener">http://nginx.org/download/nginx-1.6.2.tar.gz</a><br>2、解压安装包</p><p>[root@bogon src]# tar zxvf nginx-1.6.2.tar.gz<br>3、进入安装包目录</p><p>[root@bogon src]# cd nginx-1.6.2<br>4、编译安装</p><p>[root@bogon nginx-1.6.2]# ./configure –prefix=/usr/local/webserver/nginx –with-http_stub_status_module –with-http_ssl_module –with-pcre=/usr/local/src/pcre-8.35<br>[root@bogon nginx-1.6.2]# make<br>[root@bogon nginx-1.6.2]# make install<br>5、查看nginx版本</p><p>[root@bogon nginx-1.6.2]# /usr/local/webserver/nginx/sbin/nginx -v</p><p>** 到此，nginx安装完成。</p><h3 id="Nginx-配置"><a href="#Nginx-配置" class="headerlink" title="Nginx 配置"></a>Nginx 配置</h3><p>创建 Nginx 运行使用的用户 www：</p><p>[root@bogon conf]# /usr/sbin/groupadd www<br>[root@bogon conf]# /usr/sbin/useradd -g www www<br>配置nginx.conf ，将/usr/local/webserver/nginx/conf/nginx.conf替换为以下内容</p><p>[root@bogon conf]#  cat /usr/local/webserver/nginx/conf/nginx.conf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">user www www;</span><br><span class="line">worker_processes 2; #设置值和CPU核心数一致</span><br><span class="line">error_log /usr/local/webserver/nginx/logs/nginx_error.log crit; #日志位置和日志级别</span><br><span class="line">pid /usr/local/webserver/nginx/nginx.pid;</span><br><span class="line">#Specifies the value for maximum file descriptors that can be opened by this process.</span><br><span class="line">worker_rlimit_nofile 65535;</span><br><span class="line">events</span><br><span class="line">&#123;</span><br><span class="line">  use epoll;</span><br><span class="line">  worker_connections 65535;</span><br><span class="line">&#125;</span><br><span class="line">http</span><br><span class="line">&#123;</span><br><span class="line">  include mime.types;</span><br><span class="line">  default_type application/octet-stream;</span><br><span class="line">  log_format main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</span><br><span class="line">               &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</span><br><span class="line">               &apos;&quot;$http_user_agent&quot; $http_x_forwarded_for&apos;;</span><br><span class="line">  </span><br><span class="line">#charset gb2312;</span><br><span class="line">     </span><br><span class="line">  server_names_hash_bucket_size 128;</span><br><span class="line">  client_header_buffer_size 32k;</span><br><span class="line">  large_client_header_buffers 4 32k;</span><br><span class="line">  client_max_body_size 8m;</span><br><span class="line">     </span><br><span class="line">  sendfile on;</span><br><span class="line">  tcp_nopush on;</span><br><span class="line">  keepalive_timeout 60;</span><br><span class="line">  tcp_nodelay on;</span><br><span class="line">  fastcgi_connect_timeout 300;</span><br><span class="line">  fastcgi_send_timeout 300;</span><br><span class="line">  fastcgi_read_timeout 300;</span><br><span class="line">  fastcgi_buffer_size 64k;</span><br><span class="line">  fastcgi_buffers 4 64k;</span><br><span class="line">  fastcgi_busy_buffers_size 128k;</span><br><span class="line">  fastcgi_temp_file_write_size 128k;</span><br><span class="line">  gzip on; </span><br><span class="line">  gzip_min_length 1k;</span><br><span class="line">  gzip_buffers 4 16k;</span><br><span class="line">  gzip_http_version 1.0;</span><br><span class="line">  gzip_comp_level 2;</span><br><span class="line">  gzip_types text/plain application/x-javascript text/css application/xml;</span><br><span class="line">  gzip_vary on;</span><br><span class="line"> </span><br><span class="line">  #limit_zone crawler $binary_remote_addr 10m;</span><br><span class="line"> #下面是server虚拟主机的配置</span><br><span class="line"> server</span><br><span class="line">  &#123;</span><br><span class="line">    listen 80;#监听端口</span><br><span class="line">    server_name localhost;#域名</span><br><span class="line">    index index.html index.htm index.php;</span><br><span class="line">    root /usr/local/webserver/nginx/html;#站点目录</span><br><span class="line">      location ~ .*\.(php|php5)?$</span><br><span class="line">    &#123;</span><br><span class="line">      #fastcgi_pass unix:/tmp/php-cgi.sock;</span><br><span class="line">      fastcgi_pass 127.0.0.1:9000;</span><br><span class="line">      fastcgi_index index.php;</span><br><span class="line">      include fastcgi.conf;</span><br><span class="line">    &#125;</span><br><span class="line">    location ~ .*\.(gif|jpg|jpeg|png|bmp|swf|ico)$</span><br><span class="line">    &#123;</span><br><span class="line">      expires 30d;</span><br><span class="line">#access_log off;</span><br><span class="line">    &#125;</span><br><span class="line">    location ~ .*\.(js|css)?$</span><br><span class="line">    &#123;</span><br><span class="line">      expires 15d;</span><br><span class="line">#access_log off;</span><br><span class="line">    &#125;</span><br><span class="line">    access_log off;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>检查配置文件ngnix.conf的正确性命令：</p><p>[root@bogon conf]# /usr/local/webserver/nginx/sbin/nginx -t</p><h3 id="启动-Nginx"><a href="#启动-Nginx" class="headerlink" title="启动 Nginx"></a>启动 Nginx</h3><p>Nginx 启动命令如下：</p><p>[root@bogon conf]# /usr/local/webserver/nginx/sbin/nginx</p><h3 id="访问站点"><a href="#访问站点" class="headerlink" title="访问站点"></a>访问站点</h3><p>从浏览器访问我们配置的站点ip：</p><p>** Nginx 其他命令<br>以下包含了 Nginx 常用的几个命令：</p><p>/usr/local/webserver/nginx/sbin/nginx -s reload            # 重新载入配置文件<br>/usr/local/webserver/nginx/sbin/nginx -s reopen            # 重启 Nginx<br>/usr/local/webserver/nginx/sbin/nginx -s stop              # 停止 Nginx</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;** Nginx(“engine x”)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。&lt;/p&gt;
&lt;p&gt;** 在高连接并发的情况下，Nginx是Apache服务器不错的替代品。
      
    
    </summary>
    
      <category term="运维" scheme="http://www.wenchong.top/categories/operation/"/>
    
    
      <category term="nginx" scheme="http://www.wenchong.top/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>CDH安装</title>
    <link href="http://www.wenchong.top/2018/05/04/CDH%E6%90%AD%E5%BB%BA/"/>
    <id>http://www.wenchong.top/2018/05/04/CDH搭建/</id>
    <published>2018-05-04T01:13:27.168Z</published>
    <updated>2018-05-21T07:11:05.465Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、准备"><a href="#一、准备" class="headerlink" title="一、准备"></a>一、准备</h1><ul><li>1.官网下载Cloudera Manager和CDH 5.11<br>Cloudera Manager下载地址：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://archive.cloudera.com/cm5/cm/5/</span><br></pre></td></tr></table></figure></li></ul><p>cloudera-manager-el6-cm5.11.0_x86_64.tar.gz</p><p>CDH安装包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://archive.cloudera.com/cdh5/parcels/latest/</span><br></pre></td></tr></table></figure></p><p>CDH-5.11.0-1.cdh5.11.0.p0.34-el6.parcel</p><p>CDH-5.11.0-1.cdh5.11.0.p0.34-el6.parcel.sha1</p><p>manifest.json</p><p>2.JDK 7<br>jdk-7u80-linux-x64.tar.gz</p><p>3.mysql 5.6和最新的mysql JDBC connector<br>mysql-5.6.36-linux-glibc2.5-x86_64.tar.gz</p><p>mysql-connector-java-5.1.44-bin.jar</p><h1 id="二、CentOS6-5安装"><a href="#二、CentOS6-5安装" class="headerlink" title="二、CentOS6.5安装"></a>二、CentOS6.5安装</h1><p>用户/密码</p><p>root/XXXX</p><p>XXX/123</p><p>主机名 IP</p><p>master 192.168.10.111</p><p>slave1 192.168.10.121</p><p>slave2 192.168.10.131</p><p>配置/etc/hostname</p><p>分别是maser和slave1和slave2</p><p>主机名 IP<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">master 192.168.10.11  </span><br><span class="line"></span><br><span class="line">slave1 192.168.10.12</span><br><span class="line"></span><br><span class="line">slave2 192.168.10.13</span><br></pre></td></tr></table></figure></p><p>配置/etc/hostname</p><p>分别是maser和slave1和slave2</p><p>3台都配置/etc/hosts</p><p>加入:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.111 master</span><br><span class="line"></span><br><span class="line">192.168.121 slave1</span><br><span class="line"></span><br><span class="line">192.168.131 slave2</span><br></pre></td></tr></table></figure></p><p>注：安装过程都使用root用户</p><p>CDH 安装在/opt。</p><p>CDH在var目录会放文件，要预留空间</p><p>数据保存在/data</p><h1 id="三、配置SSH无密码登录"><a href="#三、配置SSH无密码登录" class="headerlink" title="三、配置SSH无密码登录"></a>三、配置SSH无密码登录</h1><p>1.查看是否安装openssh和rsync<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep openssh</span><br><span class="line"></span><br><span class="line">rpm -qa | grep rsync</span><br></pre></td></tr></table></figure></p><ol><li>三台都运行<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t dsa -P &apos;&apos; -f ~/.ssh/id_dsa</span><br></pre></td></tr></table></figure></li></ol><p>(-t dsa：表示使用密钥的加密类型，可以为’rsa’和’dsa’)</p><p>3.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>这一步，把本机的密钥写入authorized_keys文件</p><p>然后要将其他2台的密钥写入本机的authorized_keys文件</p><p>4.</p><p>master运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp ~/.ssh/id_dsa.pub root@slave1:~/.ssh/ authorized_keys_from_master</span><br><span class="line"></span><br><span class="line">scp ~/.ssh/id_dsa.pub root@slave2:~/.ssh/ authorized_keys_from_master</span><br></pre></td></tr></table></figure></p><p>slave1 运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat .ssh/authorized_keys_from_master  &gt;&gt;  ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>slave2 运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat .ssh/authorized_keys_from_master  &gt;&gt;  ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>slave1运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp ~/.ssh/id_dsa.pub root@master:~/.ssh/authorized_keys_from_slave1</span><br><span class="line"></span><br><span class="line">scp ~/.ssh/id_dsa.pub root@slave2:~/.ssh/authorized_keys_from_slave1</span><br></pre></td></tr></table></figure></p><p>master运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat .ssh/authorized_keys_from_slave1  &gt;&gt;  ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>slave2 运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat .ssh/authorized_keys_from_slave1 &gt;&gt;  ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>slave2运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp ~/.ssh/id_dsa.pub root@master:~/.ssh/authorized_keys_from_slave2</span><br><span class="line"></span><br><span class="line">scp ~/.ssh/id_dsa.pub root@slave1:~/.ssh/authorized_keys_from_slave2</span><br></pre></td></tr></table></figure></p><p>master运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat .ssh/authorized_keys_from_slave2  &gt;&gt;  ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>slave2 运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat .ssh/authorized_keys_from_slave2 &gt;&gt;  ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>最后3台的authorized_keys文件，内容都有3个key</p><p>测试SSH无密码登陆</p><p>在任意一台上，ssh slave1 都可以无密码登陆</p><h1 id="四、同步各时间节点"><a href="#四、同步各时间节点" class="headerlink" title="四、同步各时间节点"></a>四、同步各时间节点</h1><ul><li>1 将3台服务器时间同步，若hbase各节点时间差距过大会报错（默认30秒）</li></ul><p>关闭3台的防火墙<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br><span class="line"></span><br><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure></p><p>重启后也生效</p><ul><li>2 配置ntp服务器（master）</li></ul><p>vim /etc/ntp.conf</p><p>然后加入以下配置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">127.127.1.0</span><br><span class="line"></span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure></p><p>注：</p><p>后面那个数字在0-15之间都可以，这样就将这台机器的本地时间作为ntp服务提供给客户端</p><ul><li>3 重启ntpd</li></ul><p>/etc/init.d/ntpd start</p><p>chkconfig  ntpd on下次开机时 自动重启</p><h1 id="五、安装JDK"><a href="#五、安装JDK" class="headerlink" title="五、安装JDK"></a>五、安装JDK</h1><ul><li>1 检查并写在openjdk</li></ul><p>检查命令：java -version 或 rpm -qa | grep java</p><p>卸载命令：rpm –e –nodeps(忽略依赖) 安装包名</p><p>或yum -y remove 安装包名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -e --nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64</span><br><span class="line"></span><br><span class="line">rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64</span><br></pre></td></tr></table></figure></p><ul><li>2 解压安装</li></ul><p>Jdk版本：jdk-7u80-linux-x64.tar.gz</p><p>解压到/usr/local :</p><p>cd /usr/local;</p><p>tar –zxvf jdk-7u80-linux-x64.tar.gz</p><ul><li>3 配置环境变量</li></ul><p>3台都编辑/etc/profile文件：vim /etc/profile</p><p>加入</p><p>#set java environment<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.7.0_80</span><br><span class="line"></span><br><span class="line">export LASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line"></span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure></p><p>使配置生效：source /etc/profile</p><ul><li><p>六、安装mysql<br>mysql安装在slave2 192.168.10.131</p></li><li><p>1 检查是否存在mysql库文件</p></li></ul><p>检查命令：rpm –qa | grep mysql</p><p>若存在则卸载：rpm -e –nodeps mysql-libs-5.1.71-1.el6.x86_64</p><ul><li>2 检查是否存在mysql用户和组</li></ul><p>检查命令：cat /etc/group | grep mysql;</p><p>cat /etc/passwd | grep mysql</p><p>若不存在则创建：groupadd mysql;</p><p>useradd –r –g mysql mysql</p><p>(-r参数表示mysql用户是系统用户，不可用于登录系统)</p><ul><li>3 创建数据、日志存储目录</li></ul><p>命令：mkdir -p /data/mysql/data;  mkdir /var/lib/mysql/log</p><p>更改所属用户：</p><p>chown –R mysql:mysql  /data/mysql</p><p>chown –R mysql:mysql  /usr/local/mysql</p><ul><li>4 解压并初始化</li></ul><p>解压安装包：</p><p>/usr/local目录</p><p>tar –zxvf mysql-5.6.36-linux-glibc2.5-x86_64.tar.gz</p><p>改目录名</p><p>mv mysql-5.6.36-linux-glibc2.5-x86_64 mysql</p><p>初始化参数：</p><p>到mysql目录</p><p>bin/mysqld –initialize –user=mysql –basedir=/usr/local/mysql –datadir=/data/mysql/data</p><p>复制配置文件</p><p>cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql</p><p>cp /usr/local/mysql/support-files/my-default.cnf /etc/my.cnf</p><p>修改</p><p>vi /etc/init.d/mysql</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">basedir=/usr/local/mysql</span><br><span class="line"></span><br><span class="line">datadir=/data/mysql/data</span><br></pre></td></tr></table></figure><p>vi /etc/my.cnf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">basedir =/usr/local/mysql</span><br><span class="line"></span><br><span class="line">datadir =/data/mysql/data</span><br><span class="line"></span><br><span class="line">character_set_server=utf8</span><br></pre></td></tr></table></figure></p><p>在usr/local/mysql/script目录运行</p><p>./mysql_install_db –user=mysql –basedir=/usr/local/mysql –datadir=/data/mysql/data</p><ul><li>5 添加环境变量</li></ul><p>编辑文件：vim /etc/profile<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/bin:$PATH</span><br><span class="line">export PATH=/usr/local/mysql/bin:$PATH</span><br></pre></td></tr></table></figure></p><ul><li>6 配置mysql自动启动<br>chmod 755 /etc/init.d/mysql</li></ul><p>chkconfig –add mysql</p><p>设置在运行级别为3和5时mysql自动启动：<br>chkconfig –level 35 mysql on</p><ul><li>7 启动mysql</li></ul><p>命令：bin/mysqld_safe –user=mysql &amp;</p><p>登录mysql  </p><p>bin/mysql –user=root -p</p><p>设置root密码</p><p>set password=password(‘anjian123’);</p><p>()允许root在任何主机登录：<br>grant all privileges on <em>.</em> to ‘root’@’%’ identified by ‘anjian123’  with grant option;</p><p>grant all privileges on <em>.</em> to ‘root’@’master’ identified by ‘anjian123’  with grant option;</p><p>刷新权限表：flush privileges;</p><ul><li>8 创建以下数据库</li></ul><p>create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</p><p>create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci; </p><p>create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</p><p>create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</p><h1 id="七、安装CDH"><a href="#七、安装CDH" class="headerlink" title="七、安装CDH"></a>七、安装CDH</h1><ul><li>1 安装Cloudera Manager Server 和Agent</li></ul><p>主节点解压安装</p><p>cloudera manager的目录默认位置在/opt下，解压：tar xzvf cloudera-manager*.tar.gz将解压后的cm-5.11.0和cloudera目录放到/opt目录下。</p><p>复制mysql-connector-java-5.1.44-bin.jar到目录/usr/share/java和/opt/cm-5.11.0/share/cmf/lib/</p><p>并改名成mysql-connector-java.jar</p><ul><li>2 创建用户cloudera-scm</li></ul><p>在所有节点上执行</p><p>useradd –system –no-create-home –shell=/bin/false –comment “Cloudera SCM User” cloudera-scm</p><ul><li>3 agent配置</li></ul><p>主节点，修改/opt/cm-5.11.0/etc/cloudera-scm-agent/config.ini中的server_host为主节点的主机名。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> # Hostname of the CM server.</span><br><span class="line"></span><br><span class="line">server_host=master</span><br></pre></td></tr></table></figure></p><p>同步Agent到其他所有节点：</p><p>scp -r /opt/cm-5.11.0 root@slave1:/opt/</p><p>scp -r /opt/cm-5.11.0 root@slave2:/opt/</p><ul><li>4 在主节点初始化CM5数据库</li></ul><p>运行</p><p>/opt/cm-5.11.0/share/cmf/schema/scm_prepare_database.sh mysql cm -h slave2 -uroot -panjian123 –scm-host master scm scm scm</p><p>-h后是mysql的主机名</p><p>–scm-host 后是SCM server的主机名（主节点）</p><p>如果有报错，一般是msyql的权限设置没设好，root不能远程登录</p><ul><li>5 准备Parcels，用以安装CDH5</li></ul><p>将CHD5相关的Parcel包放到主节点的/opt/cloudera/parcel-repo/目录中</p><p>相关的文件如下：</p><p>CDH-5.11.0-1.cdh5.11.0.p0.34-el6.parcel</p><p>CDH-5.11.0-1.cdh5.11.0.p0.34-el6.parcel.sha1</p><p>manifest.json</p><p>最后将CDH-5.1.3-1.cdh5.1.3.p0.12-el6.parcel.sha1，重命名为CDH-5.11.0-1.cdh5.11.0.p0.34-el6.parcel.sha</p><ul><li>6 启动脚本</li></ul><p>主节点：通过/opt/cm-5.11.0/etc/init.d/cloudera-scm-server start启动服务端。</p><p>所有节点（包括主节点）：通过/opt/cm-5.11.0/etc/init.d/cloudera-scm-agent start启动Agent服务</p><p>注：</p><p>停止可以用/init.d/cloudera-scm-server stop</p><p>启动成功后，可以用ps –ef|grep cloudera看到这2个进程</p><p>cloudera-scm-agent  的log如果报错:</p><p>[04/Sep/2017 16:29:38 +0000] 2983 MainThread agent        ERROR    Heartbeating to master:7182 failed.</p><p>一般是主节点防火墙没关闭</p><ul><li>7 CDH5的安装配置</li></ul><p><a href="http://192.168.10.111:7180/cmf/login" target="_blank" rel="noopener">http://192.168.10.111:7180/cmf/login</a></p><p>默认的用户名和密码均为admin</p><h1 id="注：CDH安装失败后，重新安装："><a href="#注：CDH安装失败后，重新安装：" class="headerlink" title="注：CDH安装失败后，重新安装："></a>注：CDH安装失败后，重新安装：</h1><p>1) 主节点关闭cloudera-scm-server。所有节点关闭cloudera-scm-agent</p><p>2）删除Agent节点目录</p><p>rm -rf /opt/cm-5.11.0 </p><pre><code>rm -rf /opt/cloudera</code></pre><p>删除主节点目录</p><p>rm -rf /opt/cm-5.11.0</p><p>rm -rf /opt/cloudera</p><p>删除主节点，/var/log/下cloudera相关的目录</p><p>删除主节点，/var/lib/下cloudera相关的目录</p><p>3) 清空CM数据库</p><pre><code>进入Mysql数据库，然后drop database cm;</code></pre><p>4) 删除各节点namenode和datanode节点信息</p><pre><code># rm -rf /opt/dfs/nn/*# rm -rf /opt/dfs/dn/*</code></pre><p>5) 重新安装CDH，主节点解解压：tar xzvf cloudera-manager*.tar.gz将解压后的cm-5.11.0和cloudera目录放到/opt目录下。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、准备&quot;&gt;&lt;a href=&quot;#一、准备&quot; class=&quot;headerlink&quot; title=&quot;一、准备&quot;&gt;&lt;/a&gt;一、准备&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;1.官网下载Cloudera Manager和CDH 5.11&lt;br&gt;Cloudera Manager下载地址：
      
    
    </summary>
    
      <category term="搭建" scheme="http://www.wenchong.top/categories/%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="CDH" scheme="http://www.wenchong.top/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>spark伪分布安装</title>
    <link href="http://www.wenchong.top/2018/02/15/spark%E4%BC%AA%E5%88%86%E5%B8%83%E5%AE%89%E8%A3%85/"/>
    <id>http://www.wenchong.top/2018/02/15/spark伪分布安装/</id>
    <published>2018-02-14T16:00:00.000Z</published>
    <updated>2018-06-26T07:26:43.239Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>本文选用：centos6.5、zookeeper-3.4.11、hadoop2.8.0、jdk1.8.0、Scala2.12.1</p></li><li><p>前言.Spark简介和hadoop的区别 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。 </span><br><span class="line">1.架构不同。 </span><br><span class="line">Hadoop是对大数据集进行分布式计算的标准工具。提供了包括工具和技巧在内的丰富的生态系统，允许使用相对便宜的商业硬件集群进行超级计算机级别的计算。 </span><br><span class="line">Spark使用函数式编程范式扩展了MapReduce编程模型以支持更多计算类型，可以涵盖广泛的工作流。且需要一个第三方的分布式存储系统作为依赖。 </span><br><span class="line">2.处理对象不同 </span><br><span class="line">Spark处理数据的方式不一样，会比MapReduce快上很多。MapReduce是分步对数据进行处理的: 从集群中读取数据，进行一次处理，将结果写到集群，从集群中读取更新后的数据，进行下一次的处理，将结果写到集群,反观Spark，它会在内存中以接近“实时”的时间完成所有的数据分析：从集群中读取数据，完成所有必须的分析处理，将结果写回集群.所以Hadoop适合处理静态数据，而Spark适合对流数据进行分析。 </span><br><span class="line">3.速度 </span><br><span class="line">Spark基于内存：Spark使用内存缓存来提升性能，因此进行交互式分析也足够快速(就如同使用Python解释器，与集群进行交互一样)。缓存同时提升了迭代算法的性能，这使得Spark非常适合数据理论任务，特别是机器学习。 </span><br><span class="line">Hadoop基于磁盘：MapReduce要求每隔步骤之间的数据要序列化到磁盘，这意味着MapReduce作业的I/O成本很高，导致交互分析和迭代算法（iterative algorithms）开销很大。而事实是，几乎所有的最优化和机器学习都是迭代的。 </span><br><span class="line">4.灾难恢复 </span><br><span class="line">Hadoop将每次处理后的数据都写入到磁盘上，所以其天生就能很有弹性的对系统错误进行处理。 </span><br><span class="line">Spark的数据对象存储在分布于数据集群中的叫做弹性分布式数据集(RDD: Resilient Distributed Dataset)中。这些数据对象既可以放在内存，也可以放在磁盘，所以RDD同样也可以提供完成的灾难恢复功能。</span><br></pre></td></tr></table></figure></li><li><p>spark安装模式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">spark有以下几种安装模式，每种安装模式都有自己不同的优点和长处。 </span><br><span class="line">local(本地模式)： </span><br><span class="line">常用于本地开发测试，本地还分为local单线程和local-cluster多线程; </span><br><span class="line">standalone(集群模式)： </span><br><span class="line">典型的Mater/slave模式，Master可能有单点故障的；Spark支持ZooKeeper来实现 HA。 </span><br><span class="line">on yarn(集群模式)： </span><br><span class="line">运行在 yarn 资源管理器框架之上，由 yarn 负责资源管理，Spark 负责任务调度和计算。 </span><br><span class="line">on mesos(集群模式)： </span><br><span class="line">运行在 mesos 资源管理器框架之上，由 mesos 负责资源管理，Spark 负责任务调度和计算。 </span><br><span class="line">on cloud(集群模式)： </span><br><span class="line">比如 AWS 的 EC2，使用这个模式能很方便的访问 Amazon的 S3;Spark 支持多种分布式存储系统：HDFS 和 S3。 </span><br><span class="line">目前Apache Spark支持三种分布式部署方式，分别是standalone、Spark on mesos和 spark on YARN，其中，</span><br><span class="line">第一种类似于MapReduce 1.0所采用的模式，内部实现了容错性和资源管理，后两种则是未来发展的趋势，部分容错性和资源管理交由统一的资源管理系统完成：</span><br><span class="line">让Spark运行在一个通用的资源管理系统之上，这样可以与其他计算框架，比如MapReduce，公用一个集群资源，最大的好处是降低运维成本和提高资源利用率（资源按需分配）。</span><br></pre></td></tr></table></figure></li></ul><p>【步骤】</p><ul><li><p>1.安装scala </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">上传scala包，解压缩 </span><br><span class="line">配置环境变量SCALA_HOME </span><br><span class="line">source /etc/profile使得生效 </span><br><span class="line">验证scala安装情况 </span><br><span class="line">scala -version </span><br><span class="line">及上scala 安装完成</span><br></pre></td></tr></table></figure></li><li><ol><li>伪分布式Spark安装部署<br>下载地址：<a href="http://saprk.apache.org" target="_blank" rel="noopener">http://saprk.apache.org</a><br>解压<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf saprk*******</span><br><span class="line">cd saprk*******/conf</span><br></pre></td></tr></table></figure></li></ol></li></ul><p>拷贝配置文件,修改为运行主机名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp slaves.template slaves</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cp spark-env.sh.template spark-env.sh </span><br><span class="line">vim spark-env.sh </span><br><span class="line">--------------------</span><br><span class="line">进行以下配置 </span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0 </span><br><span class="line">export SCALA_HOME=/usr/local/scala-2.12.1 </span><br><span class="line">export SPARK_WORKER_MEMORY=1G </span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop2.8.0</span><br><span class="line">export HADOOP_CONF_DIR=/usr/local/hadoop2.8.0/etc/hadoop </span><br><span class="line">export SPARK_MASTER_IP=192.168.174.175</span><br></pre></td></tr></table></figure><ul><li>3.启动spark<br>(1)先启动hadoop 环境 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/hadoop/sbin# start-all.sh</span><br></pre></td></tr></table></figure></li></ul><p>（2）启动spark环境<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/spark-2.0.1/sbin# ./start-all.sh</span><br></pre></td></tr></table></figure></p><p>[注] 如果使用start-all.sh时候会重复启动hadoop配置，需要./在当前工作目录下执行脚本文件。<br>jps 观察进程 多出 worker 和 mater 两个进程</p><p>　　　　　　　</p><ul><li>4、查看spark的web控制页面 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.137.133:8080/ </span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">* 5.进入交互</span><br></pre></td></tr></table></figure></li></ul><p>/bin/spark-shell<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 6.查看sparl-shell web界面</span><br></pre></td></tr></table></figure></p><p><a href="http://192.168.137.133:4040/jobs/" target="_blank" rel="noopener">http://192.168.137.133:4040/jobs/</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 7.Spark测试</span><br></pre></td></tr></table></figure></p><p>1.在HDFS上建立目录<br>hadoop fs -mkdir -p /data<br>hadoop fs -ls /data/ 可以查看<br>hadoop fs -mkdir -p /data/input 建立文件夹input<br>将本地目录传送到HDFS上<br>hadoop fs -put /home/santiago/data/spark/spark_test.txt /data/input<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">测试spark </span><br><span class="line">val text_in = sc.textFile(&quot;/data/input/hbase.txt&quot;)</span><br><span class="line">执行后打印：</span><br><span class="line">println(text_in.count())</span><br><span class="line">观察job网页</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;本文选用：centos6.5、zookeeper-3.4.11、hadoop2.8.0、jdk1.8.0、Scala2.12.1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;前言.Spark简介和hadoop的区别 &lt;/p&gt;
&lt;figure class=&quot;highl
      
    
    </summary>
    
      <category term="spark" scheme="http://www.wenchong.top/categories/spark/"/>
    
    
      <category term="spark" scheme="http://www.wenchong.top/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper搭建</title>
    <link href="http://www.wenchong.top/2018/02/15/zookeeper%E5%AE%89%E8%A3%85/"/>
    <id>http://www.wenchong.top/2018/02/15/zookeeper安装/</id>
    <published>2018-02-14T16:00:00.000Z</published>
    <updated>2018-05-18T14:23:32.913Z</updated>
    
    <content type="html"><![CDATA[<ul><li>本文选用：centos6.5、zookeeper-3.4.6<h3 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h3>【软件】准备好jdk环境，此次我们的环境是open_jdk1.8.0_101<br>　　　　zookeeper-3.4.6.tar.gz<br>【步骤】</li><li><ol><li>准备条件<br>如果有内部dns或者外网有域名，则直接使用域名<br>如果没有需要修改/etc/hosts文件，或者直接使用IP</li></ol></li></ul><p>集群规划</p><p>主机类型 IP地址  域名<br>zookeeper1 192.168.1.1zookeeper1.chinasoft.com<br>zookeeper2 192.168.1.2zookeeper2.chinasoft.com<br>zookeeper3 192.168.1.3zookeeper3.chinasoft.com</p><p>注意：zookeeper因为有主节点和从节点的关系，所以部署的集群台数最好为奇数个，否则可能出现脑裂导致服务异常</p><ul><li><ol><li>安装<br>下载地址：<a href="http://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/" target="_blank" rel="noopener">http://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/</a><br>解压</li></ol></li></ul><p>tar -zxf zookeeper-3.4.6.tar.gz<br>cd zookeeper-3.4.6</p><p>拷贝配置文件，修改完成后分发给其他节点<br>cd /data/zookeeper-3.4.6/<br>cp zoo_sample.cfg zoo.cfg</p><p>cat zoo.cfg<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/data/zookeeper-3.4.6/data</span><br><span class="line">dataLogDir=/data/zookeeper-3.4.6/logs</span><br><span class="line">clientPort=2181</span><br><span class="line">server.1=u04rtv01.yaya.corp:2888:3888</span><br><span class="line">server.2=u04rtv02.yaya.corp:2888:3888</span><br><span class="line">server.3=u04rtv03.yaya.corp:2888:3888</span><br></pre></td></tr></table></figure></p><ul><li>3.创建data和Log文件夹<br>mkdir /data/zookeeper-3.4.6/data<br>mkdir /data/zookeeper-3.4.6/logs</li></ul><p>　　　　　　　</p><ul><li><p>4、在zoo.cfg中的dataDir指定的目录下，新建myid文件。<br>例如：$ZK_INSTALL/data下，新建myid。在myid文件中输入1。表示为server.1。<br>如果为snapshot/d_2，则myid文件中的内容为 2，依此类推。 </p></li><li><p>启动：在集群中的每台主机上执行如下命令<br>bin/zkServer.sh start </p></li><li><p>查看状态，可以看到其中一台为主节点，其他两台为从节点：<br>bin/zkServer.sh status</p></li><li><p>主节点：<br>./zkServer.sh status</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JMX enabled by default</span><br><span class="line">Using config: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfg</span><br><span class="line">Mode: leader</span><br></pre></td></tr></table></figure></li><li><p>从属节点：<br>./zkServer.sh status</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JMX enabled by default</span><br><span class="line">Using config: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure></li><li><p>停止：<br>bin/zkServer.sh stop</p></li></ul><p>连接：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/zkCli.sh -server zookeeper1:2181 </span><br><span class="line">bin/zkCli.sh -server zookeeper2:2181 </span><br><span class="line">bin/zkCli.sh -server zookeeper3:2181</span><br></pre></td></tr></table></figure></p><ul><li>报错：<br>原因就是没有在dataDir目录下创建myid文件并且赋值(如1、2、3分别代表集群中的server1,server2,server3)</li></ul><p>2016-08-22 17:55:16,145 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfg<br>2016-08-22 17:55:16,150 [myid:] - INFO  [main:QuorumPeerConfig@340] - Defaulting to majority quorums<br>2016-08-22 17:55:16,150 [myid:] - ERROR [main:QuorumPeerMain@85] - Invalid config, exiting abnormally<br>org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfg<br>        at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:123)<br>        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:101)<br>        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)<br>Caused by: java.lang.IllegalArgumentException: /data/yunva/zookeeper-3.4.6/data/myid file is missing<br>        at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parseProperties(QuorumPeerConfig.java:350)<br>        at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:119)<br>        … 2 more<br>Invalid config, exiting abnormally</p><h3 id="单机部署——适用于开发测试"><a href="#单机部署——适用于开发测试" class="headerlink" title="单机部署——适用于开发测试"></a>单机部署——适用于开发测试</h3><p>tar -zxvf zookeeper-3.4.6.tar.gz<br>cd zookeeper-3.4.6/conf<br>cp zoo_sample.cfg zoo.cfg</p><ul><li><p>创建日志目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /data/yunva/zookeeper-3.4.6/data</span><br><span class="line">mkdir /data/yunva/zookeeper-3.4.6/logs</span><br></pre></td></tr></table></figure></li><li><p>配置：conf/zoo.cfg</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000 </span><br><span class="line">initLimit=10 </span><br><span class="line">syncLimit=5 </span><br><span class="line">dataDir=/data/yunva/zookeeper-3.4.6/logs</span><br><span class="line">dataLogDir=/data/yunva/zookeeper-3.4.6/logs</span><br><span class="line">clientPort=2181</span><br></pre></td></tr></table></figure><p>#自动清除日志文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">autopurge.snapRetainCount=20</span><br><span class="line">autopurge.purgeInterval=48</span><br></pre></td></tr></table></figure></p><ul><li>启动：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/zkServer.sh start</span><br></pre></td></tr></table></figure><ul><li><p>连接到Zookeeper：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">bin/zkCli.sh -server 127.0.0.1:2181  适用于Java开发</span><br></pre></td></tr></table></figure></li><li><p>查看状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/zkServer.sh status</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfg</span><br><span class="line">Mode: standalone</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;本文选用：centos6.5、zookeeper-3.4.6&lt;h3 id=&quot;集群部署&quot;&gt;&lt;a href=&quot;#集群部署&quot; class=&quot;headerlink&quot; title=&quot;集群部署&quot;&gt;&lt;/a&gt;集群部署&lt;/h3&gt;【软件】准备好jdk环境，此次我们的环境是open_
      
    
    </summary>
    
      <category term="zookeeper" scheme="http://www.wenchong.top/categories/zookeeper/"/>
    
    
      <category term="zookeepere" scheme="http://www.wenchong.top/tags/zookeepere/"/>
    
  </entry>
  
  <entry>
    <title>Structured Streaming模型</title>
    <link href="http://www.wenchong.top/2018/02/15/Struct%20Streaming/"/>
    <id>http://www.wenchong.top/2018/02/15/Struct Streaming/</id>
    <published>2018-02-14T16:00:00.000Z</published>
    <updated>2018-07-12T02:03:23.213Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概观"><a href="#概观" class="headerlink" title="概观"></a>概观</h3><p>结构化流式是一种基于Spark SQL的可扩展且容错的流处理引擎。可以像表达静态数据的批处理计算一样表达流式计算。park SQL引擎将负责逐步和连续地运行它，并在流数据继续到达时更新最终结果。<br>自Spark 2.3以来，我们引入了一种称为连续处理的新型低延迟处理模式，它可以实现低至1毫秒的端到端延迟，并且具有至少一次保证。</p><ul><li>开始示例<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.sql.*;</span><br><span class="line">import org.apache.spark.sql.streaming.StreamingQuery;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">//spark程序入口</span><br><span class="line">SparkSession spark = SparkSession.bulider().appname(&quot;JavaStructuredNetworkWordCount&quot;).getOrCrete();</span><br><span class="line"></span><br><span class="line">//创建一个流数据框，表示从侦听localhost：9999的服务器接受文本数据，并转换为DataFrame以计算字数。</span><br><span class="line">Dataset&lt;Row&gt; lines = spark.readStream().format(&quot;socket&quot;).option(&quot;host&quot;,&quot;localhsot&quot;).option(&quot;port&quot;,9999).load();</span><br><span class="line"></span><br><span class="line">Dataset&lt;String&gt; words = lines.as(Encoders.STRING()).flatMap((FlatMapFunction&lt;String, String&gt;)x-&gt;Arrays.asList(x.split(&quot; &quot;)).iterator(), Encoders.STRING());</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; wordCounts = words.groupBy(&quot;value&quot;).count();</span><br><span class="line"></span><br><span class="line">//开始运行并将结果打印在控制台</span><br><span class="line">StreamingQuery query = wordCounts.writeStream().outputMode(&quot;complete&quot;).format(&quot;console&quot;).start();</span><br><span class="line">query.awaitTermination();//执行此代码后，流式计算将在后台启动。该query对象是该活动流式查询的句柄，我们决定等待查询终止，awaitTermination()以防止在查询处于活动状态时退出该进程。</span><br></pre></td></tr></table></figure></li></ul><h3 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h3><p>结构化流中的关键思想是将实时数据流视为连续追加的表。</p><ul><li>基本概念<br>将输入数据流视为“输入表”。到达流的每个数据项都像一个新行被附加到输入表。<br>对输入的查询将生成“结果表”。每个触发间隔（例如，每1秒），新行将附加到输入表，最终更新结果表。每当结果表更新时，我们都希望将更改的结果行写入外部接收器。</li><li>“输出”定义为写入外部存储器的内容。输出可以以不同的模式定义：<br>Complete Mode -  整个更新的结果表将写入外部存储器。由存储连接器决定如何处理整个表的写入。<br>Append Mode - 自上次触发后，只有结果表中附加的新行才会写入外部存储器。这仅适用于预期结果表中的现有行不会更改的查询。<br>Update Mode -仅将自上次触发后在结果表中更新的行写入外部存储（自Spark 2.1.1起可用）。请注意，这与完整模式的不同之处在于此模式仅输出自上次触发后已更改的行。如果查询不包含聚合，则它将等同于追加模式。</li><li>请注意，Structured Streaming不会实现整个表。它从流数据源读取最新的可用数据，逐步处理以更新结果，然后丢弃源数据。它仅保留更新结果所需的最小中间状态数据<h3 id="输入源"><a href="#输入源" class="headerlink" title="输入源"></a>输入源</h3>文件来源    path：输入目录的路径，并且对所有文件格式都是通用的。 <pre><code>maxFilesPerTrigger：每个触发器中要考虑的最大新文件数（默认值：无最大值） latestFirst：是否先处理最新的新文件，当存在大量积压文件时有用（默认值：false）fileNameOnly：是否基于以下方法检查新文件只有文件名而不是完整路径（默认值：false）。将此设置为“true”时，以下文件将被视为同一文件，因为它们的文件名“dataset.txt”是相同的： “file：///dataset.txt” “s3：// a / dataset.txt“ ”s3n：//a/b/dataset.txt“ ”s3a：//a/b/c/dataset.txt“ </code></pre></li></ul><p>套接字源    host：要连接的主机，必须指定<br>            port：要连接的端口，必须指定<br>Rate Source    rowsPerSecond（例如100，默认值：1）：每秒应生成多少行。<br>            ampUpTime（例如5s，默认值：0s）：在生成速度变为之前加速多长时间rowsPerSecond。使用比秒更精细的粒度将被截断为整数秒。<br>            numPartitions（例如10，默认值：Spark的默认并行性）：生成的行的分区号。源将尽力达到rowsPerSecond，但查询可能受资源约束，并且numPartitions可以进行调整以帮助达到所需的速度。</p><ul><li>示例<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">SparkSession spark=.....</span><br><span class="line"></span><br><span class="line">//read text from socket</span><br><span class="line">Dataset&lt;Row&gt; socketDF = spark.readStream().format(&quot;socket&quot;).option(&quot;host&quot;,&quot;localhsot&quot;).option(&quot;port&quot;,9999).load();</span><br><span class="line"></span><br><span class="line">socketDF.isStreaming();//return true for Dataframes that have streaming sources</span><br><span class="line"></span><br><span class="line">socketDF.printSchema();</span><br><span class="line"></span><br><span class="line">// Read all the csv files written atomically in a directory</span><br><span class="line">StructType userSchema = new StructType().add(&quot;name&quot;, &quot;string&quot;).add(&quot;age&quot;, &quot;integer&quot;);</span><br><span class="line">Dataset&lt;Row&gt; csvDF = spark</span><br><span class="line">  .readStream()</span><br><span class="line">  .option(&quot;sep&quot;, &quot;;&quot;)</span><br><span class="line">  .schema(userSchema)      // Specify schema of the csv files</span><br><span class="line">  .csv(&quot;/path/to/directory&quot;);// Equivalent to format(&quot;csv&quot;).load(&quot;/path/to/directory&quot;)</span><br></pre></td></tr></table></figure></li></ul><h3 id="流式传输DataFrames-Datasets的操作"><a href="#流式传输DataFrames-Datasets的操作" class="headerlink" title="流式传输DataFrames / Datasets的操作"></a>流式传输DataFrames / Datasets的操作</h3><p>您可以将各种操作上的流DataFrames /数据集-从无类型，类似于SQL的操作（例如select，where，groupBy），为键入RDD般的操作（例如map，filter，flatMap）。</p><ul><li>基本操作 - 选择，投影，聚合<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.api.java.function.*;</span><br><span class="line">import org.apache.spark.sql.*;</span><br><span class="line">import org.apache.spark.sql.expressions.javalang.typed;</span><br><span class="line">import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;</span><br><span class="line"></span><br><span class="line">public class DeviceData &#123;</span><br><span class="line">  private String device;</span><br><span class="line">  private String deviceType;</span><br><span class="line">  private Double signal;</span><br><span class="line">  private java.sql.Date time;</span><br><span class="line">  ...</span><br><span class="line">  // Getter and setter methods for each field</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df = ...;    // streaming DataFrame with IOT device data with schema &#123; device: string, type: string, signal: double, time: DateType &#125;</span><br><span class="line">Dataset&lt;DeviceData&gt; ds = df.as(ExpressionEncoder.javaBean(DeviceData.class)); // streaming Dataset with IOT device data</span><br><span class="line"></span><br><span class="line">// Select the devices which have signal more than 10</span><br><span class="line">df.select(&quot;device&quot;).where(&quot;signal &gt; 10&quot;); // using untyped APIs</span><br><span class="line">ds.filter((FilterFunction&lt;DeviceData&gt;) value -&gt; value.getSignal() &gt; 10)</span><br><span class="line">  .map((MapFunction&lt;DeviceData, String&gt;) value -&gt; value.getDevice(), Encoders.STRING());</span><br><span class="line"></span><br><span class="line">// Running count of the number of updates for each device type</span><br><span class="line">df.groupBy(&quot;deviceType&quot;).count(); // using untyped API</span><br><span class="line"></span><br><span class="line">// Running average signal for each device type</span><br><span class="line">ds.groupByKey((MapFunction&lt;DeviceData, String&gt;) value -&gt; value.getDeviceType(), Encoders.STRING())</span><br><span class="line">  .agg(typed.avg((MapFunction&lt;DeviceData, Double&gt;) value -&gt; value.getSignal()));</span><br></pre></td></tr></table></figure></li></ul><p>您还可以将流式DataFrame / Dataset注册为临时视图，然后在其上应用SQL命令。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(&quot;updates&quot;);</span><br><span class="line">spark.sql(&quot;select count(*) from updates&quot;);  // returns another streaming DF</span><br></pre></td></tr></table></figure></p><ul><li>注意，您可以使用确定DataFrame / Dataset是否具有流数据df.isStreaming。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.isStreaming()</span><br></pre></td></tr></table></figure></li></ul><h3 id="事件时间的窗口操作"><a href="#事件时间的窗口操作" class="headerlink" title="事件时间的窗口操作"></a>事件时间的窗口操作</h3><p>使用结构化流式传输时，滑动事件时间窗口上的聚合非常简单，并且与分组聚合非常相似。在分组聚合中，为用户指定的分组列中的每个唯一值维护聚合值（例如计数）。在基于窗口的聚合的情况下，为每个窗口维护一行的事件时间的聚合值。<br>计算10分钟内的单词，每5分钟更新一次。也就是说，在10分钟窗口12：00-12：10,12：05-12：15,12：10-12：20等之间收到的单词数量。请注意，12：00 - 12:10表示数据在12:00之后但在12:10之前到达。现在，考虑一下在12:07收到的一个字。这个词应该增加对应于两个窗口12:00 - 12:10和12:05 - 12:15的计数。因此，计数将由两者，分组键（即单词）和窗口（可以从事件时间计算）索引。<br>窗口类似于分组，因此在代码中，您可以使用groupBy()和window()操作来表示窗口化聚合。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; words = ... // streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span><br><span class="line"></span><br><span class="line">// Group the data by window and word and compute the count of each group</span><br><span class="line">Dataset&lt;Row&gt; windowedCounts = words.groupBy(</span><br><span class="line">  functions.window(words.col(&quot;timestamp&quot;), &quot;10 minutes&quot;, &quot;5 minutes&quot;),</span><br><span class="line">  words.col(&quot;word&quot;)</span><br><span class="line">).count();</span><br></pre></td></tr></table></figure></p><h3 id="处理延迟数据和水印"><a href="#处理延迟数据和水印" class="headerlink" title="处理延迟数据和水印"></a>处理延迟数据和水印</h3><p>应用程序在12:11可以接收在12:04（即事件时间）生成的单词。应用程序应使用时间12:04而不是12:11来更新窗口的旧计数12:00 - 12:10。这在我们基于窗口的分组中自然发生 - 结构化流可以长时间维持部分聚合的中间状态，以便后期数据可以正确更新旧窗口的聚合.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; words = ... // streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span><br><span class="line"></span><br><span class="line">// Group the data by window and word and compute the count of each group</span><br><span class="line">Dataset&lt;Row&gt; windowedCounts = words</span><br><span class="line">    .withWatermark(&quot;timestamp&quot;, &quot;10 minutes&quot;)</span><br><span class="line">    .groupBy(</span><br><span class="line">        functions.window(words.col(&quot;timestamp&quot;), &quot;10 minutes&quot;, &quot;5 minutes&quot;),</span><br><span class="line">        words.col(&quot;word&quot;))</span><br><span class="line">    .count();</span><br></pre></td></tr></table></figure></p><ul><li>请注意，withWatermark在非流式数据集上使用是no-op。</li><li>引擎等待“10分钟”以计算延迟日期，然后丢弃窗口的中间状态&lt;水印，并将最终计数附加到结果表/接收器。</li><li>重要的是要注意，水印在聚合查询中清除状态必须满足以下条件（从Spark 2.1.1开始，将来可能会有变化）。<br>输出模式必须为Append或Update。完整模式要求保留所有聚合数据，因此不能使用水印来降低中间状态。<br>聚合必须具有事件时间列或window事件时间列上的a。<br>withWatermark必须在与聚合中使用的时间戳列相同的列上调用。例如， df.withWatermark(“time”, “1 min”).groupBy(“time2”).count()在追加输出模式下无效，因为水印是在与聚合列不同的列上定义的。<br>withWatermark必须在聚合之前调用要使用的水印细节。例如，df.groupBy(“time”).count().withWatermark(“time”, “1 min”)在追加输出模式下无效。<br>水印延迟（设置为withWatermark）为“2小时”可确保引擎永不丢弃延迟小于2小时的任何数据。换句话说，任何不到2小时（在事件时间方面）数据的数据都保证汇总到那时处理的最新数据。</li><li>但是，保证只在一个方向严格。延迟2小时以上的数据不能保证被丢弃; 它可能会也可能不会聚合。数据更加延迟，引擎处理它的可能性更小。<h3 id="流静态连接"><a href="#流静态连接" class="headerlink" title="流静态连接"></a>流静态连接</h3>自Spark 2.0引入以来，Structured Streaming支持流和静态DataFrame / Dataset之间的连接（内连接和某种类型的外连接）。这是一个简单的例子。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; staticDf = spark.read(). ...;</span><br><span class="line">Dataset&lt;Row&gt; streamingDf = spark.readStream(). ...;</span><br><span class="line">streamingDf.join(staticDf, &quot;type&quot;);         // inner equi-join with a static DF</span><br><span class="line">streamingDf.join(staticDf, &quot;type&quot;, &quot;right_join&quot;);  // right outer join with a static DF</span><br></pre></td></tr></table></figure></li></ul><h3 id="流式重复数据删除"><a href="#流式重复数据删除" class="headerlink" title="流式重复数据删除"></a>流式重复数据删除</h3><p>使用withWatermark - 如果重复记录的到达时间有上限，则可以在事件时间列上定义水印，并使用guid和事件时间列进行重复数据删除。查询将使用水印从过去的记录中删除旧的状态数据，这些记录不再需要重复。这限制了查询必须维护的状态量。<br>没有withWatermark - 由于重复记录可能到达时没有界限，查询将来自所有过去记录的数据存储为状态。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; streamingDf = spark.readStream(). ...;  // columns: guid, eventTime, ...</span><br><span class="line"></span><br><span class="line">// Without watermark using guid column</span><br><span class="line">streamingDf.dropDuplicates(&quot;guid&quot;);</span><br><span class="line"></span><br><span class="line">// With watermark using guid and eventTime columns</span><br><span class="line">streamingDf</span><br><span class="line">  .withWatermark(&quot;eventTime&quot;, &quot;10 seconds&quot;)</span><br><span class="line">  .dropDuplicates(&quot;guid&quot;, &quot;eventTime&quot;);</span><br></pre></td></tr></table></figure></p><h3 id="启动流式查询"><a href="#启动流式查询" class="headerlink" title="启动流式查询"></a>启动流式查询</h3><ul><li>一旦定义了最终结果DataFrame / Dataset，剩下的就是启动流式计算。为此，您必须使用返回的DataStreamWriter （Scala / Java / Python文档）Dataset.writeStream()。您必须在此界面中指定以下一项或多项。<br>输出接收器的详细信息：数据格式，位置等。</li></ul><p>输出模式：指定写入输出接收器的内容。</p><p>查询名称：（可选）指定查询的唯一名称以进行标识。</p><p>触发间隔：可选，指定触发间隔。如果未指定，则系统将在前一处理完成后立即检查新数据的可用性。如果由于先前的处理尚未完成而错过了触发时间，则系统将立即触发处理。</p><p>检查点位置：对于可以保证端到端容错的某些输出接收器，请指定系统写入所有检查点信息的位置。这应该是与HDFS兼容的容错文件系统中的目录。</p><ul><li>输出模式<br>Append mode (default) - 这是默认模式，其中只有自上次触发后添加到结果表的新行将输出到接收器。仅支持那些添加到结果表中的行永远不会更改的查询。因此，此模式保证每行仅输出一次（假设容错接收器）。例如，仅查询select， where，map，flatMap，filter，join，等会支持追加模式。</li></ul><p>Complete mode  - 每次触发后，整个结果表将输出到接收器。聚合查询支持此功能。</p><p>Update mode - （自Spark 2.1.1起可用）仅将结果表中自上次触发后更新的行输出到接收器。</p><h3 id="输出接收器"><a href="#输出接收器" class="headerlink" title="输出接收器"></a>输出接收器</h3><p>有几种类型的内置输出接收器。</p><ul><li><p>文件接收器 - 将输出存储到目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;parquet&quot;)        // can be &quot;orc&quot;, &quot;json&quot;, &quot;csv&quot;, etc.</span><br><span class="line">    .option(&quot;path&quot;, &quot;path/to/destination/dir&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure></li><li><p>Kafka sink - 将输出存储到Kafka中的一个或多个主题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;kafka&quot;)</span><br><span class="line">    .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">    .option(&quot;topic&quot;, &quot;updates&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure></li><li><p>Foreach接收器 - 对输出中的记录运行任意计算。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .foreach(...)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure></li><li><p>控制台接收器（用于调试） - 每次触发时将输出打印到控制台/标准输出。支持Append和Complete输出模式。这应该用于低数据量的调试目的，因为在每次触发后收集整个输出并将其存储在驱动程序的内存中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;console&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure></li><li><p>内存接收器（用于调试） - 输出作为内存表存储在内存中。支持Append和Complete输出模式。这应该用于低数据量的调试目的，因为整个输出被收集并存储在驱动程序的内存中。因此，请谨慎使用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;memory&quot;)</span><br><span class="line">    .queryName(&quot;tableName&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure></li><li><p>请注意，您必须调用start()实际开始执行查询。这将返回一个StreamingQuery对象，该对象是持续运行的执行的句柄。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">/ ========== DF with no aggregations ==========</span><br><span class="line">Dataset&lt;Row&gt; noAggDF = deviceDataDf.select(&quot;device&quot;).where(&quot;signal &gt; 10&quot;);</span><br><span class="line"></span><br><span class="line">// Print new data to console</span><br><span class="line">noAggDF</span><br><span class="line">  .writeStream()</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">// Write new data to Parquet files</span><br><span class="line">noAggDF</span><br><span class="line">  .writeStream()</span><br><span class="line">  .format(&quot;parquet&quot;)</span><br><span class="line">  .option(&quot;checkpointLocation&quot;, &quot;path/to/checkpoint/dir&quot;)</span><br><span class="line">  .option(&quot;path&quot;, &quot;path/to/destination/dir&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">// ========== DF with aggregation ==========</span><br><span class="line">Dataset&lt;Row&gt; aggDF = df.groupBy(&quot;device&quot;).count();</span><br><span class="line"></span><br><span class="line">// Print updated aggregations to console</span><br><span class="line">aggDF</span><br><span class="line">  .writeStream()</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">// Have all the aggregates in an in-memory table</span><br><span class="line">aggDF</span><br><span class="line">  .writeStream()</span><br><span class="line">  .queryName(&quot;aggregates&quot;)    // this query name will be the table name</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;memory&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select * from aggregates&quot;).show();   // interactively query in-memory table</span><br></pre></td></tr></table></figure></li></ul><h3 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h3><ul><li>流式查询的触发器设置定义流式数据处理的时间<br>以下是支持的方式<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.streaming.Trigger</span><br><span class="line"></span><br><span class="line">// Default trigger (runs micro-batch as soon as it can)</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">// ProcessingTime trigger with two-seconds micro-batch interval</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .trigger(Trigger.ProcessingTime(&quot;2 seconds&quot;))</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">// One-time trigger</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .trigger(Trigger.Once())</span><br><span class="line">  .start();</span><br><span class="line"></span><br><span class="line">// Continuous trigger with one-second checkpointing interval</span><br><span class="line">df.writeStream</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .trigger(Trigger.Continuous(&quot;1 second&quot;))</span><br><span class="line">  .start();</span><br></pre></td></tr></table></figure></li></ul><h3 id="管理流式查询"><a href="#管理流式查询" class="headerlink" title="管理流式查询"></a>管理流式查询</h3><ul><li><p>StreamingQuery启动查询时创建的对象可用于监视和管理查询。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">StreamingQuery query = df.writeStream().format(&quot;console&quot;).start();   // get the query object</span><br><span class="line"></span><br><span class="line">query.id();          // get the unique identifier of the running query that persists across restarts from checkpoint data</span><br><span class="line"></span><br><span class="line">query.runId();       // get the unique id of this run of the query, which will be generated at every start/restart</span><br><span class="line"></span><br><span class="line">query.name();        // get the name of the auto-generated or user-specified name</span><br><span class="line"></span><br><span class="line">query.explain();   // print detailed explanations of the query</span><br><span class="line"></span><br><span class="line">query.stop();      // stop the query</span><br><span class="line"></span><br><span class="line">query.awaitTermination();   // block until query is terminated, with stop() or with error</span><br><span class="line"></span><br><span class="line">query.exception();       // the exception if the query has been terminated with error</span><br><span class="line"></span><br><span class="line">query.recentProgress();  // an array of the most recent progress updates for this query</span><br><span class="line"></span><br><span class="line">query.lastProgress();    // the most recent progress update of this streaming query</span><br></pre></td></tr></table></figure></li><li><p>可以在单个SparkSession中启动任意数量的查询。它们将同时运行，共享群集资源。您可以使用sparkSession.streams()来获取StreamingQueryManager，可用于管理当前活动查询。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SparkSession spark = ...</span><br><span class="line"></span><br><span class="line">spark.streams().active();    // get the list of currently active streaming queries</span><br><span class="line"></span><br><span class="line">spark.streams().get(id);   // get a query object by its unique id</span><br><span class="line"></span><br><span class="line">spark.streams().awaitAnyTermination();   // block until any one of them terminates</span><br></pre></td></tr></table></figure></li></ul><h3 id="通过检查点从故障中恢复"><a href="#通过检查点从故障中恢复" class="headerlink" title="通过检查点从故障中恢复"></a>通过检查点从故障中恢复</h3><ul><li>如果发生故障或故意关机，您可以恢复先前查询的先前进度和状态，并从中断处继续。这是使用检查点和预写日志完成的。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">aggDF</span><br><span class="line">  .writeStream()</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .option(&quot;checkpointLocation&quot;, &quot;path/to/HDFS/dir&quot;)</span><br><span class="line">  .format(&quot;memory&quot;)</span><br><span class="line">  .start();</span><br></pre></td></tr></table></figure></li></ul><h3 id="连续处理"><a href="#连续处理" class="headerlink" title="连续处理"></a>连续处理</h3><ul><li>连续处理是Spark 2.3中引入的一种新的实验性流执行模式，可实现低（~1 ms）端到端延迟，并且至少具有一次容错保证。<br>要在连续处理模式下运行支持的查询，您只需指定一个连续触发器，并将所需的检查点间隔作为参数。例如，<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.streaming.Trigger;</span><br><span class="line">spark.readStream</span><br><span class="line">  .format(&quot;kafka&quot;)</span><br><span class="line">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">  .option(&quot;subscribe&quot;, &quot;topic1&quot;)</span><br><span class="line">  .load()</span><br><span class="line">  .selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)</span><br><span class="line">  .writeStream</span><br><span class="line">  .format(&quot;kafka&quot;)</span><br><span class="line">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">  .option(&quot;topic&quot;, &quot;topic1&quot;)</span><br><span class="line">  .trigger(Trigger.Continuous(&quot;1 second&quot;))  // only change in query</span><br><span class="line">  .start();</span><br></pre></td></tr></table></figure></li></ul><p>检查点间隔为1秒意味着连续处理引擎将每秒记录查询的进度。生成的检查点采用与微批处理引擎兼容的格式，因此可以使用任何触发器重新启动任何查询。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;概观&quot;&gt;&lt;a href=&quot;#概观&quot; class=&quot;headerlink&quot; title=&quot;概观&quot;&gt;&lt;/a&gt;概观&lt;/h3&gt;&lt;p&gt;结构化流式是一种基于Spark SQL的可扩展且容错的流处理引擎。可以像表达静态数据的批处理计算一样表达流式计算。park SQL引擎将负责
      
    
    </summary>
    
      <category term="spark" scheme="http://www.wenchong.top/categories/spark/"/>
    
    
      <category term="spark" scheme="http://www.wenchong.top/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>centos下sqoop环境搭建</title>
    <link href="http://www.wenchong.top/2018/02/15/centos%E4%B8%8Bsqoop%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>http://www.wenchong.top/2018/02/15/centos下sqoop环境搭建/</id>
    <published>2018-02-14T16:00:00.000Z</published>
    <updated>2018-05-18T14:19:23.815Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Sqoop是一个用来将Hadoop（Hive、HBase）和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库（例如：MySQL ,Oracle ,Postgres等）中的数据导入到Hadoop的HDFS中，也可以将HDFS的数据导入到关系型数据库中。</li></ul><h3 id="Sqoop安装"><a href="#Sqoop安装" class="headerlink" title="Sqoop安装"></a>Sqoop安装</h3><ul><li>1.下载Sqoop安装包<br>在Sqoop官网下载安装包，本次使用的是sqoop-1.4.6.bin<strong>hadoop-2.0.4-alpha.tar.gz安装在/usr/local目录下，下载地址为<a href="http://apache.fayea.com/sqoop/1.4.6/sqoop-1.4.6.bin" target="_blank" rel="noopener">http://apache.fayea.com/sqoop/1.4.6/sqoop-1.4.6.bin</a></strong>hadoop-2.0.4-alpha.tar.gz</li><li>2.解压Sqoop安装包<br>#进入sqoop安装目录<br>[hadoop@BigData ~]$ cd /usr/local<br>#解压sqoop安装包<br>[hadoop@BigData ~]$ tar -zxvf sqoop-1.4.6.bin<strong>hadoop-2.0.4-alpha.tar.gz<br>#删除sqoop安装包<br>[hadoop@BigData ~]$ rm -rf sqoop-1.4.6.bin</strong>hadoop-2.0.4-alpha.tar.gz<br>#重命名sqoop目录名<br>[hadoop@BigData ~]$ mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop</li><li>3.配置Sqoop环境变量<br>#配置Sqoop环境变量<br>[root@BigData ~]# vi /etc/profile<br>export SQOOP_HOME=/usr/local/sqoop<br>export PATH=$PATH:$SQOOP_HOME/bin<br>#保存之后记得source，使之前的配置生效<br>source /etc/profile</li><li>4.将关系型数据库驱动包放到sqoop/lib目录下<br>MySql：mysql-connector-java-5.1.30.jar<br>Oracle：ojdbc14.jar</li><li>5.修改Sqoop配置文件<br>[hadoop@BigData ~]$ mv sqoop-env-template.sh sqoop-env.sh<br>[hadoop@BigData ~]$ vi sqoop-env.sh<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#Set path to where bin/hadoop is available</span><br><span class="line">export HADOOP_COMMON_HOME=/usr/local/hadoop</span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/usr/local/hadoop</span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">export HBASE_HOME=/usr/local/hbase</span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">#Set the path for where zookeper config dir is</span><br><span class="line">export ZOOCFGDIR=/usr/local/zookeeper</span><br></pre></td></tr></table></figure></li></ul><p>到此，sqoop环境就已搭建成功！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Sqoop是一个用来将Hadoop（Hive、HBase）和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库（例如：MySQL ,Oracle ,Postgres等）中的数据导入到Hadoop的HDFS中，也可以将HDFS的数据导入到关系型数据库中。&lt;
      
    
    </summary>
    
      <category term="Sqoop" scheme="http://www.wenchong.top/categories/Sqoop/"/>
    
    
      <category term="sqoop" scheme="http://www.wenchong.top/tags/sqoop/"/>
    
  </entry>
  
  <entry>
    <title>spark Streaming(一)</title>
    <link href="http://www.wenchong.top/2018/02/15/spark%20Streaming%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://www.wenchong.top/2018/02/15/spark Streaming（一）/</id>
    <published>2018-02-14T16:00:00.000Z</published>
    <updated>2018-07-12T03:26:20.859Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Spark Streaming是核心Spark API的扩展，可实现实时数据流的可扩展，高吞吐量，容错流处理。数据可以从许多来源（如Kafka，Flume，Kinesis或TCP套接字）中获取，并且可以使用以高级函数表示的复杂算法进行处理map，例如reduce，join和window。最后，处理后的数据可以推送到文件系统，数据库和实时仪表板。实际上，您可以在数据流上应用Spark的 机器学习和 图形处理算法。</li><li>在内部，它的工作原理。Spark Streaming接收实时输入数据流并将数据分成批处理，然后由Spark引擎处理，以批量生成最终结果流。</li><li>Spark Streaming提供称为discretized stream或Dstream的高级抽象，表示连续的数据流。DStream可以从来自Kafka，Flume和Kinesis等源的输入数据流创建，也可以通过在其他DStream上应用高级操作来创建。在内部，DStream表示为一系列 RDD。</li><li>本指南向您展示如何使用DStreams开始编写Spark Streaming程序。<h3 id="一个快速的例子"><a href="#一个快速的例子" class="headerlink" title="一个快速的例子"></a>一个快速的例子</h3>在我们详细介绍如何编写自己的Spark Streaming程序之前，让我们快速了解一下简单的Spark Streaming程序是什么样的。假设我们想要计算从TCP套接字上侦听的数据服务器接收的文本数据中的字数。您需要做的就是如下。<br>首先，我们创建一个 JavaStreamingContext对象，它是所有流功能的主要入口点。我们使用两个执行线程创建一个本地StreamingContext，批处理间隔为1秒。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.*;</span><br><span class="line">import org.apache.spark.api.java.function.*;</span><br><span class="line">import org.apache.spark.streaming.*;</span><br><span class="line">import org.apache.spark.streaming.api.java.*;</span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">SparkConf conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;NetworkWordCount&quot;);</span><br><span class="line">JavaStreamingContext jssc = new JavaStreamingContext(conf,Duations.second(1));</span><br><span class="line"></span><br><span class="line">//使用此上下文，我们可以创建一个DStream来表示来自TCP源的流数据，指定为主机名（例如localhost）和端口（例如9999）。</span><br><span class="line">JavaReceiverInputDstream&lt;String&gt; lines = jssc.socketTextStream(&quot;localhost&quot;,9999);</span><br><span class="line"></span><br><span class="line">//此linesDStream表示将从数据服务器接收的数据流。此流中的每条记录都是一行文本。然后，我们想要将空格分割为单词。</span><br><span class="line">JavaDstream&lt;String&gt; words = lines.flatMap(x-&gt;Arrays.asList(x.split(&quot; &quot;)).iterator());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//flatMap是一个DStream操作，它通过从源DStream中的每个记录生成多个新记录来创建新的DStream。在这种情况下，每行将被分成多个单词，单词流表示为 wordsDStream。请注意，我们使用FlatMapFunction对象定义了转换 。正如我们将要发现的那样，Java API中有许多这样的便利类可以帮助定义DStream转换。</span><br><span class="line"></span><br><span class="line">//接下来，我们要计算这些单词。</span><br><span class="line">JavaPairDstream&lt;String,String&gt; pairs = words.mapToPair(s-&gt;new Tuple2&lt;&gt;(s,1));</span><br><span class="line">JavaPairDstream&lt;String,String&gt; wordCounts = pairs.reduceByKey((i1,i2)-&gt;i1+i2);</span><br><span class="line">wordCounts.print();</span><br><span class="line">//使用PairFunction 对象将wordsDStream进一步映射（一对一转换）到(word, 1)对的DStream。然后，使用Function2对象将其缩小以获得每批数据中的单词频率。最后，wordCounts.print()将打印每秒生成的一些计数。</span><br><span class="line"></span><br><span class="line">jssc.start();</span><br><span class="line">jssc.awaitTermination();</span><br></pre></td></tr></table></figure></li></ul><p>执行这些行时，Spark Streaming仅设置它在启动后将执行的计算，并且尚未启动实际处理。要在设置完所有转换后开始处理，我们最终调用start方法。</p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>详细介绍Spark Streaming的基础知识。</p><ul><li><p>与Spark类似，Spark Streaming可通过Maven Central获得。要编写自己的Spark Streaming程序，必须将以下依赖项添加到SBT或Maven项目中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.3.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li><li><p>如需要添加kafka、Flume、Kinesis等源中提取数据。必须将相应的包添加到maven中<br>###</p></li><li>初始化StreamingContext<br>要初始化Spark Streaming程序，必须创建一个StreamingContext对象，他是所有Spark Streaming功能的入口。<br>JavaStreamingContext可以从SparkConf对象中创建。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.*;</span><br><span class="line">import org.apache.spark.streaming.api.java.*;</span><br><span class="line"></span><br><span class="line">SparkConf conf = new sparkConf().setAppName(appname).setMaster(master);</span><br><span class="line">JavaStreamingContext ssc = new JavaStreamingContext(conf,new Duation(1000));</span><br></pre></td></tr></table></figure></li></ul><p>该appName参数是应用程序在群集UI上显示的名称。 master是Spark，Mesos或YARN群集URL，或在本地模式下运行的特殊“local [<em>]”字符串。实际上，当在群集上运行时，您不希望master在程序中进行硬编码，而是启动应用程序spark-submit并在那里接收它。但是，对于本地测试和单元测试，您可以传递“local [</em>]”以在进程中运行Spark Streaming。请注意，这会在内部创建一个JavaSparkContext（所有Spark功能的起点），可以作为访问ssc.sparkContext。</p><ul><li>JavaStreamingContext目的还可以从现有的创建JavaSparkContext。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.streaming.api.java.*;</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = ...   //existing JavaSparkContext</span><br><span class="line">JavaStreamingContext ssc = new JavaStreamingContext(sc, Durations.seconds(1));</span><br></pre></td></tr></table></figure><ul><li>定义上下文后，您必须执行以下操作。</li></ul><p>通过创建输入DStreams来定义输入源。<br>通过将转换和输出操作应用于DStream来定义流式计算。<br>开始接收数据并使用它进行处理streamingContext.start()。<br>等待处理停止（手动或由于任何错误）使用streamingContext.awaitTermination()。<br>可以使用手动停止处理streamingContext.stop()。</p><ul><li>要记住的要点：<br>一旦启动了上下文，就不能设置或添加新的流式计算。<br>上下文停止后，无法重新启动。<br>在JVM中只能同时激活一个StreamingContext。<br>StreamingContext上的stop（）也会停止SparkContext。要仅停止StreamingContext，请将stop()called 的可选参数设置stopSparkContext为false。<br>只要在创建下一个StreamingContext之前停止前一个StreamingContext（不停止SparkContext），就可以重复使用SparkContext来创建多个StreamingContexts。</li></ul><h3 id="离散流（DStreams）"><a href="#离散流（DStreams）" class="headerlink" title="离散流（DStreams）"></a>离散流（DStreams）</h3><p>Discretized Stream或DStream是Spark Streaming提供的基本抽象。它表示连续的数据流，可以是从源接收的输入数据流，也可以是通过转换输入流生成的已处理数据流。在内部，DStream由一系列连续的RDD表示，这是Spark对不可变分布式数据集的抽象。</p><h3 id="输入DStreams和Receivers"><a href="#输入DStreams和Receivers" class="headerlink" title="输入DStreams和Receivers"></a>输入DStreams和Receivers</h3><ul><li>请注意，如果要在流应用程序中并行接收多个数据流，可以创建多个输入DStream（在“ 性能调整”部分中进一步讨论）。这将创建多个接收器，这些接收器将同时接收多个数据流。但请注意，Spark worker / executor是一个长期运行的任务，因此它占用了分配给Spark Streaming应用程序的其中一个核心。因此，重要的是要记住，Spark Streaming应用程序需要分配足够的内核（或线程，如果在本地运行）来处理接收的数据，以及运行接收器。</li><li>要记住的要点<br>在本地运行Spark Streaming程序时，请勿使用“local”或“local [1]”作为主URL。这两种方法都意味着只有一个线程将用于本地运行任务。如果您正在使用基于接收器的输入DStream（例如套接字，Kafka，Flume等），则单线程将用于运行接收器，不会留下任何线程来处理接收到的数据。因此，在本地运行时，始终使用“local [ n ]”作为主URL，其中n &gt;要运行的接收器数量.<br>将逻辑扩展到在集群上运行，分配给Spark Streaming应用程序的核心数必须大于接收器数。否则系统将接收数据，但无法处理数据。</li></ul><h4 id="基本来源"><a href="#基本来源" class="headerlink" title="基本来源"></a>基本来源</h4><ul><li>文件流<br>对于从与HDFS API兼容的任何文件系统（即HDFS，S3，NFS等）上的文件读取数据，可以创建DStream作为via StreamingContext.fileStream[KeyClass, ValueClass, InputFormatClass]。<br>文件流不需要运行接收器，因此不需要分配任何内核来接收文件数据。<br>对于简单的文本文件，最简单的方法是StreamingContext.textFileStream(dataDirectory)。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">streamingContext.fileStream&lt;KeyClass, ValueClass, InputFormatClass&gt;(dataDirectory);</span><br></pre></td></tr></table></figure></li></ul><p>对于文本文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">streamingContext.textFileStream(dataDirectory);</span><br></pre></td></tr></table></figure></p><ul><li><p>如何监控目录<br>Spark Streaming将监视目录dataDirectory并处理在该目录中创建的任何文件。<br>可以监视一个简单的目录，例如”hdfs://namenode:8040/logs/“。直接在这种路径下的所有文件将在发现时进行处理。<br>甲POSIX glob模式可以被提供，例如 “hdfs://namenode:8040/logs/2017/<em>“。这里，DStream将包含与模式匹配的目录中的所有文件。那就是：它是目录的模式，而不是目录中的文件。<br>所有文件必须采用相同的数据格式。<br>根据文件的修改时间而不是创建时间，文件被视为时间段的一部分。<br>处理完毕后，对当前窗口中文件的更改不会导致重新读取文件。即：忽略更新。<br>目录下的文件越多，扫描更改所需的时间就越长 - 即使没有修改过任何文件。<br>如果使用通配符来标识目录，例如”hdfs://namenode:8040/logs/2016-</em>“，重命名整个目录以匹配路径，则会将目录添加到受监视目录列表中。只有修改时间在当前窗口内的目录中的文件才会包含在流中。<br>调用FileSystem.setTimes() 时间戳是一种在稍后的窗口中拾取文件的方法，即使其内容未更改。</p></li><li><p>使用对象存储作为数据源<br>“完整”文件系统（如HDFS）会在创建输出流后立即在其文件上设置修改时间。打开文件时，即使在数据完全写入之前，它也可能包含在DStream- 之后 - 将忽略同一窗口中文件的更新。即：可能会遗漏更改，并从流中省略数据。</p></li></ul><p>要保证在窗口中选择更改，请将文件写入不受监视的目录，然后在关闭输出流后立即将其重命名为目标目录。如果重命名的文件在其创建窗口期间出现在扫描的目标目录中，则将拾取新数据。</p><p>相比之下，Amazon S3和Azure Storage等对象存储通常具有较慢的重命名操作，因为实际上是复制了数据。此外，重命名的对象可能将rename()操作的时间作为其修改时间，因此可能不被视为原始创建时间所暗示的窗口的一部分。</p><p>需要对目标对象存储进行仔细测试，以验证存储的时间戳行为是否与Spark Streaming所期望的一致。可能是直接写入目标目录是通过所选对象库流式传输数据的适当策略。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Spark Streaming是核心Spark API的扩展，可实现实时数据流的可扩展，高吞吐量，容错流处理。数据可以从许多来源（如Kafka，Flume，Kinesis或TCP套接字）中获取，并且可以使用以高级函数表示的复杂算法进行处理map，例如reduce
      
    
    </summary>
    
      <category term="spark Streaming" scheme="http://www.wenchong.top/categories/spark-Streaming/"/>
    
    
      <category term="spark" scheme="http://www.wenchong.top/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>docker安装</title>
    <link href="http://www.wenchong.top/2018/02/05/docker%E5%AE%89%E8%A3%85/"/>
    <id>http://www.wenchong.top/2018/02/05/docker安装/</id>
    <published>2018-02-05T13:09:56.999Z</published>
    <updated>2018-05-18T14:20:59.192Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1：关闭selinux"><a href="#1：关闭selinux" class="headerlink" title="1：关闭selinux"></a>1：关闭selinux</h2><p>临时关闭：setenforce 0<br>永久关闭：</p><h2 id="1-vi-etc-sysconfig-selinux"><a href="#1-vi-etc-sysconfig-selinux" class="headerlink" title="1. vi /etc/sysconfig/selinux"></a>1. vi /etc/sysconfig/selinux</h2><p>插入/编辑以下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure></p><p>#重启生效</p><h2 id="2：在Fedora-EPEL源中已经提供了docker-io包，下载安装epel："><a href="#2：在Fedora-EPEL源中已经提供了docker-io包，下载安装epel：" class="headerlink" title="2：在Fedora EPEL源中已经提供了docker-io包，下载安装epel："></a>2：在Fedora EPEL源中已经提供了docker-io包，下载安装epel：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">sed -i &apos;s/^mirrorlist=https/mirrorlist=http/&apos; /etc/yum.repos.d/epel.repo</span><br><span class="line">（elpe.repo）</span><br><span class="line">[epel]</span><br><span class="line">name=epel</span><br><span class="line">mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=epel-$releasever&amp;arch=$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br></pre></td></tr></table></figure><h2 id="3：安装docker"><a href="#3：安装docker" class="headerlink" title="3：安装docker"></a>3：安装docker</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install docker-io</span><br></pre></td></tr></table></figure><p>安装完成后</p><h2 id="4：启动docker"><a href="#4：启动docker" class="headerlink" title="4：启动docker"></a>4：启动docker</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service  docker  start</span><br></pre></td></tr></table></figure><h2 id="5：查看docker版本"><a href="#5：查看docker版本" class="headerlink" title="5：查看docker版本"></a>5：查看docker版本</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker vesion</span><br></pre></td></tr></table></figure><h2 id="6：查看docker日志"><a href="#6：查看docker日志" class="headerlink" title="6：查看docker日志"></a>6：查看docker日志</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /var/log/docker</span><br></pre></td></tr></table></figure><p>docker安装完成</p><h1 id="一：卸载docker"><a href="#一：卸载docker" class="headerlink" title="一：卸载docker"></a>一：卸载docker</h1><p>列出你安装过的包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# yum list installed | grep docker</span><br><span class="line">docker-io.x86_64   1.7.1-2.el6                        @epel</span><br></pre></td></tr></table></figure></p><p>删除软件包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y remove docker-io.x86_64</span><br></pre></td></tr></table></figure></p><p>删除镜像/容器等<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /var/lib/docker</span><br></pre></td></tr></table></figure></p><h1 id="二：升级docker版本为1-10-3"><a href="#二：升级docker版本为1-10-3" class="headerlink" title="二：升级docker版本为1.10.3"></a>二：升级docker版本为1.10.3</h1><p>升级之前停止docker服务,并将原有的docker服务进行备份.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv /usr/bin/docker /usr/bin/docker.bak</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup wget -c https://get.docker.com/builds/Linux/x86_64/docker-1.10.3 -O /usr/bin/docker</span><br></pre></td></tr></table></figure><p>给执行权限：chmod 755 /usr/bin/docker 然后重启服务，并查看版本.</p><h2 id="报错："><a href="#报错：" class="headerlink" title="报错："></a>报错：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Starting cgconfig service: Error: cannot mount memory to /cgroup/memory: No such file or directory</span><br><span class="line">/sbin/cgconfigparser; error loading /etc/cgconfig.conf: Cgroup mounting failed</span><br><span class="line">Failed to parse /etc/cgconfig.conf                         [FAILED]</span><br><span class="line">Starting docker:                                       [  OK  ]</span><br></pre></td></tr></table></figure><h2 id="修改：-etc-cgconfig-conf文件"><a href="#修改：-etc-cgconfig-conf文件" class="headerlink" title="修改：/etc/cgconfig.conf文件"></a>修改：/etc/cgconfig.conf文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mount &#123;</span><br><span class="line">    cpuset  = /cgroup/cpuset;</span><br><span class="line">    cpu = /cgroup/cpu;</span><br><span class="line">    cpuacct = /cgroup/cpuacct;</span><br><span class="line">#   memory  = /cgroup/memory;</span><br><span class="line">    devices = /cgroup/devices;</span><br><span class="line">    freezer = /cgroup/freezer;</span><br><span class="line">    net_cls = /cgroup/net_cls;</span><br><span class="line">    blkio   = /cgroup/blkio;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重新启动docker 正常</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1：关闭selinux&quot;&gt;&lt;a href=&quot;#1：关闭selinux&quot; class=&quot;headerlink&quot; title=&quot;1：关闭selinux&quot;&gt;&lt;/a&gt;1：关闭selinux&lt;/h2&gt;&lt;p&gt;临时关闭：setenforce 0&lt;br&gt;永久关闭：&lt;/p&gt;
&lt;h2
      
    
    </summary>
    
      <category term="docker" scheme="http://www.wenchong.top/categories/docker/"/>
    
    
      <category term="docker" scheme="http://www.wenchong.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL</title>
    <link href="http://www.wenchong.top/2018/01/15/Spark%20SQL/"/>
    <id>http://www.wenchong.top/2018/01/15/Spark SQL/</id>
    <published>2018-01-14T16:00:00.000Z</published>
    <updated>2018-07-05T08:15:06.256Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概观"><a href="#概观" class="headerlink" title="概观"></a>概观</h3><p>Spark SQL是用于结构化数据处理的Spark模块。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了有关数据结构和正在执行的计算的更多信息。在内部，Spark SQL使用此额外信息来执行额外的优化。有几种与Spark SQL交互的方法，包括SQL和Dataset API。在计算结果时，使用相同的执行引擎，与您用于表达计算的API /语言无关。这种统一意味着开发人员可以轻松地在不同的API之间来回切换，从而提供表达给定转换的最自然的方式。</p><h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Spark SQL的一个用途是执行SQL查询。Spark SQL还可用于从现有Hive安装中读取数据。</p><h3 id="数据集和数据框架"><a href="#数据集和数据框架" class="headerlink" title="数据集和数据框架"></a>数据集和数据框架</h3><p>数据集是分布式数据集合。Dataset是Spark 1.6中添加的一个新接口，它提供了RDD的优势（强类型，使用强大的lambda函数的能力）以及Spark SQL优化执行引擎的优点。数据集可以被构造从JVM对象，然后使用功能性的转换（操作map，flatMap，filter等等）。<br>DataFrame是组织为命名列的数据集。它在概念上等同于关系数据库中的表或R / Python中的数据框，但在引擎盖下具有更丰富的优化。DataFrame可以从多种来源构建，例如：结构化数据文件，Hive中的表，外部数据库或现有RDD。DataFrame API在Scala，Java，Python和R中可用。在Scala和Java中，DataFrame由Rows 的数据集表示。在Scala API中，DataFrame它只是一个类型别名Dataset[Row]。而在Java API中，用户需要使用Dataset<row>来表示DataFrame。</row></p><h3 id="起点：SparkSession"><a href="#起点：SparkSession" class="headerlink" title="起点：SparkSession"></a>起点：SparkSession</h3><p>Spark中所有功能的入口点都是SparkSession类。要创建基本的SparkSession，只需使用SparkSession.builder()：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sparkSession spark = SparkSession.builder()</span><br><span class="line">.appName(&quot;Java Spark SQL示例&quot;)</span><br><span class="line">.config(&quot;spark.some.config.option&quot;,&quot;some-value&quot;)</span><br><span class="line">.getOrCreate();</span><br></pre></td></tr></table></figure></p><h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataFrame&lt;Row&gt; df = spark.read().json(&quot;examples / src / main / resources / people.json&quot;);</span><br><span class="line"></span><br><span class="line">//将df中的内容限速出来</span><br><span class="line">df.show();</span><br></pre></td></tr></table></figure><h3 id="DataFrame操作"><a href="#DataFrame操作" class="headerlink" title="DataFrame操作"></a>DataFrame操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import static  org.apache.spark.sql.functions.col ;</span><br><span class="line"></span><br><span class="line">//以树型结构打印</span><br><span class="line">df.printSchema();</span><br><span class="line"></span><br><span class="line">//仅仅打印一列数据</span><br><span class="line">df.select(&quot;name&quot;).show();</span><br><span class="line"></span><br><span class="line">//打印所有数据，但年龄+1</span><br><span class="line">df.select(col(&quot;name&quot;),col(&quot;age&quot;).plus(1)).show();</span><br><span class="line"></span><br><span class="line">//打印年龄大于21的</span><br><span class="line">df.select(col(&quot;age&quot;).gt(21)).show();</span><br><span class="line"></span><br><span class="line">//按年龄计算人数</span><br><span class="line">df.groupBy(&quot;age&quot;).count().show();</span><br></pre></td></tr></table></figure><h3 id="以编程方式运行SQL查询"><a href="#以编程方式运行SQL查询" class="headerlink" title="以编程方式运行SQL查询"></a>以编程方式运行SQL查询</h3><p>该sql上的功能SparkSession使应用程序以编程方式运行SQL查询并返回结果的Dataset<row>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import  org.apache.spark.sql.Dataset ; </span><br><span class="line">import  org.apache.spark.sql.Row ;</span><br><span class="line"></span><br><span class="line">//将DataFrame注册为SQL临时视图</span><br><span class="line">df.createOrReplaceTempView(&quot;people&quot;);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(&quot;SELECT * FROM people&quot;);</span><br><span class="line">sqlDF.show();</span><br></pre></td></tr></table></figure></row></p><h3 id="全球临时观点"><a href="#全球临时观点" class="headerlink" title="全球临时观点"></a>全球临时观点</h3><p>Spark SQL中的临时视图是会话范围的，如果创建它的会话终止，它将消失。如果您希望拥有一个在所有会话之间共享的临时视图并保持活动状态，直到Spark应用程序终止，您可以创建一个全局临时视图。全局临时视图与系统保留的数据库绑定global_temp，我们必须使用限定名称来引用它，例如SELECT * FROM global_temp.view1。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//将DataFrame注册为全局临时视图</span><br><span class="line">df.createGlobalTempView（“people” ）;</span><br><span class="line">//全局临时视图绑定到系统保留的数据库`global_temp` </span><br><span class="line">spark.sql(&quot;SELECT * FROM global_temp.people&quot;).show();</span><br><span class="line">//全局临时视图是跨会话 </span><br><span class="line">spark.newSession().sql(&quot;SELECT * FROM global_temp.people&quot;).show();</span><br></pre></td></tr></table></figure></p><h3 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h3><p>数据集与RDD类似，但是，它们不使用Java序列化或Kryo，而是使用专门的编码器来序列化对象以便通过网络进行处理或传输。虽然编码器和标准序列化都负责将对象转换为字节，但编码器是动态生成的代码，并使用一种格式，允许Spark执行许多操作，如过滤，排序和散列，而无需将字节反序列化为对象。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Collections;</span><br><span class="line">import java.io.Serializable;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.api.java.function.MapFunction;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.Encoder;</span><br><span class="line">import org.apache.spark.sql.Encoders;</span><br><span class="line"></span><br><span class="line">public static class Person implements Serializable &#123;</span><br><span class="line">  private String name;</span><br><span class="line">  private int age;</span><br><span class="line"></span><br><span class="line">  public String getName() &#123;</span><br><span class="line">    return name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void setName(String name) &#123;</span><br><span class="line">    this.name = name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public int getAge() &#123;</span><br><span class="line">    return age;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void setAge(int age) &#123;</span><br><span class="line">    this.age = age;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Create an instance of a Bean class</span><br><span class="line">Person person = new Person();</span><br><span class="line">person.setName(&quot;Andy&quot;);</span><br><span class="line">person.setAge(32);</span><br><span class="line"></span><br><span class="line">// Encoders are created for Java beans</span><br><span class="line">Encoder&lt;Person&gt; personEncoder = Encoders.bean(Person.class);</span><br><span class="line">Dataset&lt;Person&gt; javaBeanDS = spark.createDataset(</span><br><span class="line">  Collections.singletonList(person),</span><br><span class="line">  personEncoder</span><br><span class="line">);</span><br><span class="line">javaBeanDS.show();</span><br><span class="line">// +---+----+</span><br><span class="line">// |age|name|</span><br><span class="line">// +---+----+</span><br><span class="line">// | 32|Andy|</span><br><span class="line">// +---+----+</span><br><span class="line"></span><br><span class="line">// Encoders for most common types are provided in class Encoders</span><br><span class="line">Encoder&lt;Integer&gt; integerEncoder = Encoders.INT();</span><br><span class="line">Dataset&lt;Integer&gt; primitiveDS = spark.createDataset(Arrays.asList(1, 2, 3), integerEncoder);</span><br><span class="line">Dataset&lt;Integer&gt; transformedDS = primitiveDS.map(</span><br><span class="line">    (MapFunction&lt;Integer, Integer&gt;) value -&gt; value + 1,</span><br><span class="line">    integerEncoder);</span><br><span class="line">transformedDS.collect(); // Returns [2, 3, 4]</span><br><span class="line"></span><br><span class="line">// DataFrames can be converted to a Dataset by providing a class. Mapping based on name</span><br><span class="line">String path = &quot;examples/src/main/resources/people.json&quot;;</span><br><span class="line">Dataset&lt;Person&gt; peopleDS = spark.read().json(path).as(personEncoder);</span><br><span class="line">peopleDS.show();</span><br><span class="line">// +----+-------+</span><br><span class="line">// | age|   name|</span><br><span class="line">// +----+-------+</span><br><span class="line">// |null|Michael|</span><br><span class="line">// |  30|   Andy|</span><br><span class="line">// |  19| Justin|</span><br><span class="line">// +----+-------+</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;概观&quot;&gt;&lt;a href=&quot;#概观&quot; class=&quot;headerlink&quot; title=&quot;概观&quot;&gt;&lt;/a&gt;概观&lt;/h3&gt;&lt;p&gt;Spark SQL是用于结构化数据处理的Spark模块。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供
      
    
    </summary>
    
      <category term="spark" scheme="http://www.wenchong.top/categories/spark/"/>
    
    
      <category term="spark" scheme="http://www.wenchong.top/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>致橡树</title>
    <link href="http://www.wenchong.top/2018/01/13/%E8%87%B4%E6%A9%A1%E6%A0%91/"/>
    <id>http://www.wenchong.top/2018/01/13/致橡树/</id>
    <published>2018-01-12T16:00:00.000Z</published>
    <updated>2018-05-25T09:25:35.759Z</updated>
    
    <content type="html"><![CDATA[<ul><li>文学的力量往往是在于对人信仰的改变</li></ul><p>我如果爱你——<br>绝不像攀援的凌霄花，<br>借你的高枝炫耀自己；<br>我如果爱你——<br>绝不学痴情的鸟儿，<br>为绿荫重复单调的歌曲；<br>也不止像泉源，<br>常年送来清凉的慰藉；<br>也不止像险峰，<br>增加你的高度，衬托你的威仪。<br>甚至日光，<br>甚至春雨。</p><p>不，这些都还不够！<br>我必须是你近旁的一株木棉，<br>作为树的形象和你站在一起。<br>根，紧握在地下；<br>叶，相触在云里。<br>每一阵风过，<br>我们都互相致意，<br>但没有人，<br>听懂我们的言语。<br>你有你的铜枝铁干，<br>像刀，像剑，也像戟；<br>我有我红硕的花朵，<br>像沉重的叹息，<br>又像英勇的火炬。</p><p>我们分担寒潮、风雷、霹雳；<br>我们共享雾霭、流岚、虹霓。<br>仿佛永远分离，<br>却又终身相依。<br>这才是伟大的爱情，<br>坚贞就在这里：<br>爱——<br>不仅爱你伟岸的身躯，<br>也爱你坚持的位置，<br>足下的土地。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;文学的力量往往是在于对人信仰的改变&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我如果爱你——&lt;br&gt;绝不像攀援的凌霄花，&lt;br&gt;借你的高枝炫耀自己；&lt;br&gt;我如果爱你——&lt;br&gt;绝不学痴情的鸟儿，&lt;br&gt;为绿荫重复单调的歌曲；&lt;br&gt;也不止像泉源，&lt;br&gt;常年送来清凉的慰藉；
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>kafka编程模型</title>
    <link href="http://www.wenchong.top/2018/01/13/kafka%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/"/>
    <id>http://www.wenchong.top/2018/01/13/kafka编程模型/</id>
    <published>2018-01-12T16:00:00.000Z</published>
    <updated>2018-07-05T03:03:04.517Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Class-KafkaProduce"><a href="#Class-KafkaProduce" class="headerlink" title="Class KafkaProduce"></a>Class KafkaProduce<k,v></k,v></h3><p>所有已实现的接口：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.io.Closeable，java.lang.AutoCloseable，Producer&lt;K,V&gt;</span><br></pre></td></tr></table></figure></p><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public class KafkaProducer&lt;K,V&gt; extends java.lang.Object implements Producer&lt;K,V&gt;&#123;</span><br><span class="line">Properties props = new Properties();</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//指定kafka服务器</span><br><span class="line">props.put(&quot;acks&quot;, &quot;all&quot;);//发送消息方式，此处采用异步发送,在acks配置控制下，其请求被认为是完整的标准。我们指定的“all”设置将导致阻止完整提交记录，这是最慢但最耐用的设置。如果请求失败，则生产者可以自动重试</span><br><span class="line">props.put(&quot;retries&quot;, 0);//副本个数，一般设置为3</span><br><span class="line">props.put(&quot;batch.size&quot;, 16384);//生产者为每个分区维护未发送记录的缓冲区。这些缓冲区的大小由batch.sizeconfig 指定。</span><br><span class="line">props.put(&quot;linger.ms&quot;, 1);//默认情况下，即使缓冲区中有其他未使用的空间，也可以立即发送缓冲区。但是，如果您希望减少可以设置linger.ms为大于0 的请求数。这将指示生产者在发送请求之前等待该毫秒数，希望更多记录到达以填充同一批次。这类似于TCP中的Nagle算法。</span><br><span class="line">/*buffer.memory控制提供给生产者用于缓冲总量的存储器。如果记录的发送速度快于传输到服务器的速度，则此缓冲区空间将耗尽。</span><br><span class="line">当缓冲区空间耗尽时，额外的发送调用将被阻止。阻塞时间的阈值由max.block.ms之后确定它抛出TimeoutException。*/</span><br><span class="line">props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);//序列化</span><br><span class="line">props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);//序列化</span><br><span class="line"></span><br><span class="line">Producer&lt;String,String&gt; produce = new Producer(props);</span><br><span class="line">for(int i=0; i &lt; 100; i++)&#123;</span><br><span class="line">Producer.send(new ProducerRecord&lt;String,String&gt;(&quot;my-topic&quot;,Integer(i),Integer(i)));</span><br><span class="line">&#125;</span><br><span class="line">Producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="下面的示例说明了如何使用新API。每个消息都可以是单个事务的一部分。"><a href="#下面的示例说明了如何使用新API。每个消息都可以是单个事务的一部分。" class="headerlink" title="下面的示例说明了如何使用新API。每个消息都可以是单个事务的一部分。"></a>下面的示例说明了如何使用新API。每个消息都可以是单个事务的一部分。</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Properties props = new Properties();</span><br><span class="line"> props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line"> props.put(&quot;transactional.id&quot;, &quot;my-transactional-id&quot;);</span><br><span class="line"> Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props, new StringSerializer(), new StringSerializer());</span><br><span class="line"></span><br><span class="line"> producer.initTransactions();</span><br><span class="line"></span><br><span class="line"> try &#123;</span><br><span class="line"> /*在beginTransaction()和commitTransaction()调用之间发送的所有消息 都将是单个事务的一部分。</span><br><span class="line"> 当 transactional.id指定时，由生产者发送的所有消息必须是事务的一部分。*/</span><br><span class="line">     producer.beginTransaction();</span><br><span class="line">     for (int i = 0; i &lt; 100; i++)</span><br><span class="line">         producer.send(new ProducerRecord&lt;&gt;(&quot;my-topic&quot;, Integer.toString(i), Integer.toString(i)));</span><br><span class="line">     producer.commitTransaction();</span><br><span class="line"> &#125; catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) &#123;</span><br><span class="line">     // We can&apos;t recover from these exceptions, so our only option is to close the producer and exit.</span><br><span class="line">     producer.close();</span><br><span class="line"> &#125; catch (KafkaException e) &#123;</span><br><span class="line">     // For all other exceptions, just abort the transaction and try again.</span><br><span class="line">     producer.abortTransaction();</span><br><span class="line"> &#125;</span><br><span class="line"> producer.close();</span><br></pre></td></tr></table></figure><h3 id="consumer"><a href="#consumer" class="headerlink" title="consumer"></a>consumer</h3> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Package org.apache.kafka.clients.consumer</span><br><span class="line">Class KafkaConsumer&lt;K,V&gt;</span><br><span class="line">-----</span><br><span class="line">All Implemented Interfaces:</span><br><span class="line">java.io.Closeable, java.lang.AutoCloseable, Consumer&lt;K,V&gt;</span><br></pre></td></tr></table></figure><ul><li>示例</li><li><p>自动抵消提交</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Properties props = new Properties();</span><br><span class="line">     props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line">     props.put(&quot;group.id&quot;, &quot;test&quot;);</span><br><span class="line">     props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);//设置enable.auto.commit意味着使用由配置控制的频率自动提交偏移auto.commit.interval.ms。</span><br><span class="line">     props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);</span><br><span class="line">     props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">     props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">     KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line">     consumer.subscribe(Arrays.asList(&quot;foo&quot;, &quot;bar&quot;));//通过其中一个subscribeAPI 动态设置要订阅的主题列表 。</span><br><span class="line">     while (true) &#123;</span><br><span class="line">         ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);</span><br><span class="line">         for (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">             System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value());</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure></li><li><p>手动偏移控制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Properties props = new Properties（）; </span><br><span class="line">   props.put（“bootstrap.servers”，“localhost：9092”）; </span><br><span class="line">   props.put（“group.id”，“test”）; </span><br><span class="line">   props.put（“enable.auto.commit”，“false”）; </span><br><span class="line">   props.put（“key.deserializer”，“org.apache.kafka.common.serialization.StringDeserializer”）; </span><br><span class="line">   props.put（“value.deserializer”，“org.apache.kafka.common.serialization.StringDeserializer”）; </span><br><span class="line">   KafkaConsumer &lt;String，String&gt; consumer = new KafkaConsumer &lt;&gt;（props）; </span><br><span class="line">   consumer.subscribe（Arrays.asList（“foo”，“bar”））; </span><br><span class="line">   final int minBatchSize = 200; </span><br><span class="line">   List &lt;ConsumerRecord &lt;String，String &gt;&gt; buffer = new ArrayList &lt;&gt; （）; </span><br><span class="line">   while（true）&#123; </span><br><span class="line">       ConsumerRecords &lt;String，String&gt; records = consumer.poll（100）;</span><br><span class="line">       for（ConsumerRecord &lt;String，String&gt; record：records）&#123; </span><br><span class="line">           buffer.add（record）; </span><br><span class="line">       &#125; </span><br><span class="line">       if（buffer.size（）&gt; = minBatchSize）&#123; </span><br><span class="line">           insertIntoDb（buffer）; </span><br><span class="line">           consumer.commitSync（）; //commitSync()是一种安全机制，可确保只有组中的活动成员才能提交偏移量。</span><br><span class="line">           buffer.clear（）; </span><br><span class="line">       &#125; </span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="下面的示例中，我们在完成处理每个分区中的记录后提交偏移量。"><a href="#下面的示例中，我们在完成处理每个分区中的记录后提交偏移量。" class="headerlink" title="下面的示例中，我们在完成处理每个分区中的记录后提交偏移量。"></a>下面的示例中，我们在完成处理每个分区中的记录后提交偏移量。</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">try &#123; </span><br><span class="line">         while（running）&#123; </span><br><span class="line">             ConsumerRecords &lt;String，String&gt; records = consumer.poll（Long.MAX_VALUE）; </span><br><span class="line">             for（TopicPartition partition：records.partitions（））&#123; </span><br><span class="line">                 List &lt;ConsumerRecord &lt;String，String &gt;&gt; partitionRecords = records.records（partition）; </span><br><span class="line">                 for（ConsumerRecord &lt;String，String&gt; record：partitionRecords）&#123; </span><br><span class="line">                     System.out.println（record.offset（）+“：”+ record.value（））; </span><br><span class="line">                 &#125; </span><br><span class="line">                 long lastOffset = partitionRecords.get（partitionRecords.size（） -  1）.offset（）; </span><br><span class="line">                 consumer.commitSync（Collections.singletonMap（partition，new OffsetAndMetadata（lastOffset + 1）））; </span><br><span class="line">             &#125;</span><br><span class="line">         &#125; </span><br><span class="line">     &#125;finally&#123; </span><br><span class="line">       consumer.close（）; </span><br><span class="line">     &#125;</span><br><span class="line"> /*注意：提交的偏移量应始终是应用程序将读取的下一条消息的偏移量。 因此，在调用时，commitSync(offsets)您应该在处理的最后一条消息的偏移量中添加一个。*/</span><br></pre></td></tr></table></figure><h3 id="手动分区分配"><a href="#手动分区分配" class="headerlink" title="手动分区分配"></a>手动分区分配</h3><ul><li>使用此模式，subscribe您只需使用要使用assign(Collection)的分区的完整列表进行调用 ，而不是使用主题订阅。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String topic =“foo”; </span><br><span class="line">    TopicPartition partition0 = new TopicPartition（topic，0）; </span><br><span class="line">    TopicPartition partition1 = new TopicPartition（topic，1）; </span><br><span class="line">    consumer.assign（Arrays.asList（partition0，partition1））;</span><br></pre></td></tr></table></figure></li></ul><h3 id="多线程处理"><a href="#多线程处理" class="headerlink" title="多线程处理"></a>多线程处理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public class KafkaConsumerRunner implements Runnable &#123;</span><br><span class="line">     private final AtomicBoolean closed = new AtomicBoolean(false);</span><br><span class="line">     private final KafkaConsumer consumer;</span><br><span class="line"></span><br><span class="line">     public void run() &#123;</span><br><span class="line">         try &#123;</span><br><span class="line">             consumer.subscribe(Arrays.asList(&quot;topic&quot;));</span><br><span class="line">             while (!closed.get()) &#123;</span><br><span class="line">                 ConsumerRecords records = consumer.poll(10000);</span><br><span class="line">                 // Handle new records</span><br><span class="line">             &#125;</span><br><span class="line">         &#125; catch (WakeupException e) &#123;</span><br><span class="line">             // Ignore exception if closing</span><br><span class="line">             if (!closed.get()) throw e;</span><br><span class="line">         &#125; finally &#123;</span><br><span class="line">             consumer.close();</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     // Shutdown hook which can be called from a separate thread</span><br><span class="line">     public void shutdown() &#123;</span><br><span class="line">         closed.set(true);</span><br><span class="line">         consumer.wakeup();</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><ul><li>然后在单独的线程中，可以通过设置关闭标志并唤醒消费者来关闭消费者。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">closed.set（true）; </span><br><span class="line">   consumer.wakeup（）;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Class-KafkaProduce&quot;&gt;&lt;a href=&quot;#Class-KafkaProduce&quot; class=&quot;headerlink&quot; title=&quot;Class KafkaProduce&quot;&gt;&lt;/a&gt;Class KafkaProduce&lt;k,v&gt;&lt;/k,v&gt;&lt;/h
      
    
    </summary>
    
      <category term="kafka" scheme="http://www.wenchong.top/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://www.wenchong.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Phoenix使用</title>
    <link href="http://www.wenchong.top/2017/12/15/Phoenix%E4%BD%BF%E7%94%A8/"/>
    <id>http://www.wenchong.top/2017/12/15/Phoenix使用/</id>
    <published>2017-12-14T16:00:00.000Z</published>
    <updated>2018-07-06T03:01:15.596Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概观"><a href="#概观" class="headerlink" title="概观"></a>概观</h3><ul><li><p>Phoenix作为应用层和HBASE之间的中间件,以下特性使它在大数据量的简单查询场景有着独有的优势<br>二级索引支持(global index + local index)<br>编译SQL成为原生HBASE的可并行执行的scan<br>在数据层完成计算，server端的coprocessor执行聚合<br>下推where过滤条件到server端的scan filter上<br>利用统计信息优化、选择查询计划（5.x版本将支持CBO）<br>skip scan功能提高扫描速度</p></li><li><p>一般可以使用以下三种方式访问Phoenix<br>1.JDBC API<br>2.使用Python编写的命令行工具（sqlline, sqlline-thin和psql等）<br>3.SQuirrel</p></li></ul><h3 id="一、命令行工具psql使用示例"><a href="#一、命令行工具psql使用示例" class="headerlink" title="一、命令行工具psql使用示例"></a>一、命令行工具psql使用示例</h3><p>1.创建一个建表的sql脚本文件us_population.sql：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS us_population (</span><br><span class="line">    state CHAR(2) NOT NULL,</span><br><span class="line">    city VARCHAR NOT NULL,</span><br><span class="line">    population BIGINT</span><br><span class="line">    CONSTRAINT my_pk PRIMARY KEY (state, city));</span><br></pre></td></tr></table></figure></p><ol><li><p>创建csv格式的数据文件us_population.csv：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NY,New York,8143197</span><br><span class="line">CA,Los Angeles,3844829</span><br><span class="line">IL,Chicago,2842518</span><br><span class="line">TX,Houston,2016582</span><br><span class="line">PA,Philadelphia,1463281</span><br><span class="line">AZ,Phoenix,1461575</span><br><span class="line">TX,San Antonio,1256509</span><br><span class="line">CA,San Diego,1255540</span><br><span class="line">TX,Dallas,1213825</span><br><span class="line">CA,San Jose,912332</span><br></pre></td></tr></table></figure></li><li><p>创建一个查询sql脚本文件us_population_queries.sql</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT state as &quot;State&quot;,count(city) as &quot;City Count&quot;,sum(population) as &quot;Population Sum&quot;</span><br><span class="line">FROM us_population</span><br><span class="line">GROUP BY state</span><br><span class="line">ORDER BY sum(population) DESC;</span><br></pre></td></tr></table></figure></li><li><p>执行psql.py工具运行sql脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./psql.py &lt;your_zookeeper_quorum&gt; us_population.sql us_population.csv us_population_queries.sql</span><br></pre></td></tr></table></figure></li></ol><h3 id="二、JDBC-API使用示例"><a href="#二、JDBC-API使用示例" class="headerlink" title="二、JDBC API使用示例"></a>二、JDBC API使用示例</h3><ol><li><p>使用Maven构建工程时，需要添加以下依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;com.aliyun.phoenix&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;ali-phoenix-core&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;$&#123;version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure></li><li><p>创建名为test.java的文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line">import java.sql.SQLException;</span><br><span class="line">import java.sql.PreparedStatement;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line"></span><br><span class="line">public class test &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws SQLException &#123;</span><br><span class="line">        Statement stmt = null;</span><br><span class="line">        ResultSet rset = null;</span><br><span class="line">        </span><br><span class="line">        Connection con = DriverManager.getConnection(&quot;jdbc:phoenix:[zookeeper]&quot;);</span><br><span class="line">        stmt = con.createStatement();</span><br><span class="line">        </span><br><span class="line">        stmt.executeUpdate(&quot;create table test (mykey integer not null primary key, mycolumn varchar)&quot;);</span><br><span class="line">        stmt.executeUpdate(&quot;upsert into test values (1,&apos;Hello&apos;)&quot;);</span><br><span class="line">        stmt.executeUpdate(&quot;upsert into test values (2,&apos;World!&apos;)&quot;);</span><br><span class="line">        con.commit();</span><br><span class="line">        </span><br><span class="line">        PreparedStatement statement = con.prepareStatement(&quot;select * from test&quot;);</span><br><span class="line">        rset = statement.executeQuery();</span><br><span class="line">        while (rset.next()) &#123;</span><br><span class="line">            System.out.println(rset.getString(&quot;mycolumn&quot;));</span><br><span class="line">        &#125;</span><br><span class="line">        statement.close();</span><br><span class="line">        con.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>3.执行test.java<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">javac test.java</span><br><span class="line"></span><br><span class="line">java -cp &quot;../phoenix-[version]-client.jar:.&quot; test</span><br></pre></td></tr></table></figure></p><h3 id="三、数据类型"><a href="#三、数据类型" class="headerlink" title="三、数据类型"></a>三、数据类型</h3><p>目前Phoenix支持24种简单数据类型和1个一维Array的复杂类型。</p><h3 id="四、DML语法"><a href="#四、DML语法" class="headerlink" title="四、DML语法"></a>四、DML语法</h3><ul><li>HBASE上Phoenix支持的DML<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">upsert values</span><br><span class="line">upsert select</span><br><span class="line">delete</span><br></pre></td></tr></table></figure></li></ul><ol><li>SELECT<br>从一个或者多个表中查询数据。<br>LIMIT(或者FETCH FIRST) 在ORDER BY子句后将转换为top-N查询。<br>OFFSET子句指定返回查询结果前跳过的行数。</li></ol><ul><li>示例<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM TEST LIMIT 1000;</span><br><span class="line">SELECT * FROM TEST LIMIT 1000 OFFSET 100;</span><br><span class="line">SELECT full_name FROM SALES_PERSON WHERE ranking &gt;= 5.0</span><br><span class="line">    UNION ALL SELECT reviewer_name FROM </span><br><span class="line">    CUSTOMER_REVIEW WHERE score &gt;= 8.0</span><br></pre></td></tr></table></figure></li></ul><ol><li>UPSERT VALUES<br>此处upsert语义有异于标准SQL中的Insert，当写入值不存在时，表示写入数据，否则更新数据。其中列的声明是可以省略的，当省略时，values指定值的顺序和目标表中schema声明列的顺序需要一致。</li></ol><p>ON DUPLICATE KEY是4.9版本中的功能，表示upsert原子写入的语义，在写入性能上弱于非原子语义。相同的row在同一batch中按照执行顺序写入。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">UPSERT INTO TEST VALUES(&apos;foo&apos;,&apos;bar&apos;,3);</span><br><span class="line">UPSERT INTO TEST(NAME,ID) VALUES(&apos;foo&apos;,123);</span><br><span class="line">UPSERT INTO TEST(ID, COUNTER) VALUES(123, 0) ON DUPLICATE KEY UPDATE COUNTER = COUNTER + 1;</span><br><span class="line">UPSERT INTO TEST(ID, MY_COL) VALUES(123, 0) ON DUPLICATE KEY IGNORE;</span><br></pre></td></tr></table></figure></p><ol><li>UPSERT SELECT<br>从另外一张表中读取数据写入到目标表中，如果数据存在则更新，否则插入数据。插入目标表的值顺序和查询表指定查询字段一致。当auto commit被打开并且select子句没有聚合时，写入目标表这个过程是在server端完成的，否则查询的数据会先缓存在客户端再写入目标表中（phoenix.mutate.upsertBatchSize表示从客户端一次commit的行数，默认10000行）。</li></ol><ul><li>示例<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">UPSERT INTO test.targetTable(col1, col2) SELECT col3, col4 FROM test.sourceTable WHERE col5 &lt; 100</span><br><span class="line">UPSERT INTO foo SELECT * FROM bar;</span><br></pre></td></tr></table></figure></li></ul><ol><li>DELETE<br>删除选定的列。如果auto commit打开，删除操作将在server端执行。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DELETE FROM TABLENAME;</span><br><span class="line">DELETE FROM TABLENAME WHERE PK=123;</span><br><span class="line">DELETE FROM TABLENAME WHERE NAME LIKE &apos;%&apos;;</span><br></pre></td></tr></table></figure></li></ol><h3 id="五、加盐表"><a href="#五、加盐表" class="headerlink" title="五、加盐表"></a>五、加盐表</h3><ol><li>什么是加盐？<br>在密码学中，加盐是指在散列之前将散列内容（例如：密码）的任意固定位置插入特定的字符串。这个在散列中加入字符串的方式称为“加盐”。其作用是让加盐后的散列结果和没有加盐的结果不相同，在不同的应用情景中，这个处理可以增加额外的安全性。而Phoenix中加盐是指对pk对应的byte数组插入特定的byte数据。</li><li>加盐能解决什么问题？<br>加盐能解决HBASE读写热点问题，例如:单调递增rowkey数据的持续写入，使得负载集中在某一个RegionServer上引起的热点问题。</li><li><p>怎么对表加盐？<br>在创建表的时候指定属性值：SALT_BUCKETS，其值表示所分buckets(region)数量， 范围是1~256。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE table (key VARCHAR PRIMARY KEY, col VARCHAR) SALT_BUCKETS = 8;</span><br></pre></td></tr></table></figure></li><li><p>加盐的原理是什么？<br>加盐的过程就是在原来key的基础上增加一个byte作为前缀,计算公式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_row_key = (++index % BUCKETS_NUMBER) + original_key</span><br></pre></td></tr></table></figure></li><li><p>一个表“加多少盐合适”？<br>当可用block cache的大小小于表数据大小时，较优的slated bucket是和region server数量相同，这样可以得到更好的读写性能。<br>当表的数量很大时，基本上会忽略blcok cache的优化收益，大部分数据仍然需要走磁盘IO。比如对于10个region server集群的大表，可以考虑设计64~128个slat buckets。</p></li><li>加盐时需要注意<br>创建加盐表时不能再指定split key。<br>太大的slated buckets会减小range查询的灵活性，甚至降低查询性能。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;概观&quot;&gt;&lt;a href=&quot;#概观&quot; class=&quot;headerlink&quot; title=&quot;概观&quot;&gt;&lt;/a&gt;概观&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Phoenix作为应用层和HBASE之间的中间件,以下特性使它在大数据量的简单查询场景有着独有的优势&lt;br&gt;二级索引支持(
      
    
    </summary>
    
      <category term="Phoenix" scheme="http://www.wenchong.top/categories/Phoenix/"/>
    
    
      <category term="Phoenix" scheme="http://www.wenchong.top/tags/Phoenix/"/>
    
  </entry>
  
  <entry>
    <title>Centos7伪分布式安装Hadoop2.6和Hbase0.94</title>
    <link href="http://www.wenchong.top/2017/12/01/centos7+hadoop2.6+hbase1.0.x/"/>
    <id>http://www.wenchong.top/2017/12/01/centos7+hadoop2.6+hbase1.0.x/</id>
    <published>2017-11-30T16:00:00.000Z</published>
    <updated>2018-05-18T14:17:28.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="只记录那二年踏过的坑"><a href="#只记录那二年踏过的坑" class="headerlink" title="只记录那二年踏过的坑"></a>只记录那二年踏过的坑</h1><h2 id="一、安装Jdk：首先需要卸载系统自带的openjava，查看系统的Java：-rpm-qa-grep-java。"><a href="#一、安装Jdk：首先需要卸载系统自带的openjava，查看系统的Java：-rpm-qa-grep-java。" class="headerlink" title="一、安装Jdk：首先需要卸载系统自带的openjava，查看系统的Java： rpm -qa|grep java。"></a>一、安装Jdk：首先需要卸载系统自带的openjava，查看系统的Java： rpm -qa|grep java。</h2><ul><li>卸载： yum -y remove java javaxxxxx(系统自带的Java版本)</li></ul><p>安装jdk，将jdk.tar.gz文件复制到/usr/java中,终端进入/mnt/share ,cp jdk.tar.gz /usr/ava，进入/usr/java解压：tar xzvf jdk.targz</p><p>配置环境变量：vi /etc/profile 输入i编辑<br>在尾部添加：export JAVA_HOME=/usr/java/jdkxxxx<br>export PATH=$JAVA_HOME/bin:$PATH<br>export CLASSPATH=.:JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</p><p>保存并退出： wq<br>使修改生效： source /etc/profile<br>查看Java版本：java -version</p><h2 id="二、Hadoop伪分布式安装"><a href="#二、Hadoop伪分布式安装" class="headerlink" title="二、Hadoop伪分布式安装"></a>二、Hadoop伪分布式安装</h2><ul><li>1、ssh无密码登陆</li></ul><p>终端：ssh-keygen -t rsa (获得rsa公钥私钥,id_rsa和id_rsa.pub)<br>cd .ssh<br>cp id_rsa.pub authorized_keys (将公钥复制给authorized_keys) &lt;分布式则要将所有节点id_rsa.pub相互复制&gt;</p><ul><li>2、 /mnt/share cp hadoop2.x /usr.hadoop</li></ul><p>解压tar xzvf hadoop 2.x</p><ul><li>3、修改core-site.xml、hadoop-env.sh、hdfs-site.xml、mapred-site.xml 、yarn-site.xml(hadoop2.x版本的配置文件在/hadoop2.x/etc/hadoop下)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">①core-site.xml：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fs.default.name</span><br><span class="line">hdfs://localhost:9000</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">② hadoop-env.sh：</span><br><span class="line">export JAVA_HOME=/usr/java/jdkxxx (jdk路径)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">③ hdfs-site.xml： 先创建好数据节点和名称节点的存放路径</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dfs.datanode.data.dir</span><br><span class="line">/user/hadoop/hadoop-2.5.1/data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dfs.namenode.name.dir</span><br><span class="line">/user/hadoop/hadoop-2.5.1/name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dfs.replication</span><br><span class="line">1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">④mapred-site.xml: (注意：这个文件是将/hadoop2.x/etc/hadoop下的mapred-site.xml.template复制并重命名 )</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mapreduce.framework.name</span><br><span class="line">yarn</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">⑤yarn-site.xml：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">yarn.nodemanager.aux-services</span><br><span class="line">mapreduce_shuffle</span><br></pre></td></tr></table></figure><ul><li>4、namenode格式化（一定要完成）</li></ul><p>终端：cd /usr/hadoop/hadoop-2.5.1/bin</p><p>./hdfs namenode -format (输入./hadoop namenode -format也行)</p><ul><li>5、运行hadoop</li></ul><p>终端： cd /usr/hadoop/hadoop-2.5.1/sbin (2.x版本后启动/停止在sbin目录下)<br>./start-hdfs.sh<br>./start-yarn.sh<br>(也可以只输入./start-all.sh)</p><p>输入jps查看启动项，当启动了NameNode、DataNode、SecondaryNameNode、ResourceManager、NodeManager即ok。</p><p>可进入Firefox中，输入端口号： localhost:50070 进入hadoop可视化页面。</p><h2 id="三、Hbase0-94安装"><a href="#三、Hbase0-94安装" class="headerlink" title="三、Hbase0.94安装"></a>三、Hbase0.94安装</h2><ul><li>1、/mnt/share cp hbase1.0.1 /usr.hbase</li></ul><p>解压tar xzvf hbase1.0.1</p><ul><li>2、修改hbase配置文件hbase-env.sh、hbase-site.xml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hbase-env.sh:</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/java/jdkxxxx (java路径)</span><br><span class="line">export HBASE_MANAGES_ZK=true (都得去掉前面#)</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hbase-site.xml：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hbase.rootdir</span><br><span class="line">hdfs://localhost:9000/hbase</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hbase.cluster.distributed</span><br><span class="line">true</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hbase.zookeeper.quorum</span><br><span class="line">localhost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hbase.tmp.dir</span><br><span class="line">file:/usr/hbase/tmp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hbase.zookeeper.property.dataDir</span><br><span class="line">file:/usr/hbase/zookeeperdata</span><br></pre></td></tr></table></figure><ul><li>3、运行hbase</li></ul><p>运行前需先启动hadoop，再进入hbase的bin目录下输入指令 ./start-hbase.sh<br>输入jps查看启动项，如有HMaster、HRegionServer、HQuormPeer,则说明hbase启动成功。<br>输入./hbase Shell (进入shell指令，可通过shell指令建表)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;只记录那二年踏过的坑&quot;&gt;&lt;a href=&quot;#只记录那二年踏过的坑&quot; class=&quot;headerlink&quot; title=&quot;只记录那二年踏过的坑&quot;&gt;&lt;/a&gt;只记录那二年踏过的坑&lt;/h1&gt;&lt;h2 id=&quot;一、安装Jdk：首先需要卸载系统自带的openjava，查看系统的
      
    
    </summary>
    
      <category term="搭建" scheme="http://www.wenchong.top/categories/%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="ambari" scheme="http://www.wenchong.top/tags/ambari/"/>
    
  </entry>
  
  <entry>
    <title>centos 6.5 内核升级</title>
    <link href="http://www.wenchong.top/2017/11/15/centos6.5%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/"/>
    <id>http://www.wenchong.top/2017/11/15/centos6.5内核升级/</id>
    <published>2017-11-14T16:00:00.000Z</published>
    <updated>2018-05-18T14:16:20.683Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-查看centos的内核版本"><a href="#1-查看centos的内核版本" class="headerlink" title="1.查看centos的内核版本"></a>1.查看centos的内核版本</h2><p>rname -r</p><h2 id="2-查看系统版本"><a href="#2-查看系统版本" class="headerlink" title="2.查看系统版本"></a>2.查看系统版本</h2><p>cat /etc/centos-release</p><h2 id="3-安装软件"><a href="#3-安装软件" class="headerlink" title="3.安装软件"></a>3.安装软件</h2><p>编译安装新内核，依赖于开发环境和开发库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum grouplist  //查看已经安装的和未安装的软件包组，来判断我们是否安装了相应的开发环境和开发库；</span><br><span class="line"></span><br><span class="line">yum groupinstall &quot;Development Tools&quot;  //一般是安装这两个软件包组，这样做会确定你拥有编译时所需的一切工具</span><br><span class="line"></span><br><span class="line">yum install ncurses-devel //你必须这样才能让 make *config 这个指令正确地执行</span><br><span class="line"></span><br><span class="line">yum install qt-devel //如果你没有 X 环境，这一条可以不用</span><br><span class="line"></span><br><span class="line">yum install hmaccalc zlib-devel binutils-devel elfutils-libelf-devel //创建 CentOS-6 内核时需要它们</span><br></pre></td></tr></table></figure></p><h2 id="4-编译内核"><a href="#4-编译内核" class="headerlink" title="4.编译内核"></a>4.编译内核</h2><p>Linux内核版本有两种：稳定版和开发版 ，Linux内核版本号由3个数字组成： r.x.y</p><ul><li>r: 主版本号</li><li>x: 次版本号，偶数表示稳定版本；奇数表示开发中版本。</li><li>y: 修订版本号 ， 表示修改的次数<br>官网上有stable, longterm等版本，longterm是比stable更稳定的版本。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">`[root@sean ~]#curl -O -L  https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.10.28.tar.xz</span><br><span class="line">[root@sean ~]# tar -xf linux-3.10.58.tar.xz -C /usr/src/</span><br><span class="line">[root@sean ~]# cd /usr/src/linux-3.10.58/</span><br><span class="line">[root@sean linux-3.10.58]# cp /boot/config-2.6.32-220.el6.x86_64 .config`</span><br></pre></td></tr></table></figure></li></ul><p>我们在系统原有的内核配置文件的基础上建立新的编译选项，所以复制一份到当前目录下，命名为.config。接下来继续配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">[root@sean linux-3.10.58]# sh -c &apos;yes &quot;&quot; | make oldconfig&apos;</span><br><span class="line">  </span><br><span class="line">HOSTCC  scripts/basic/fixdep</span><br><span class="line">  </span><br><span class="line">HOSTCC  scripts/kconfig/conf.o</span><br><span class="line">  </span><br><span class="line">SHIPPED scripts/kconfig/zconf.tab.c</span><br><span class="line">  </span><br><span class="line">SHIPPED scripts/kconfig/zconf.lex.c</span><br><span class="line">  </span><br><span class="line">SHIPPED scripts/kconfig/zconf.hash.c</span><br><span class="line">  </span><br><span class="line">HOSTCC  scripts/kconfig/zconf.tab.o</span><br><span class="line">  </span><br><span class="line">HOSTLD  scripts/kconfig/conf</span><br><span class="line"></span><br><span class="line">scripts/kconfig/conf --oldconfig Kconfig</span><br><span class="line"></span><br><span class="line">.config:555:warning: symbol value &apos;m&apos; invalid for PCCARD_NONSTATIC</span><br><span class="line">.config:2567:warning: symbol value &apos;m&apos; invalid for MFD_WM8400</span><br><span class="line">.config:2568:warning: symbol value &apos;m&apos; invalid for MFD_WM831X</span><br><span class="line">.config:2569:warning: symbol value &apos;m&apos; invalid for MFD_WM8350</span><br><span class="line">.config:2582:warning: symbol value &apos;m&apos; invalid for MFD_WM8350_I2C</span><br><span class="line">.config:2584:warning: symbol value &apos;m&apos; invalid for AB3100_CORE</span><br><span class="line">.config:3502:warning: symbol value &apos;m&apos; invalid for MMC_RICOH_MMC</span><br><span class="line"></span><br><span class="line">*</span><br><span class="line"></span><br><span class="line">* Restart config...</span><br><span class="line"></span><br><span class="line">*</span><br><span class="line"></span><br><span class="line">*</span><br><span class="line"></span><br><span class="line">* General setup</span><br><span class="line"></span><br><span class="line">*</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">XZ decompressor tester (XZ_DEC_TEST) [N/m/y/?] (NEW)</span><br><span class="line"></span><br><span class="line">Averaging functions (AVERAGE) [Y/?] (NEW) </span><br><span class="line">y</span><br><span class="line">CORDIC algorithm (CORDIC) [N/m/y/?] (NEW) </span><br><span class="line"></span><br><span class="line">JEDEC DDR data (DDR) [N/y/?] (NEW) </span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">configuration written to .config</span><br></pre></td></tr></table></figure></p><p>make oldconfig会读取当前目录下的.config文件，在.config文件里没有找到的选项则提示用户填写，然后备份.config文件为.config.old，并生成新的.config文件</p><h2 id="5-开始编译"><a href="#5-开始编译" class="headerlink" title="5.开始编译"></a>5.开始编译</h2><p>[root@sean linux-3.10.58]# make -j4 bzImage  //生成内核文件<br>[root@sean linux-3.10.58]# make -j4 modules  //编译模块<br>[root@sean linux-3.10.58]# make -j4 modules_install  //编译安装模块</p><p>-j后面的数字是线程数，用于加快编译速度，一般的经验是，逻辑CPU，就填写那个数字，例如有8核，则为-j8。（modules部分耗时30多分钟）</p><h2 id="6-安装"><a href="#6-安装" class="headerlink" title="6.安装"></a>6.安装</h2><p>[root@sean linux-3.10.58]# make install<br>实际运行到这一步时，出现ERROR: modinfo: could not find module vmware_balloon，但是不影响内核安装，是由于vsphere需要的模块没有编译，要避免这个问题，需要在make之前时修改.config文件，加入<br>HYPERVISOR_GUEST=yCONFIG_VMWARE_BALLOON=m<br>（这一部分比较容易出问题，参考下文异常部分）</p><h2 id="7-修改grub引导，重启"><a href="#7-修改grub引导，重启" class="headerlink" title="7.修改grub引导，重启"></a>7.修改grub引导，重启</h2><p>安装完成后，需要修改Grub引导顺序，让新安装的内核作为默认内核。<br>编辑 grub.conf文件，<br>vi /etc/grub.conf</p><p>#boot=/dev/sda<br>default=0<br>timeout=5<br>splashimage=(hd0,0)/grub/splash.xpm.gz<br>hiddenmenu<br>title CentOS (3.10.58)<br>    root (hd0,0)<br>…</p><p>数一下刚刚新安装的内核在哪个位置，从0开始，然后设置default为那个数字，一般新安装的内核在第一个位置，所以设置default=0。<br>重启reboot now</p><h2 id="8-确认当内核版本"><a href="#8-确认当内核版本" class="headerlink" title="8.确认当内核版本"></a>8.确认当内核版本</h2><p>[root@sean ~]# uname -r<br>3.10.58</p><p>升级内核成功!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-查看centos的内核版本&quot;&gt;&lt;a href=&quot;#1-查看centos的内核版本&quot; class=&quot;headerlink&quot; title=&quot;1.查看centos的内核版本&quot;&gt;&lt;/a&gt;1.查看centos的内核版本&lt;/h2&gt;&lt;p&gt;rname -r&lt;/p&gt;
&lt;h2 i
      
    
    </summary>
    
      <category term="运维" scheme="http://www.wenchong.top/categories/operation/"/>
    
    
      <category term="operation" scheme="http://www.wenchong.top/tags/operation/"/>
    
  </entry>
  
  <entry>
    <title>centos6.5下搭建hadoop2.7单机伪分布环境</title>
    <link href="http://www.wenchong.top/2017/10/01/centos6.5+hadoop2.7/"/>
    <id>http://www.wenchong.top/2017/10/01/centos6.5+hadoop2.7/</id>
    <published>2017-09-30T16:00:00.000Z</published>
    <updated>2018-05-18T14:15:22.366Z</updated>
    
    <content type="html"><![CDATA[<h3 id="设置固定IP地址及网关"><a href="#设置固定IP地址及网关" class="headerlink" title="设置固定IP地址及网关"></a>设置固定IP地址及网关</h3><ul><li><p>设置IP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure></li><li><p>修改内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=eth0</span><br><span class="line">HWADDR=08:00:27:BD:9D:B5  #不用改</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">UUID=53e4e4b6-9724-43ab-9da7-68792e611031 #不用改</span><br><span class="line">ONBOOT=yes  #开机启动</span><br><span class="line">NM_CONTROLLED=yes</span><br><span class="line">BOOTPROTO=static  #静态IP</span><br><span class="line">IPADDR=192.168.30.50  #IP地址</span><br><span class="line">NETMASK=255.255.255.0 #子网掩码</span><br></pre></td></tr></table></figure></li><li><p>设置网关</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sysconfig/network</span><br></pre></td></tr></table></figure></li><li><p>添加内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HOSTNAME=Hadoop.Master</span><br><span class="line">GATEWAY=192.168.30.1 #网关</span><br></pre></td></tr></table></figure></li><li><p>设置DNS<br>vi /etc/resolv.conf</p></li><li>添加内容<br>nameserver xxx.xxx.xxx.xxx #根据实际情况设置<br>nameserver 114.114.114.114 #可以设置多个</li><li><p>重启网卡</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service network restart</span><br></pre></td></tr></table></figure></li><li><p>设置主机名对应IP地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/hosts</span><br></pre></td></tr></table></figure></li></ul><p>#添加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.30.50 Hadoop.Master</span><br></pre></td></tr></table></figure></p><h3 id="添加Hadoop用户"><a href="#添加Hadoop用户" class="headerlink" title="添加Hadoop用户"></a>添加Hadoop用户</h3><ul><li><p>添加用户组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupadd hadoop</span><br></pre></td></tr></table></figure></li><li><p>添加用户并分配用户组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd -g hadoop hadoop</span><br></pre></td></tr></table></figure></li><li><p>修改用户密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd hadoop</span><br></pre></td></tr></table></figure></li></ul><h3 id="关闭服务"><a href="#关闭服务" class="headerlink" title="关闭服务"></a>关闭服务</h3><ul><li><p>关闭防火墙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop #关闭防火墙服务</span><br><span class="line">chkconfig iptables off #关闭防火墙开机启动</span><br><span class="line">service ip6tables stop</span><br><span class="line">chkconfig ip6tables off</span><br></pre></td></tr></table></figure></li><li><p>关闭SELinux</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sysconfig/selinux</span><br></pre></td></tr></table></figure></li></ul><p>#修改如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELINUX=enforcing -&gt; SELINUX=disabled</span><br></pre></td></tr></table></figure></p><p>#再执行如下命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">setenforce 0</span><br><span class="line">getenforce</span><br></pre></td></tr></table></figure></p><ul><li>关闭其他服务<h3 id="VSFTP安装与配置"><a href="#VSFTP安装与配置" class="headerlink" title="VSFTP安装与配置"></a>VSFTP安装与配置</h3></li><li><p>检查是否安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig --list|grep vsftpd</span><br></pre></td></tr></table></figure></li><li><p>安装vsftp</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install vsftpd</span><br></pre></td></tr></table></figure></li><li><p>创建日志文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch /var/log/vdftpd.log</span><br></pre></td></tr></table></figure></li><li><p>配置vsftpd服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/vsftpd/vsftpd.conf</span><br></pre></td></tr></table></figure></li></ul><p>#修改如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">anonymous_enable=NO #关闭匿名访问</span><br><span class="line">xferlog_file=/var/log/vsftpd.log #设置日志文件 -- 我们上一步所创建的文件</span><br><span class="line">idle_session_timeout=600 #会话超时时间</span><br><span class="line">async_abor_enable=YES  #开启异步传输</span><br><span class="line">ascii_upload_enable=YES #开启ASCII上传</span><br><span class="line">ascii_download_enable=YES #开启ASCII下载</span><br></pre></td></tr></table></figure></p><ul><li>查看vsftp运行状态<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service vsftpd status</span><br></pre></td></tr></table></figure></li></ul><h3 id="启动vsftp"><a href="#启动vsftp" class="headerlink" title="启动vsftp"></a>启动vsftp</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service vsftpd start</span><br><span class="line">#重启 service vsftpd restart</span><br><span class="line">#关闭 service vsftpd stop</span><br></pre></td></tr></table></figure><ul><li><p>查看vsftpd服务启动项</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig --list|grep vsftpd</span><br></pre></td></tr></table></figure></li><li><p>设置vsftp开机启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig vsftpd on</span><br></pre></td></tr></table></figure></li></ul><h3 id="SSH无密码配置"><a href="#SSH无密码配置" class="headerlink" title="SSH无密码配置"></a>SSH无密码配置</h3><ul><li><p>查看ssh与rsync安装状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa|grep openssh</span><br><span class="line">rpm -qa|grep rsync</span><br></pre></td></tr></table></figure></li><li><p>安装ssh与rsync</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ssh</span><br><span class="line">yum -y install rsync</span><br></pre></td></tr></table></figure></li><li><p>切换hadoop用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br></pre></td></tr></table></figure></li><li><p>生成ssh密码对</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa</span><br></pre></td></tr></table></figure></li><li><p>将id_dsa.pub追加到授权的key中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></li></ul><h3 id="设置授权key权限"><a href="#设置授权key权限" class="headerlink" title="设置授权key权限"></a>设置授权key权限</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p>#权限的设置非常重要，因为不安全的设置安全设置，会让你不能使用RSA功能</p><h3 id="测试ssh连接"><a href="#测试ssh连接" class="headerlink" title="测试ssh连接"></a>测试ssh连接</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost</span><br></pre></td></tr></table></figure><p>#如果不需要输入密码，则是成功</p><h2 id="安装Java"><a href="#安装Java" class="headerlink" title="安装Java"></a>安装Java</h2><ul><li>下载地址<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html</span><br></pre></td></tr></table></figure></li></ul><p>注：我这里使用的是：jdk-7u80-linux-i586.tar.gz</p><ul><li><p>安装Java<br>切换至root用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su root</span><br></pre></td></tr></table></figure></li><li><p>创建/usr/java文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/java</span><br></pre></td></tr></table></figure></li><li><p>使用winscp工具上传至服务器</p></li><li><p>将压缩包上传至/home/hadoop目录</p></li></ul><p>注：我这里使用的是winscp，使用hadoop用户连接</p><ul><li><p>将压缩包解压至/usr/java 目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf /home/hadoop/jdk-7u80-linux-i586.tar.gz -C /usr/java/</span><br></pre></td></tr></table></figure></li><li><p>设置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br></pre></td></tr></table></figure></li></ul><p>#追加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_80</span><br><span class="line">export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure></p><ul><li><p>使环境变量生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>测试环境变量设置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li></ul><h2 id="Hadoop安装与配置"><a href="#Hadoop安装与配置" class="headerlink" title="Hadoop安装与配置"></a>Hadoop安装与配置</h2><p>下载地址<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://hadoop.apache.org/releases.html</span><br></pre></td></tr></table></figure></p><p>注:我下载的是hadoop-2.7.1.tar.gz</p><h3 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h3><p>使用winscp工具上传至服务器<br>将压缩包上传至/home/hadoop目录<br>*将压缩包解压至/usr目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf /home/hadoop/hadoop-2.7.1.tar.gz -C /usr/</span><br></pre></td></tr></table></figure></p><ul><li><p>修改文件夹名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv /usr/hadoop-2.7.1/ /usr/hadoop</span><br></pre></td></tr></table></figure></li><li><p>创建hadoop数据目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/hadoop/tmp</span><br></pre></td></tr></table></figure></li><li><p>将hadoop文件夹授权给hadoop用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown -R hadoop:hadoop /usr/hadoop/</span><br></pre></td></tr></table></figure></li><li><p>设置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br></pre></td></tr></table></figure></li></ul><p>#追加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib</span><br></pre></td></tr></table></figure></p><ul><li><p>使环境变量生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>测试环境变量设置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure></li></ul><h3 id="配置HDFS"><a href="#配置HDFS" class="headerlink" title="配置HDFS"></a>配置HDFS</h3><ul><li><p>切换至Hadoop用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br></pre></td></tr></table></figure></li><li><p>修改hadoop-env.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/hadoop/etc/hadoop/</span><br></pre></td></tr></table></figure></li></ul><p>vi hadoop-env.sh </p><p>#追加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.7.0_80</span><br></pre></td></tr></table></figure></p><ul><li><p>修改core-site.xml<br>vi core-site.xml<br>#添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://Hadoop.Master:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/usr/hadoop/tmp/&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;A base for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>修改hdfs-site.xml<br>vi hdfs-site.xml<br>#添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>格式化hdfs<br>hdfs namenode -format<br>注：出现Exiting with status 0即为成功</p></li><li><p>启动hdfs<br>start-dfs.sh<br>#停止命令 stop-dfs.sh<br>注：输出如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">15/09/21 18:09:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Starting namenodes on [Hadoop.Master]</span><br><span class="line">Hadoop.Master: starting namenode, logging to /usr/hadoop/logs/hadoop-hadoop-namenode-Hadoop.Master.out</span><br><span class="line">Hadoop.Master: starting datanode, logging to /usr/hadoop/logs/hadoop-hadoop-datanode-Hadoop.Master.out</span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br><span class="line">The authenticity of host &apos;0.0.0.0 (0.0.0.0)&apos; can&apos;t be established.</span><br><span class="line">RSA key fingerprint is b5:96:b2:68:e6:63:1a:3c:7d:08:67:4b:ae:80:e2:e3.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">0.0.0.0: Warning: Permanently added &apos;0.0.0.0&apos; (RSA) to the list of known hosts.</span><br><span class="line">0.0.0.0: starting secondarynamenode, logging to /usr/hadoop/logs/hadoop-hadoop-secondarynamenode-Hadoop.Master.out</span><br><span class="line">15/09/21 18:09:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicab</span><br></pre></td></tr></table></figure></li><li><p>查看进程<br>jps<br>注：输出类似如下内容<br>1763 NameNode<br>1881 DataNode<br>2146 Jps<br>2040 SecondaryNameNode</p></li><li>使用web查看Hadoop运行状态<br><a href="http://你的服务器ip地址:50070/" target="_blank" rel="noopener">http://你的服务器ip地址:50070/</a><h3 id="在HDFS上运行WordCount"><a href="#在HDFS上运行WordCount" class="headerlink" title="在HDFS上运行WordCount"></a>在HDFS上运行WordCount</h3></li><li>创建HDFS用户目录<br>hdfs dfs -mkdir /user<br>hdfs dfs -mkdir /user/hadoop #根据自己的情况调整/user/<username></username></li><li>复制输入文件（要处理的文件）到HDFS上<br>hdfs dfs -put /usr/hadoop/etc/hadoop input</li><li>查看我们复制到HDFS上的文件<br>hdfs dfs -ls input</li><li>运行单词检索（grep）程序<br>hadoop jar /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input output ‘dfs[a-z.]+’<br>#WordCount<br>#hadoop jar /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount input output<br>#说明：output文件夹如已经存在则需要删除或指定其他文件夹。</li><li>查看运行结果<br>hdfs dfs -cat output/*<h3 id="配置YARN"><a href="#配置YARN" class="headerlink" title="配置YARN"></a>配置YARN</h3></li><li><p>修改mapred-site.xml<br>cd /usr/hadoop/etc/hadoop/<br>cp mapred-site.xml.template mapred-site.xml<br>vi mapred-site.xml<br>#添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>修改yarn-site.xml<br>vi yarn-site.xml<br>#添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>启动YARN<br>start-yarn.sh<br>#停止yarn stop-yarn.sh</p></li><li>查看当前java进程<br>jsp<br>#输出如下<br>4918 ResourceManager<br>1663 NameNode<br>1950 SecondaryNameNode<br>5010 NodeManager<br>5218 Jps<br>1759 DataNode</li><li><p>运行你的mapReduce程序</p></li><li><p>配置好如上配置再运行mapReduce程序时即是yarn中运行。</p></li><li><p>使用web查看Yarn运行状态<br><a href="http://你的服务器ip地址:8088/" target="_blank" rel="noopener">http://你的服务器ip地址:8088/</a></p><h3 id="HDFS常用命令"><a href="#HDFS常用命令" class="headerlink" title="HDFS常用命令"></a>HDFS常用命令</h3></li><li>创建HDFS文件夹</li><li>在根目录创建input文件夹<br>hdfs dfs -mkdir -p /input</li><li>在用户目录创建input文件夹<br>说明：如果不指定“/目录”，则默认在用户目录创建文件夹<br>hdfs dfs -mkdir -p input<br>#等同于 hdfs dfs -mkdir -p /user/hadoop/input</li><li>查看HDFS文件夹</li><li>查看HDFS根文件夹<br>hdfs  dfs  -ls /</li><li>查看HDFS用户目录文件夹<br>hdfs  dfs  -ls</li><li>查看HDFS用户目录文件夹下input文件夹<br>hdfs  dfs  -ls input<br>#等同与 hdfs  dfs  -ls /user/hadoop/input</li><li>复制文件到HDFS<br>hdfs dfs -put /usr/hadoop/etc/hadoop input</li><li>删除文件夹<br>hdfs  dfs  -rm -r input</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;设置固定IP地址及网关&quot;&gt;&lt;a href=&quot;#设置固定IP地址及网关&quot; class=&quot;headerlink&quot; title=&quot;设置固定IP地址及网关&quot;&gt;&lt;/a&gt;设置固定IP地址及网关&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;设置IP&lt;/p&gt;
&lt;figure class=&quot;h
      
    
    </summary>
    
      <category term="搭建" scheme="http://www.wenchong.top/categories/%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="ambari" scheme="http://www.wenchong.top/tags/ambari/"/>
    
  </entry>
  
  <entry>
    <title>hive参数优化</title>
    <link href="http://www.wenchong.top/2017/08/29/hive%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/"/>
    <id>http://www.wenchong.top/2017/08/29/hive参数优化/</id>
    <published>2017-08-28T16:00:00.000Z</published>
    <updated>2018-02-18T17:03:39.661Z</updated>
    
    <content type="html"><![CDATA[<h3 id="只记录那二年踏过的坑"><a href="#只记录那二年踏过的坑" class="headerlink" title="只记录那二年踏过的坑"></a>只记录那二年踏过的坑</h3><ul><li>1.设置合理solt数<br>mapred.tasktracker.map.tasks.maximum<br>每个tasktracker可同时运行的最大map task数，默认值2。<br>mapred.tasktracker.reduce.tasks.maximum<br>每个tasktracker可同时运行的最大reduce task数，默认值1</li><li>2.配置磁盘块<br>mapred.local.dir<br>map task中间结果写本地磁盘路径，默认值${hadoop.tmp.dir}/mapred/local。<br>可配置多块磁盘缓解写压力。当存在多个可以磁盘时，Hadoop将采用轮询方式将不同的map task中间结果写到磁盘上。</li><li>3.配置RPC Handler数<br>mapred.job.tracker.handler.count<br>jobtracker可并发处理来自tasktracker的RPC请求数，默认值10。</li><li>4.配置HTTP线程数<br>tasktracker.http.threads<br>HTTP服务器的工作线程数，用于获取map task的输出结果，默认值40。</li><li>5.启用批调度</li><li>6.选择合适的压缩算法<br>Job输出结果是否压缩<br>mapred.output.compress<br>是否压缩，默认值false。<br>mapred.output.compression.type<br>压缩类型，有NONE, RECORD和BLOCK，默认值RECORD。<br>mapred.output.compression.codec<br>压缩算法，默认值org.apache.hadoop.io.compress.DefaultCodec。<br>map task输出是否压缩<br>mapred.compress.map.output<br>是否压缩，默认值false<br>mapred.map.output.compression.codec<br>压缩算法，默认值org.apache.hadoop.io.compress.DefaultCodec。</li><li>7.设置失败容忍度<br>mapred.max.map.failures.percent<br>例如：set mapred.max.map.failures.percent=30;<br>作业最多允许失败的map task比例，默认值0。<br>mapred.max.reduce.failures.percent<br>作业最多允许失败的reduce task比例，默认值0。<br>mapred.map.max.attempts<br>一个map task的最多重试次数，默认值4。<br>mapred.reduce.max.attempts<br>一个reduce task的最多重试次数，默认值4。</li><li>8.设置跳过坏记录<br>mapred.skip.attempts.to.start.skipping<br>当任务失败次数达到该值时，启用跳过坏记录功能，默认值2。</li></ul><p>mapred.skip.out.dir<br>检测出的坏记录存放目录，默认值为输出目录的_logs/skip，设置为none表示不输出。<br>mapred.skip.map.max.skip.records<br>map task最多允许的跳过记录数，默认值0。<br>mapred.skip.reduce.max.skip.groups<br>reduce task最多允许的跳过记录数，默认值0。</p><ul><li>9.配置jvm重用<br>mapred.job.reuse.jvm.num.tasks<br>一个jvm可连续启动多个同类型任务，默认值1，若为-1表示不受限制。</li><li>10.配置jvm参数<br>mapred.child.java.opts<br>任务启动的jvm参数，默认值-Xmx200m，建议值-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m -verbose:gc -Xloggc:/tmp/@taskid@.gc</li><li><ol><li>map task调优<br>io.sort.mb<br>默认值100M<br>io.sort.record.percent<br>默认值0.05<br>io.sort.spill.percent<br>默认值0.80</li></ol></li><li>12.reduce task调优<br>io.sort.factor<br>默认值10<br>mapred.reduce.parallel.copies<br>默认值5</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;只记录那二年踏过的坑&quot;&gt;&lt;a href=&quot;#只记录那二年踏过的坑&quot; class=&quot;headerlink&quot; title=&quot;只记录那二年踏过的坑&quot;&gt;&lt;/a&gt;只记录那二年踏过的坑&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;1.设置合理solt数&lt;br&gt;mapred.tasktracke
      
    
    </summary>
    
      <category term="hive" scheme="http://www.wenchong.top/categories/hive/"/>
    
    
      <category term="Hhive" scheme="http://www.wenchong.top/tags/Hhive/"/>
    
  </entry>
  
</feed>
