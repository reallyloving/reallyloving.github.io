<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python——基础数据类型]]></title>
    <url>%2F2018%2F05%2F16%2FPython%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[** Python 中的变量不需要声明。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。在 Python 中，变量就是变量，它没有类型，我们所说的”类型”是变量所指的内存中对象的类型。 等号（=）用来给变量赋值。 等号（=）运算符左边是一个变量名,等号（=）运算符右边是存储在变量中的值。例如： ** 123456789#!/usr/bin/python3 counter = 100 # 整型变量miles = 1000.0 # 浮点型变量name = &quot;小二&quot; # 字符串 print (counter)print (miles)print (name) 运行实例 »执行以上程序会输出如下结果： 1001000.0小二 多个变量赋值Python允许你同时为多个变量赋值。例如：1a = b = c = 1 以上实例，创建一个整型对象，值为 1，三个变量都指向同一个内存位置。 您也可以为多个对象指定多个变量。例如：1a, b, c = 1, 2, &quot;小二&quot; 以上实例，两个整型对象 1 和 2 的分配给变量 a 和 b，字符串对象 “小二” 分配给变量 c。 标准数据类型Python3 中有六个标准的数据类型： Number（数字）String（字符串）List（列表）Tuple（元组）Sets（集合）Dictionary（字典）Python3 的六个标准数据类型中： 不可变数据（四个）：Number（数字）、String（字符串）、Tuple（元组）、Sets（集合）；可变数据（两个）：List（列表）、Dictionary（字典）。 Number（数字）Python3 支持 int、float、bool、complex（复数）。 在Python 3里，只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 像大多数语言一样，数值类型的赋值和计算都是很直观的。 内置的 type() 函数可以用来查询变量所指的对象类型。123&gt;&gt;&gt; a, b, c, d = 20, 5.5, True, 4+3j&gt;&gt;&gt; print(type(a), type(b), type(c), type(d))&lt;class &apos;int&apos;&gt; &lt;class &apos;float&apos;&gt; &lt;class &apos;bool&apos;&gt; &lt;class &apos;complex&apos;&gt; 此外还可以用 isinstance 来判断：1234&gt;&gt;&gt;a = 111&gt;&gt;&gt; isinstance(a, int)True&gt;&gt;&gt; 您也可以使用del语句删除一些对象引用。del语句的语法是： del var1[,var2[,var3[….,varN]]]] 数值运算实例：1234567891011121314&gt;&gt;&gt;5 + 4 # 加法9&gt;&gt;&gt; 4.3 - 2 # 减法2.3&gt;&gt;&gt; 3 * 7 # 乘法21&gt;&gt;&gt; 2 / 4 # 除法，得到一个浮点数0.5&gt;&gt;&gt; 2 // 4 # 除法，得到一个整数0&gt;&gt;&gt; 17 % 3 # 取余 2&gt;&gt;&gt; 2 ** 5 # 乘方32 1、Python可以同时为多个变量赋值，如a, b = 1, 2。2、一个变量可以通过赋值指向不同类型的对象。3、数值的除法包含两个运算符：/ 返回一个浮点数，// 返回一个整数。4、在混合计算时，Python会把整型转换成为浮点数。 String（字符串）Python中的字符串用单引号(‘)或双引号(“)括起来，同时使用反斜杠()转义特殊字符。 字符串的截取的语法格式如下：1变量[头下标:尾下标] 索引值以 0 为开始值，-1 为从末尾的开始位置。 加号 (+) 是字符串的连接符， 星号 (*) 表示复制当前字符串，紧跟的数字为复制的次数。实例如下： 实例1234567891011#!/usr/bin/python3 str = &apos;xiaoer&apos; print (str) # 输出字符串print (str[0:-1]) # 输出第一个到倒数第二个的所有字符print (str[0]) # 输出字符串第一个字符print (str[2:5]) # 输出从第三个开始到第五个的字符print (str[2:]) # 输出从第三个开始的后的所有字符print (str * 2) # 输出字符串两次print (str + &quot;TEST&quot;) # 连接字符串 执行以上程序会输出如下结果： xiaoerxiaoexaoraoerxiaoerxiaoerxiaoerTEST Python 使用反斜杠()转义特殊字符，如果你不想让反斜杠发生转义，可以在字符串前面添加一个 r，表示原始字符串：1234567&gt;&gt;&gt; print(&apos;xi\aoer&apos;)xiaoer&gt;&gt;&gt; print(r&apos;xi\aoer&apos;)xi\aoer&gt;&gt;&gt; ` 另外，反斜杠()可以作为续行符，表示下一行是上一行的延续。也可以使用 “””…””” 或者 ‘’’…’’’ 跨越多行。 注意，Python 没有单独的字符类型，一个字符就是长度为1的字符串。 实例12345&gt;&gt;&gt;word = &apos;Python&apos;&gt;&gt;&gt; print(word[0], word[5])P n&gt;&gt;&gt; print(word[-1], word[-6])n P 与 C 字符串不同的是，Python 字符串不能被改变。向一个索引位置赋值，比如word[0] = ‘m’会导致错误。 注意： 1、反斜杠可以用来转义，使用r可以让反斜杠不发生转义。2、字符串可以用+运算符连接在一起，用*运算符重复。3、Python中的字符串有两种索引方式，从左往右以0开始，从右往左以-1开始。4、Python中的字符串不能改变。 List（列表）List（列表） 是 Python 中使用最频繁的数据类型。 列表可以完成大多数集合类的数据结构实现。列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。 列表是写在方括号([])之间、用逗号分隔开的元素列表。 和字符串一样，列表同样可以被索引和截取，列表被截取后返回一个包含所需元素的新列表。 列表截取的语法格式如下：1变量[头下标:尾下标] 索引值以 0 为开始值，-1 为从末尾的开始位置。 加号（+）是列表连接运算符，星号（*）是重复操作。如下实例： 实例1234567891011#!/usr/bin/python3 list = [ &apos;abcd&apos;, 786 , 2.23, &apos;xiaoer&apos;, 70.2 ]tinylist = [123, &apos;xiaoer&apos;] print (list) # 输出完整列表print (list[0]) # 输出列表第一个元素print (list[1:3]) # 从第二个开始输出到第三个元素print (list[2:]) # 输出从第三个元素开始的所有元素print (tinylist * 2) # 输出两次列表print (list + tinylist) # 连接列表 以上实例输出结果：123456[&apos;abcd&apos;, 786, 2.23, &apos;xiaoer&apos;, 70.2]abcd[786, 2.23][2.23, &apos;xiaoer&apos;, 70.2][123, &apos;xiaoer&apos;, 123, &apos;xiaoer&apos;][&apos;abcd&apos;, 786, 2.23, &apos;xiaoer&apos;, 70.2, 123, &apos;xiaoer&apos;] 与Python字符串不一样的是，列表中的元素是可以改变的： 实例12345678&gt;&gt;&gt;a = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; a[0] = 9&gt;&gt;&gt; a[2:5] = [13, 14, 15]&gt;&gt;&gt; a[9, 2, 13, 14, 15, 6]&gt;&gt;&gt; a[2:5] = [] # 将对应的元素值设置为 [] &gt;&gt;&gt; a[9, 2, 6] List内置了有很多方法，例如append()、pop()等等。 注意： 1、List写在方括号之间，元素用逗号隔开。2、和字符串一样，list可以被索引和切片。3、List可以使用+操作符进行拼接。4、List中的元素是可以改变的。 元祖元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号 () 里，元素之间用逗号隔开。 元组中的元素类型也可以不相同： 实例1234567891011#!/usr/bin/python3 tuple = ( &apos;abcd&apos;, 786 , 2.23, &apos;xiaoer&apos;, 70.2 )tinytuple = (123, &apos;xiaoer&apos;) print (tuple) # 输出完整元组print (tuple[0]) # 输出元组的第一个元素print (tuple[1:3]) # 输出从第二个元素开始到第三个元素print (tuple[2:]) # 输出从第三个元素开始的所有元素print (tinytuple * 2) # 输出两次元组print (tuple + tinytuple) # 连接元组 以上实例输出结果：123456(&apos;abcd&apos;, 786, 2.23, &apos;xiaoer&apos;, 70.2)abcd(786, 2.23)(2.23, &apos;xiaoer&apos;, 70.2)(123, &apos;xiaoer&apos;, 123, &apos;xiaoer&apos;)(&apos;abcd&apos;, 786, 2.23, &apos;xiaoer&apos;, 70.2, 123, &apos;xiaoer&apos;) 元组与字符串类似，可以被索引且下标索引从0开始，-1 为从末尾开始的位置。也可以进行截取（看上面，这里不再赘述）。 其实，可以把字符串看作一种特殊的元组。 实例12345678910&gt;&gt;&gt;tup = (1, 2, 3, 4, 5, 6)&gt;&gt;&gt; print(tup[0])1&gt;&gt;&gt; print(tup[1:5])(2, 3, 4, 5)&gt;&gt;&gt; tup[0] = 11 # 修改元组元素的操作是非法的Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: &apos;tuple&apos; object does not support item assignment&gt;&gt;&gt; 虽然tuple的元素不可改变，但它可以包含可变的对象，比如list列表。 构造包含 0 个或 1 个元素的元组比较特殊，所以有一些额外的语法规则： tup1 = () # 空元组tup2 = (20,) # 一个元素，需要在元素后添加逗号string、list和tuple都属于sequence（序列）。 注意： 1、与字符串一样，元组的元素不能修改。2、元组也可以被索引和切片，方法一样。3、注意构造包含0或1个元素的元组的特殊语法规则。4、元组也可以使用+操作符进行拼接。 Set（集合）集合（set）是一个无序不重复元素的序列。 基本功能是进行成员关系测试和删除重复元素。 可以使用大括号 { } 或者 set() 函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。 创建格式：1parame = &#123;value01,value02,...&#125; 或者1set(value) 实例1234567891011121314151617181920212223242526#!/usr/bin/python3 student = &#123;&apos;Tom&apos;, &apos;Jim&apos;, &apos;Mary&apos;, &apos;Tom&apos;, &apos;Jack&apos;, &apos;Rose&apos;&#125; print(student) # 输出集合，重复的元素被自动去掉 # 成员测试if(&apos;Rose&apos; in student) : print(&apos;Rose 在集合中&apos;)else : print(&apos;Rose 不在集合中&apos;) # set可以进行集合运算a = set(&apos;abracadabra&apos;)b = set(&apos;alacazam&apos;) print(a) print(a - b) # a和b的差集 print(a | b) # a和b的并集 print(a &amp; b) # a和b的交集 print(a ^ b) # a和b中不同时存在的元素 以上实例输出结果：1234567&#123;&apos;Mary&apos;, &apos;Jim&apos;, &apos;Rose&apos;, &apos;Jack&apos;, &apos;Tom&apos;&#125;Rose 在集合中&#123;&apos;b&apos;, &apos;a&apos;, &apos;c&apos;, &apos;r&apos;, &apos;d&apos;&#125;&#123;&apos;b&apos;, &apos;d&apos;, &apos;r&apos;&#125;&#123;&apos;l&apos;, &apos;r&apos;, &apos;a&apos;, &apos;c&apos;, &apos;z&apos;, &apos;m&apos;, &apos;b&apos;, &apos;d&apos;&#125;&#123;&apos;a&apos;, &apos;c&apos;&#125;&#123;&apos;l&apos;, &apos;r&apos;, &apos;z&apos;, &apos;m&apos;, &apos;b&apos;, &apos;d&apos;&#125; Dictionary（字典）字典（dictionary）是Python中另一个非常有用的内置数据类型。 列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。 字典是一种映射类型，字典用”{ }”标识，它是一个无序的键(key) : 值(value)对集合。 键(key)必须使用不可变类型。 在同一个字典中，键(key)必须是唯一的。 实例1234567891011121314#!/usr/bin/python3 dict = &#123;&#125;dict[&apos;one&apos;] = &quot;1 - 小二客栈&quot;dict[2] = &quot;2 - 小二最牛&quot; tinydict = &#123;&apos;name&apos;: &apos;runoob&apos;,&apos;code&apos;:1, &apos;site&apos;: &apos;www.runoob.com&apos;&#125; print (dict[&apos;one&apos;]) # 输出键为 &apos;one&apos; 的值print (dict[2]) # 输出键为 2 的值print (tinydict) # 输出完整的字典print (tinydict.keys()) # 输出所有键print (tinydict.values()) # 输出所有值 以上实例输出结果：123451 - 小二客栈2 - 小二最牛&#123;&apos;name&apos;: &apos;runoob&apos;, &apos;site&apos;: &apos;www.runoob.com&apos;, &apos;code&apos;: 1&#125;dict_keys([&apos;name&apos;, &apos;site&apos;, &apos;code&apos;])dict_values([&apos;runoob&apos;, &apos;www.runoob.com&apos;, 1]) 构造函数 dict() 可以直接从键值对序列中构建字典如下： 实例12345678&gt;&gt;&gt;dict([(&apos;xiaoer&apos;, 1), (&apos;Google&apos;, 2), (&apos;Taobao&apos;, 3)])&#123;&apos;Taobao&apos;: 3, &apos;xiaoer&apos;: 1, &apos;Google&apos;: 2&#125; &gt;&gt;&gt; &#123;x: x**2 for x in (2, 4, 6)&#125;&#123;2: 4, 4: 16, 6: 36&#125; &gt;&gt;&gt; dict(xiaoer=1, Google=2, Taobao=3)&#123;&apos;Taobao&apos;: 3, &apos;xiaoer&apos;: 1, &apos;Google&apos;: 2&#125; 另外，字典类型也有一些内置的函数，例如clear()、keys()、values()等。 注意： 1、字典是一种映射类型，它的元素是键值对。2、字典的关键字必须为不可变类型，且不能重复。3、创建空字典使用 { }。 Python数据类型转换有时候，我们需要对数据内置的类型进行转换，数据类型的转换，你只需要将数据类型作为函数名即可。 以下几个内置的函数可以执行数据类型之间的转换。这些函数返回一个新的对象，表示转换的值。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960函数 描述int(x [,base])将x转换为一个整数float(x)将x转换到一个浮点数complex(real [,imag])创建一个复数str(x)将对象 x 转换为字符串repr(x)将对象 x 转换为表达式字符串eval(str)用来计算在字符串中的有效Python表达式,并返回一个对象tuple(s)将序列 s 转换为一个元组list(s)将序列 s 转换为一个列表set(s)转换为可变集合dict(d)创建一个字典。d 必须是一个序列 (key,value)元组。frozenset(s)转换为不可变集合chr(x)将一个整数转换为一个字符ord(x)将一个字符转换为它的整数值hex(x)将一个整数转换为一个十六进制字符串oct(x)将一个整数转换为一个八进制字符串]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础语法]]></title>
    <url>%2F2018%2F05%2F16%2FPython%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[编码Python3源码文件以UTF-8编码，所有字符串都是Unicode字符串。 标识符第一个字符必须是字母表中字母或下划线_。标识符的其他部分有字母、数字和下划线组成。标识符对大小写敏感。 Python保留字保留字即关键字。不能把它们用作任何标识符名称。 12&gt;&gt;&gt; keyword.kwlist[&apos;False&apos;, &apos;None&apos;, &apos;True&apos;, &apos;and&apos;, &apos;as&apos;, &apos;assert&apos;, &apos;break&apos;, &apos;class&apos;, &apos;continue&apos;, &apos;def&apos;, &apos;del&apos;, &apos;elif&apos;, &apos;else&apos;, &apos;except&apos;, &apos;finally&apos;, &apos;for&apos;, &apos;from&apos;, &apos;global&apos;, &apos;if&apos;, &apos;import&apos;, &apos;in&apos;, &apos;is&apos;, &apos;lambda&apos;, &apos;nonlocal&apos;, &apos;not&apos;, &apos;or&apos;, &apos;pass&apos;, &apos;raise&apos;, &apos;return&apos;, &apos;try&apos;, &apos;while&apos;, &apos;with&apos;, &apos;yield&apos;] 注释Python中单行注释以#开头多行注释可以用多个#或者’’’和”””: 1234567891011121314#!/usr/bin/python3# 第一个注释# 第二个注释&apos;&apos;&apos;第三注释第四注释&apos;&apos;&apos;&quot;&quot;&quot;第五注释第六注释&quot;&quot;&quot; 行与缩进Python最具特色的就是使用缩进来表示代码块。缩进的空格数是可变的，但是同一个代码块语句biubiu包含相同的缩进空格数。例： 1234if True: print (&quot;True&quot;)else: print (&quot;False&quot;) ** 缩进不一致，会导致运行错误。错误类型如下：IndentationError: unindent does not match any outer indentation level 多行语句Python通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠（\）来实现多行语句，例如：123total = item_one + \ item_two + \ item_three ** 在 [], {}, 或 () 中的多行语句，不需要使用反斜杠()。 数字类型Python中有是四种类型：整数、布尔型、浮点数和复数。int 整数bool 布尔float 浮点数complex 复数 字符串Python中单引号和双引号使用完全相同。使用三引号可以指定一个多行字符串。转义符’\’反斜杠可以用来转义，但r可以让反斜杠不发生转义。如： 1r&quot;this is a line with \n&quot; 则\n会显示，并不是换行。 按字面意义级联字符串，如”this””is””string”会被自动转换成this id string。字符串可以用+运算符连接诶在一起，用*运算符重复。Python中的字符串有两种索引方式，从左往右以0开始，从右往左以-1开始。Python中的字符串不能改变。Python没有单独的字符类型，一个字符就是长度为1的字符串。字符串的截取的语法格式如下：变量[头下标:尾下标]1234word = &apos;字符串&apos;sentence = &quot;这是一个句子。&quot;paragraph = &quot;&quot;&quot;这是一个段落，可以由多行组成&quot;&quot;&quot; 实例：1234567891011121314151617181920212223242526272829#!/usr/bin/python3str=&apos;Runoob&apos;print(str) # 输出字符串print(str[0:-1]) # 输出第一个到倒数第二个的所有字符print(str[0]) # 输出字符串第一个字符print(str[2:5]) # 输出从第三个开始到第五个的字符print(str[2:]) # 输出从第三个开始的后的所有字符print(str * 2) # 输出字符串两次print(str + &apos;你好&apos;) # 连接字符串print(&apos;------------------------------&apos;)print(&apos;hello\nrunoob&apos;) # 使用反斜杠(\)+n转义特殊字符print(r&apos;hello\nrunoob&apos;) # 在字符串前面添加一个 r，表示原始字符串，不会发生转义输出结果为：RunoobRunooRnoonoobRunoobRunoobRunoob你好------------------------------hellorunoobhello\nrunoob 空行函数、类的方法之间有空行分隔。 *等待用户输入执行下面的程序在按回车键后就会等待用户输入：123#!/usr/bin/python3input(&quot;\n\n按下 enter 键后退出。&quot;) 以上代码中 ，”\n\n”在结果输出前会输出两个新的空行。一旦用户按下 enter 键时，程序将退出。 同一行显示多条语句Python可以在同一行中使用多条语句，语句之间使用分号(;)分割，以下是一个简单的实例： 12345678#!/usr/bin/python3import sys; x = &apos;小二&apos;; sys.stdout.write(x + &apos;\n&apos;)执行以上代码，输出结果为：小二 多个语句构成代码组缩进相同的一组语句构成一个代码块，我们称之代码组。 像if、while、def和class这样的复合语句，首行以关键字开始，以冒号( : )结束，该行之后的一行或多行代码构成代码组。 我们将首行及后面的代码组称为一个子句(clause)。 如下实例：123456if expression : suiteelif expression : suite else : suite print输出print 默认输出是换行的，如果要实现不换行需要在变量末尾加上 end=””： 12345678910111213141516171819#!/usr/bin/python3x=&quot;a&quot;y=&quot;b&quot;# 换行输出print( x )print( y )print(&apos;---------&apos;)# 不换行输出print( x, end=&quot; &quot; )print( y, end=&quot; &quot; )print()以上实例执行结果为：ab---------a b import 与 from…import在 python 用 import 或者 from…import 来导入相应的模块。 将整个模块(somemodule)导入，格式为： import somemodule 从某个模块中导入某个函数,格式为： from somemodule import somefunction 从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc 将某个模块中的全部函数导入，格式为： from somemodule import * 123456789101112导入 sys 模块import sysprint(&apos;================Python import mode==========================&apos;);print (&apos;命令行参数为:&apos;)for i in sys.argv: print (i)print (&apos;\n python 路径为&apos;,sys.path)导入 sys 模块的 argv,path 成员from sys import argv,path # 导入特定的成员 print(&apos;================python from import===================================&apos;)print(&apos;path:&apos;,path) # 因为已经导入path成员，所以此处引用时不需要加sys.path 命令行参数很多程序可以执行一些操作来查看一些基本信息，Python可以使用-h参数查看各参数帮助信息：123456789$ python -husage: python [option] ... [-c cmd | -m mod | file | -] [arg] ...Options and arguments (and corresponding environment variables):-c cmd : program passed in as string (terminates option list)-d : debug output from parser (also PYTHONDEBUG=x)-E : ignore environment variables (such as PYTHONPATH)-h : print this help message and exit[ etc. ]]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2018%2F05%2F14%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[** 使用Linux时，会有大量的命令，而一般常用的命令并不是很多（当然Linux系统管理员除外）。1.cd命令这是一个非常基本，也是大家经常需要使用的命令，它用于切换当前目录，它的参数是要切换到的目录的路径，可以是绝对路径，也可以是相对路径。如：123cd /root/Docements # 切换到目录/root/Docements cd ./path # 切换到当前目录下的path目录中，“.”表示当前目录 cd ../path # 切换到上层目录中的path目录中，“..”表示上一层目录 2、ls命令这是一个非常有用的查看文件与目录的命令，list之意，它的参数非常多，下面就列出一些我常用的参数吧，如下：123456789-l ：列出长数据串，包含文件的属性与权限数据等 -a ：列出全部的文件，连同隐藏文件（开头为.的文件）一起列出来（常用） -d ：仅列出目录本身，而不是列出目录的文件数据 -h ：将文件容量以较易读的方式（GB，kB等）列出来 -R ：连同子目录的内容一起列出（递归列出），等于该目录下的所有文件都会显示出来 注：这些参数也可以组合使用，下面举两个例子：ls -l #以长数据串的形式列出当前目录下的数据文件和目录 ls -lR #以长数据串的形式列出当前目录下的所有文件 3、grep命令该命令常用于分析一行的信息，若当中有我们所需要的信息，就将该行显示出来，该命令通常与管道命令一起使用，用于对一些命令的输出进行筛选加工等等，它的简单语法为123456789101112grep [-acinv] [--color=auto] &apos;查找字符串&apos; filename 它的常用参数如下：-a ：将binary文件以text文件的方式查找数据 -c ：计算找到‘查找字符串’的次数 -i ：忽略大小写的区别，即把大小写视为相同 -v ：反向选择，即显示出没有‘查找字符串’内容的那一行 # 例如： # 取出文件/etc/man.config中包含MANPATH的行，并把找到的关键字加上颜色 grep --color=auto &apos;MANPATH&apos; /etc/man.config # 把ls -l的输出中包含字母file（不区分大小写）的内容输出 ls -l | grep -i file 4、find命令find是一个基于查找的功能非常强大的命令，相对而言，它的使用也相对较为复杂，参数也比较多，所以在这里将给把它们分类列出，它的基本语法如下：123456789find [PATH] [option] [action] ** 与时间有关的参数： -mtime n : n为数字，意思为在n天之前的“一天内”被更改过的文件； -mtime +n : 列出在n天之前（不含n天本身）被更改过的文件名； -mtime -n : 列出在n天之内（含n天本身）被更改过的文件名； -newer file : 列出比file还要新的文件名 # 例如： find /root -mtime 0 # 在当前目录下查找今天之内有改动的文件 1234567与用户或用户组名有关的参数： -user name : 列出文件所有者为name的文件 -group name : 列出文件所属用户组为name的文件 -uid n : 列出文件所有者为用户ID为n的文件 -gid n : 列出文件所属用户组为用户组ID为n的文件 # 例如： find /home/ljianhui -user ljianhui # 在目录/home/ljianhui中找出所有者为ljianhui的文件 123456789101112# 与文件权限及名称有关的参数： -name filename ：找出文件名为filename的文件 -size [+-]SIZE ：找出比SIZE还要大（+）或小（-）的文件 -tpye TYPE ：查找文件的类型为TYPE的文件，TYPE的值主要有：一般文件（f)、设备文件（b、c）、 目录（d）、连接文件（l）、socket（s）、FIFO管道文件（p）； -perm mode ：查找文件权限刚好等于mode的文件，mode用数字表示，如0755； -perm -mode ：查找文件权限必须要全部包括mode权限的文件，mode用数字表示 -perm +mode ：查找文件权限包含任一mode的权限的文件，mode用数字表示 # 例如： find / -name passwd # 查找文件名为passwd的文件 find . -perm 0755 # 查找当前目录中文件权限的0755的文件 find . -size +12k # 查找当前目录中大于12KB的文件，注意c表示byte 5、cp命令该命令用于复制文件，copy之意，它还可以把多个文件一次性地复制到一个目录下，它的常用参数如下：123456789-a ：将文件的特性一起复制 -p ：连同文件的属性一起复制，而非使用默认方式，与-a相似，常用于备份 -i ：若目标文件已经存在时，在覆盖时会先询问操作的进行 -r ：递归持续复制，用于目录的复制行为 -u ：目标文件与源文件有差异时才会复制 例如 ：cp -a file1 file2 #连同文件的所有特性把文件file1复制成文件file2 cp file1 file2 file3 dir #把文件file1、file2、file3复制到目录dir中 6、mv命令该命令用于移动文件、目录或更名，move之意，它的常用参数如下：123456789-f ：force强制的意思，如果目标文件已经存在，不会询问而直接覆盖 -i ：若目标文件已经存在，就会询问是否覆盖 -u ：若目标文件已经存在，且比目标文件新，才会更新 注：该命令可以把一个文件或多个文件一次移动一个文件夹中，但是最后一个目标文件一定要是“目录”。例如：[plain] view plain copymv file1 file2 file3 dir # 把文件file1、file2、file3移动到目录dir中 mv file1 file2 # 把文件file1重命名为file2 7、rm命令该命令用于删除文件或目录，remove之间，它的常用参数如下：12345678910-f ：就是force的意思，忽略不存在的文件，不会出现警告消息 -i ：互动模式，在删除前会询问用户是否操作 -r ：递归删除，最常用于目录删除，它是一个非常危险的参数 例如：[plain] view plain copyrm -i file # 删除文件file，在删除之前会询问是否进行该操作 rm -fr dir # 强制删除目录dir中的所有文件 ``` 8、ps命令该命令用于将某个时间点的进程运行情况选取下来并输出，process之意，它的常用参数如下： -A ：所有的进程均显示出来-a ：不与terminal有关的所有进程-u ：有效用户的相关进程-x ：一般与a参数一起使用，可列出较完整的信息-l ：较长，较详细地将PID的信息列出其实我们只要记住ps一般使用的命令参数搭配即可，它们并不多，如下：[plain] view plain copyps aux # 查看系统所有的进程数据ps ax # 查看不与terminal有关的所有进程ps -lA # 查看系统所有的进程数据ps axjf # 查看连同一部分进程树状态129、kill命令该命令用于向某个工作（%jobnumber）或者是某个PID（数字）传送一个信号，它通常与ps和jobs命令一起使用，它的基本语法如下： kill -signal PIDsignal的常用参数如下：注：最前面的数字为信号的代号，使用时可以用代号代替相应的信号。[plain] view plain copy1：SIGHUP，启动被终止的进程2：SIGINT，相当于输入ctrl+c，中断一个程序的进行9：SIGKILL，强制中断一个进程的进行15：SIGTERM，以正常的结束进程方式来终止进程17：SIGSTOP，相当于输入ctrl+z，暂停一个进程的进行例如：[plain] view plain copy 以正常的结束进程方式来终于第一个后台工作，可用jobs命令查看后台中的第一个工作进程kill -SIGTERM %1 重新改动进程ID为PID的进程，PID可用ps命令通过管道命令加上grep命令进行筛选获得kill -SIGHUP PID1210、killall命令该命令用于向一个命令启动的进程发送一个信号，它的一般语法如下： killall [-iIe] [command name]它的参数如下：[plain] view plain copy-i ：交互式的意思，若需要删除时，会询问用户-e ：表示后面接的command name要一致，但command name不能超过15个字符-I ：命令名称忽略大小写 例如：killall -SIGHUP syslogd # 重新启动syslogd11、file命令该命令用于判断接在file命令后的文件的基本数据，因为在Linux下文件的类型并不是以后缀为分的，所以这个命令对我们来说就很有用了，它的用法非常简单，基本语法如下：[plain] view plain copyfile filename #例如：file ./test1212、tar命令该命令用于对文件进行打包，默认情况并不会压缩，如果指定了相应的参数，它还会调用相应的压缩程序（如gzip和bzip等）进行压缩和解压。它的常用参数如下： -c ：新建打包文件-t ：查看打包文件的内容含有哪些文件名-x ：解打包或解压缩的功能，可以搭配-C（大写）指定解压的目录，注意-c,-t,-x不能同时出现在同一条命令中-j ：通过bzip2的支持进行压缩/解压缩-z ：通过gzip的支持进行压缩/解压缩-v ：在压缩/解压缩过程中，将正在处理的文件名显示出来-f filename ：filename为要处理的文件-C dir ：指定压缩/解压缩的目录dir上面的解说可以已经让你晕过去了，但是通常我们只需要记住下面三条命令即可：[plain] view plain copy压缩：tar -jcv -f filename.tar.bz2 要被处理的文件或目录名称查询：tar -jtv -f filename.tar.bz2解压：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录注：文件名并不定要以后缀tar.bz2结尾，这里主要是为了说明使用的压缩程序为bzip21213、cat命令该命令用于查看文本文件的内容，后接要查看的文件名，通常可用管道与more和less一起使用，从而可以一页页地查看数据。例如： cat text | less # 查看text文件中的内容 注：这条命令也可以使用less text来代替1214、chgrp命令该命令用于改变文件所属用户组，它的使用非常简单，它的基本用法如下： chgrp [-R] dirname/filename-R ：进行递归的持续对所有文件和子目录更改 例如：chgrp users -R ./dir # 递归地把dir目录下中的所有文件和子目录下所有文件的用户组修改为users1234515、chown命令该命令用于改变文件的所有者，与chgrp命令的使用方法相同，只是修改的文件属性不同，不再详述。16、chmod命令该命令用于改变文件的权限，一般的用法如下： chmod [-R] xyz 文件或目录-R：进行递归的持续更改，即连同子目录下的所有文件都会更改同时，chmod还可以使用u（user）、g（group）、o（other）、a（all）和+（加入）、-（删除）、=（设置）跟rwx搭配来对文件的权限进行更改。 [plain] view plain copy 例如：chmod 0755 file # 把file的文件权限改变为-rxwr-xr-xchmod g+w file # 向file的文件权限中加入用户组可写权限1234518、vim命令该命令主要用于文本编辑，它接一个或多个文件名作为参数，如果文件存在就打开，如果文件不存在就以该文件名创建一个文件。vim是一个非常好用的文本编辑器，它里面有很多非常好用的命令，在这里不再多说。你可以从这里下载vim常用操作的详细说明。19、gcc命令对于一个用Linux开发C程序的人来说，这个命令就非常重要了，它用于把C语言的源程序文件，编译成可执行程序，由于g++的很多参数跟它非常相似，所以这里只介绍gcc的参数，它的常用参数如下： -o ：output之意，用于指定生成一个可执行文件的文件名-c ：用于把源文件生成目标文件（.o)，并阻止编译器创建一个完整的程序-I ：增加编译时搜索头文件的路径-L ：增加编译时搜索静态连接库的路径-S ：把源文件生成汇编代码文件-lm：表示标准库的目录中名为libm.a的函数库-lpthread ：连接NPTL实现的线程库-std= ：用于指定把使用的C语言的版本 例如：把源文件test.c按照c99标准编译成可执行程序testgcc -o test test.c -lm -std=c99 #把源文件test.c转换为相应的汇编程序源文件test.sgcc -S test.c1220、time命令该命令用于测算一个命令（即程序）的执行时间。它的使用非常简单，就像平时输入命令一样，不过在命令的前面加入一个time即可，例如： time ./processtime ps aux在程序或命令运行结束后，在最后输出了三个时间，它们分别是：user：用户CPU时间，命令执行完成花费的用户CPU时间，即命令在用户态中执行时间总和；system：系统CPU时间，命令执行完成花费的系统CPU时间，即命令在核心态中执行时间总和；real：实际时间，从command命令行开始执行到运行终止的消逝时间；```注：用户CPU时间和系统CPU时间之和为CPU时间，即命令占用CPU执行的时间总和。实际时间要大于CPU时间，因为Linux是多任务操作系统，往往在执行一条命令时，系统还要处理其它任务。另一个需要注意的问题是即使每次执行相同命令，但所花费的时间也是不一样，其花费时间是与系统运行相关的。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx安装]]></title>
    <url>%2F2018%2F05%2F11%2Fnginx%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[** Nginx(“engine x”)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。 ** 在高连接并发的情况下，Nginx是Apache服务器不错的替代品。 Nginx 安装系统平台：CentOS release 6.6 (Final) 64位。 一、安装编译工具及库文件yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel 二、首先要安装 PCREPCRE 作用是让 Nginx 支持 Rewrite 功能。 1、下载 PCRE 安装包，下载地址： http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz [root@bogon src]# wget http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz 2、解压安装包: [root@bogon src]# tar zxvf pcre-8.35.tar.gz3、进入安装包目录 [root@bogon src]# cd pcre-8.354、编译安装 [root@bogon pcre-8.35]# ./configure[root@bogon pcre-8.35]# make &amp;&amp; make install5、查看pcre版本 [root@bogon pcre-8.35]# pcre-config –version 安装 Nginx1、下载 Nginx，下载地址：http://nginx.org/download/nginx-1.6.2.tar.gz [root@bogon src]# wget http://nginx.org/download/nginx-1.6.2.tar.gz2、解压安装包 [root@bogon src]# tar zxvf nginx-1.6.2.tar.gz3、进入安装包目录 [root@bogon src]# cd nginx-1.6.24、编译安装 [root@bogon nginx-1.6.2]# ./configure –prefix=/usr/local/webserver/nginx –with-http_stub_status_module –with-http_ssl_module –with-pcre=/usr/local/src/pcre-8.35[root@bogon nginx-1.6.2]# make[root@bogon nginx-1.6.2]# make install5、查看nginx版本 [root@bogon nginx-1.6.2]# /usr/local/webserver/nginx/sbin/nginx -v ** 到此，nginx安装完成。 Nginx 配置创建 Nginx 运行使用的用户 www： [root@bogon conf]# /usr/sbin/groupadd www[root@bogon conf]# /usr/sbin/useradd -g www www配置nginx.conf ，将/usr/local/webserver/nginx/conf/nginx.conf替换为以下内容 [root@bogon conf]# cat /usr/local/webserver/nginx/conf/nginx.conf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374user www www;worker_processes 2; #设置值和CPU核心数一致error_log /usr/local/webserver/nginx/logs/nginx_error.log crit; #日志位置和日志级别pid /usr/local/webserver/nginx/nginx.pid;#Specifies the value for maximum file descriptors that can be opened by this process.worker_rlimit_nofile 65535;events&#123; use epoll; worker_connections 65535;&#125;http&#123; include mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; $http_x_forwarded_for&apos;; #charset gb2312; server_names_hash_bucket_size 128; client_header_buffer_size 32k; large_client_header_buffers 4 32k; client_max_body_size 8m; sendfile on; tcp_nopush on; keepalive_timeout 60; tcp_nodelay on; fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 2; gzip_types text/plain application/x-javascript text/css application/xml; gzip_vary on; #limit_zone crawler $binary_remote_addr 10m; #下面是server虚拟主机的配置 server &#123; listen 80;#监听端口 server_name localhost;#域名 index index.html index.htm index.php; root /usr/local/webserver/nginx/html;#站点目录 location ~ .*\.(php|php5)?$ &#123; #fastcgi_pass unix:/tmp/php-cgi.sock; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; &#125; location ~ .*\.(gif|jpg|jpeg|png|bmp|swf|ico)$ &#123; expires 30d;#access_log off; &#125; location ~ .*\.(js|css)?$ &#123; expires 15d;#access_log off; &#125; access_log off; &#125;&#125; 检查配置文件ngnix.conf的正确性命令： [root@bogon conf]# /usr/local/webserver/nginx/sbin/nginx -t 启动 NginxNginx 启动命令如下： [root@bogon conf]# /usr/local/webserver/nginx/sbin/nginx 访问站点从浏览器访问我们配置的站点ip： ** Nginx 其他命令以下包含了 Nginx 常用的几个命令： /usr/local/webserver/nginx/sbin/nginx -s reload # 重新载入配置文件/usr/local/webserver/nginx/sbin/nginx -s reopen # 重启 Nginx/usr/local/webserver/nginx/sbin/nginx -s stop # 停止 Nginx]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka介绍]]></title>
    <url>%2F2018%2F05%2F11%2Fkafka%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[生产者发送消息到broker的流程 1.ProducerIntercptor对消息进行拦截 2.Serialzer对key和values进行序列化 3.Partitioner对消息选择合适分区 4.RecordAccumulator收集消息，实现批量发送 5.Sender从RecordAccumulator获取消息 6.构造ClientRequest 7.将ClientRequest交给Network,准备发送 8.Network将请求放在KafkaChannel的缓存 9.发送请求 10.收到响应，发送ClientRequest 11.调用RecordBath的回调函数，最终调用到每一个消息上的回调函数 在这里主要涉及到两个线程：主线程主要负责封装消息成ProducerRecord对象，之后调用send方法将消息放入RecordAccumulator中暂存Sender线程负责将消息构造成请求，并从RecordAccumulator取出消息消息并批量发送 核心字段String clientId：该生产者的唯一标示 AtomicInteger PRODUCER_CLIENT_ID_SEQUENCE: clientId生成器 Partitioner: 分区选择器，根据一定策略将消息路由到合适的分区 int maxRequestSize: 消息的最大长度 long totalMemorySize: 发送单个消息的缓冲区的大小 Metadata: 整个kafka集群的元数据 RecordAccumulator accumulator: 用于收集并缓存消息，等待sender线程获取 Sender:发送消息的sender任务 Thread ioThread: 执行sender任务发送消息的线程 CompressionType: 压缩算法，针对RecordAccumulator中多条消息进行的压缩，消息越多效果越好 Serializer keySerializer: key的序列化器 Serializer valueSerializer: value的序列化器 long maxBlockTimeMs: 等待更新kafka集群元数据的最大时长 int requestTimeoutMs: 消息超时时长 ProducerInterceptors interceptors: 拦截record，可以对record进行进一步处理再发送到服务器 重要的方法 调用ProducerInterceptors的onSend方法，对消息进行拦截 调用doSend方法，然后就调用waitOnMetadata方法获取kafka集群元数据信息，底层会唤醒Sender线程更新Metadata保存的kafka元数据 调用Serializer的serialize方法对消息的key和value进行序列化 调用partition方法为消息选择合适的分区 调用RecordAccumulator的append方法将消息追加到RecordAccumulator中 唤醒Sender线程，由Sender线程将RecordAccumulator中缓存的消息发送出去123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; // 在发送消息之前，对消息进行拦截，然后可以对消息进行修改 ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors == null ? record : this.interceptors.onSend(record); return doSend(interceptedRecord, callback);&#125; /** 异步发送record 到topic */private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; TopicPartition tp = null; try &#123; // 首先获取集群信息和加载元数据的时间 ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs); // 计算剩余的等待时间，还可以用于等待添加数据到队列（总的等待时间-获取元数据信息的时间） long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs); Cluster cluster = clusterAndWaitTime.cluster; // 序列化key &amp; value byte[] serializedKey; try &#123; serializedKey = keySerializer.serialize(record.topic(), record.key()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&apos;t convert key of class &quot; + record.key().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in key.serializer&quot;); &#125; byte[] serializedValue; try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.value()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&apos;t convert value of class &quot; + record.value().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in value.serializer&quot;); &#125; // 计算record的分区，默认是0 int partition = partition(record, serializedKey, serializedValue, cluster); // record的总长度 // size(4)+offset(8)+crc(4)+ magic(1)+attribute(1)+timestamp(8)+key(4)+keysize+value(4)+valuesize int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue); // 但是record的总长度不能大于maxRequestSize和totalMemorySize ensureValidRecordSize(serializedSize); tp = new TopicPartition(record.topic(), partition); long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp(); log.trace(&quot;Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;&quot;, record, callback, record.topic(), partition); Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp); // 将record放入RecordAccumulator队列（相当于消息缓冲区），供Sender线程去读取数据，然后发给broker RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs); // 如果满了或者是新创建的，必须满上唤醒sender线程 if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(&quot;Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch&quot;, record.topic(), partition); this.sender.wakeup(); &#125; // 返回结果 return result.future; // handling exceptions and record the errors; // for API exceptions return them in the future, // for other exceptions throw directly &#125; catch (ApiException e) &#123; log.debug(&quot;Exception occurred during message send:&quot;, e); if (callback != null) callback.onCompletion(null, e); this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); return new FutureFailure(e); &#125; catch (InterruptedException e) &#123; this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw new InterruptException(e); &#125; catch (BufferExhaustedException e) &#123; this.errors.record(); this.metrics.sensor(&quot;buffer-exhausted-records&quot;).record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (KafkaException e) &#123; this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (Exception e) &#123; // we notify interceptor about all exceptions, since onSend is called before anything else in this method if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125;&#125; private ClusterAndWaitTime waitOnMetadata(String topic, Integer partition, long maxWaitMs) throws InterruptedException &#123; // add topic to metadata topic list if it is not there already and reset expiry // 如果元数据不存在这个topic，则添加到元数据的topic集合中 metadata.add(topic); // 根据元数据获取集群信息 Cluster cluster = metadata.fetch(); // 获取指定topic的partition数量 Integer partitionsCount = cluster.partitionCountForTopic(topic); // Return cached metadata if we have it, and if the record&apos;s partition is either undefined // or within the known partition range // 如果partition数量不为空，直接返回 if (partitionsCount != null &amp;&amp; (partition == null || partition &lt; partitionsCount)) return new ClusterAndWaitTime(cluster, 0); long begin = time.milliseconds(); // 最大的等待时间 long remainingWaitMs = maxWaitMs; long elapsed; // Issue metadata requests until we have metadata for the topic or maxWaitTimeMs is exceeded. // In case we already have cached metadata for the topic, but the requested partition is greater // than expected, issue an update request only once. This is necessary in case the metadata // is stale and the number of partitions for this topic has increased in the meantime. do &#123; log.trace(&quot;Requesting metadata update for topic &#123;&#125;.&quot;, topic); // 请求更新当前的集群元数据信息，在更新之前返回当前版本 int version = metadata.requestUpdate(); // 唤醒sender线程 sender.wakeup(); try &#123; // 等待元数据更新，直到当前版本大于我们所知道的最新版本 metadata.awaitUpdate(version, remainingWaitMs); &#125; catch (TimeoutException ex) &#123; // Rethrow with original maxWaitMs to prevent logging exception with remainingWaitMs throw new TimeoutException(&quot;Failed to update metadata after &quot; + maxWaitMs + &quot; ms.&quot;); &#125; // metadata更新完了在获取一次集群信息 cluster = metadata.fetch(); elapsed = time.milliseconds() - begin; // 如果时间超过最大等待时间，抛出更新元数据失败异常 if (elapsed &gt;= maxWaitMs) throw new TimeoutException(&quot;Failed to update metadata after &quot; + maxWaitMs + &quot; ms.&quot;); // 如果集群未授权topics包含这个topic，也会抛出异常 if (cluster.unauthorizedTopics().contains(topic)) throw new TopicAuthorizationException(topic); remainingWaitMs = maxWaitMs - elapsed; // 在此获取该topic的partition数量 partitionsCount = cluster.partitionCountForTopic(topic); &#125; while (partitionsCount == null);// 直到topic的partition数量不为空 if (partition != null &amp;&amp; partition &gt;= partitionsCount) &#123; throw new KafkaException( String.format(&quot;Invalid partition given with record: %d is not in the range [0...%d).&quot;, partition, partitionsCount)); &#125; // 返回ClusterAndWaitTime return new ClusterAndWaitTime(cluster, elapsed);&#125;]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH安装]]></title>
    <url>%2F2018%2F05%2F04%2FCDH%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[一、准备 1.官网下载Cloudera Manager和CDH 5.11Cloudera Manager下载地址：1http://archive.cloudera.com/cm5/cm/5/ cloudera-manager-el6-cm5.11.0_x86_64.tar.gz CDH安装包1http://archive.cloudera.com/cdh5/parcels/latest/ CDH-5.11.0-1.cdh5.11.0.p0.34-el6.parcel CDH-5.11.0-1.cdh5.11.0.p0.34-el6.parcel.sha1 manifest.json 2.JDK 7jdk-7u80-linux-x64.tar.gz 3.mysql 5.6和最新的mysql JDBC connectormysql-5.6.36-linux-glibc2.5-x86_64.tar.gz mysql-connector-java-5.1.44-bin.jar 二、CentOS6.5安装用户/密码 root/XXXX XXX/123 主机名 IP master 192.168.10.111 slave1 192.168.10.121 slave2 192.168.10.131 配置/etc/hostname 分别是maser和slave1和slave2 主机名 IP123456master 192.168.10.11 slave1 192.168.10.12slave2 192.168.10.13 配置/etc/hostname 分别是maser和slave1和slave2 3台都配置/etc/hosts 加入:12345192.168.111 master192.168.121 slave1192.168.131 slave2 注：安装过程都使用root用户 CDH 安装在/opt。 CDH在var目录会放文件，要预留空间 数据保存在/data 三、配置SSH无密码登录1.查看是否安装openssh和rsync123rpm -qa | grep opensshrpm -qa | grep rsync 三台都运行1ssh-keygen -t dsa -P &apos;&apos; -f ~/.ssh/id_dsa (-t dsa：表示使用密钥的加密类型，可以为’rsa’和’dsa’) 3.1cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys 这一步，把本机的密钥写入authorized_keys文件 然后要将其他2台的密钥写入本机的authorized_keys文件 4. master运行123scp ~/.ssh/id_dsa.pub root@slave1:~/.ssh/ authorized_keys_from_masterscp ~/.ssh/id_dsa.pub root@slave2:~/.ssh/ authorized_keys_from_master slave1 运行1cat .ssh/authorized_keys_from_master &gt;&gt; ~/.ssh/authorized_keys slave2 运行1cat .ssh/authorized_keys_from_master &gt;&gt; ~/.ssh/authorized_keys slave1运行123scp ~/.ssh/id_dsa.pub root@master:~/.ssh/authorized_keys_from_slave1scp ~/.ssh/id_dsa.pub root@slave2:~/.ssh/authorized_keys_from_slave1 master运行1cat .ssh/authorized_keys_from_slave1 &gt;&gt; ~/.ssh/authorized_keys slave2 运行1cat .ssh/authorized_keys_from_slave1 &gt;&gt; ~/.ssh/authorized_keys slave2运行123scp ~/.ssh/id_dsa.pub root@master:~/.ssh/authorized_keys_from_slave2scp ~/.ssh/id_dsa.pub root@slave1:~/.ssh/authorized_keys_from_slave2 master运行1cat .ssh/authorized_keys_from_slave2 &gt;&gt; ~/.ssh/authorized_keys slave2 运行1cat .ssh/authorized_keys_from_slave2 &gt;&gt; ~/.ssh/authorized_keys 最后3台的authorized_keys文件，内容都有3个key 测试SSH无密码登陆 在任意一台上，ssh slave1 都可以无密码登陆 四、同步各时间节点 1 将3台服务器时间同步，若hbase各节点时间差距过大会报错（默认30秒） 关闭3台的防火墙123service iptables stopchkconfig iptables off 重启后也生效 2 配置ntp服务器（master） vim /etc/ntp.conf 然后加入以下配置:123127.127.1.0fudge 127.127.1.0 stratum 10 注： 后面那个数字在0-15之间都可以，这样就将这台机器的本地时间作为ntp服务提供给客户端 3 重启ntpd /etc/init.d/ntpd start chkconfig ntpd on下次开机时 自动重启 五、安装JDK 1 检查并写在openjdk 检查命令：java -version 或 rpm -qa | grep java 卸载命令：rpm –e –nodeps(忽略依赖) 安装包名 或yum -y remove 安装包名123rpm -e --nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64 2 解压安装 Jdk版本：jdk-7u80-linux-x64.tar.gz 解压到/usr/local : cd /usr/local; tar –zxvf jdk-7u80-linux-x64.tar.gz 3 配置环境变量 3台都编辑/etc/profile文件：vim /etc/profile 加入 #set java environment12345export JAVA_HOME=/usr/local/jdk1.7.0_80export LASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin 使配置生效：source /etc/profile 六、安装mysqlmysql安装在slave2 192.168.10.131 1 检查是否存在mysql库文件 检查命令：rpm –qa | grep mysql 若存在则卸载：rpm -e –nodeps mysql-libs-5.1.71-1.el6.x86_64 2 检查是否存在mysql用户和组 检查命令：cat /etc/group | grep mysql; cat /etc/passwd | grep mysql 若不存在则创建：groupadd mysql; useradd –r –g mysql mysql (-r参数表示mysql用户是系统用户，不可用于登录系统) 3 创建数据、日志存储目录 命令：mkdir -p /data/mysql/data; mkdir /var/lib/mysql/log 更改所属用户： chown –R mysql:mysql /data/mysql chown –R mysql:mysql /usr/local/mysql 4 解压并初始化 解压安装包： /usr/local目录 tar –zxvf mysql-5.6.36-linux-glibc2.5-x86_64.tar.gz 改目录名 mv mysql-5.6.36-linux-glibc2.5-x86_64 mysql 初始化参数： 到mysql目录 bin/mysqld –initialize –user=mysql –basedir=/usr/local/mysql –datadir=/data/mysql/data 复制配置文件 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql cp /usr/local/mysql/support-files/my-default.cnf /etc/my.cnf 修改 vi /etc/init.d/mysql 123basedir=/usr/local/mysqldatadir=/data/mysql/data vi /etc/my.cnf12345basedir =/usr/local/mysqldatadir =/data/mysql/datacharacter_set_server=utf8 在usr/local/mysql/script目录运行 ./mysql_install_db –user=mysql –basedir=/usr/local/mysql –datadir=/data/mysql/data 5 添加环境变量 编辑文件：vim /etc/profile12export PATH=/usr/local/bin:$PATHexport PATH=/usr/local/mysql/bin:$PATH 6 配置mysql自动启动chmod 755 /etc/init.d/mysql chkconfig –add mysql 设置在运行级别为3和5时mysql自动启动：chkconfig –level 35 mysql on 7 启动mysql 命令：bin/mysqld_safe –user=mysql &amp; 登录mysql bin/mysql –user=root -p 设置root密码 set password=password(‘anjian123’); ()允许root在任何主机登录：grant all privileges on . to ‘root’@’%’ identified by ‘anjian123’ with grant option; grant all privileges on . to ‘root’@’master’ identified by ‘anjian123’ with grant option; 刷新权限表：flush privileges; 8 创建以下数据库 create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci; create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci; create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci; create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 七、安装CDH 1 安装Cloudera Manager Server 和Agent 主节点解压安装 cloudera manager的目录默认位置在/opt下，解压：tar xzvf cloudera-manager*.tar.gz将解压后的cm-5.11.0和cloudera目录放到/opt目录下。 复制mysql-connector-java-5.1.44-bin.jar到目录/usr/share/java和/opt/cm-5.11.0/share/cmf/lib/ 并改名成mysql-connector-java.jar 2 创建用户cloudera-scm 在所有节点上执行 useradd –system –no-create-home –shell=/bin/false –comment “Cloudera SCM User” cloudera-scm 3 agent配置 主节点，修改/opt/cm-5.11.0/etc/cloudera-scm-agent/config.ini中的server_host为主节点的主机名。123 # Hostname of the CM server.server_host=master 同步Agent到其他所有节点： scp -r /opt/cm-5.11.0 root@slave1:/opt/ scp -r /opt/cm-5.11.0 root@slave2:/opt/ 4 在主节点初始化CM5数据库 运行 /opt/cm-5.11.0/share/cmf/schema/scm_prepare_database.sh mysql cm -h slave2 -uroot -panjian123 –scm-host master scm scm scm -h后是mysql的主机名 –scm-host 后是SCM server的主机名（主节点） 如果有报错，一般是msyql的权限设置没设好，root不能远程登录 5 准备Parcels，用以安装CDH5 将CHD5相关的Parcel包放到主节点的/opt/cloudera/parcel-repo/目录中 相关的文件如下： CDH-5.11.0-1.cdh5.11.0.p0.34-el6.parcel CDH-5.11.0-1.cdh5.11.0.p0.34-el6.parcel.sha1 manifest.json 最后将CDH-5.1.3-1.cdh5.1.3.p0.12-el6.parcel.sha1，重命名为CDH-5.11.0-1.cdh5.11.0.p0.34-el6.parcel.sha 6 启动脚本 主节点：通过/opt/cm-5.11.0/etc/init.d/cloudera-scm-server start启动服务端。 所有节点（包括主节点）：通过/opt/cm-5.11.0/etc/init.d/cloudera-scm-agent start启动Agent服务 注： 停止可以用/init.d/cloudera-scm-server stop 启动成功后，可以用ps –ef|grep cloudera看到这2个进程 cloudera-scm-agent 的log如果报错: [04/Sep/2017 16:29:38 +0000] 2983 MainThread agent ERROR Heartbeating to master:7182 failed. 一般是主节点防火墙没关闭 7 CDH5的安装配置 http://192.168.10.111:7180/cmf/login 默认的用户名和密码均为admin 注：CDH安装失败后，重新安装：1) 主节点关闭cloudera-scm-server。所有节点关闭cloudera-scm-agent 2）删除Agent节点目录 rm -rf /opt/cm-5.11.0 rm -rf /opt/cloudera 删除主节点目录 rm -rf /opt/cm-5.11.0 rm -rf /opt/cloudera 删除主节点，/var/log/下cloudera相关的目录 删除主节点，/var/lib/下cloudera相关的目录 3) 清空CM数据库 进入Mysql数据库，然后drop database cm; 4) 删除各节点namenode和datanode节点信息 # rm -rf /opt/dfs/nn/* # rm -rf /opt/dfs/dn/* 5) 重新安装CDH，主节点解解压：tar xzvf cloudera-manager*.tar.gz将解压后的cm-5.11.0和cloudera目录放到/opt目录下。]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper搭建]]></title>
    <url>%2F2018%2F02%2F15%2Fzookeeper%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[本文选用：centos6.5、zookeeper-3.4.6集群部署【软件】准备好jdk环境，此次我们的环境是open_jdk1.8.0_101 zookeeper-3.4.6.tar.gz【步骤】 准备条件如果有内部dns或者外网有域名，则直接使用域名如果没有需要修改/etc/hosts文件，或者直接使用IP 集群规划 主机类型 IP地址 域名zookeeper1 192.168.1.1zookeeper1.chinasoft.comzookeeper2 192.168.1.2zookeeper2.chinasoft.comzookeeper3 192.168.1.3zookeeper3.chinasoft.com 注意：zookeeper因为有主节点和从节点的关系，所以部署的集群台数最好为奇数个，否则可能出现脑裂导致服务异常 安装下载地址：http://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/解压 tar -zxf zookeeper-3.4.6.tar.gzcd zookeeper-3.4.6 拷贝配置文件，修改完成后分发给其他节点cd /data/zookeeper-3.4.6/cp zoo_sample.cfg zoo.cfg cat zoo.cfg123456789tickTime=2000initLimit=10syncLimit=5dataDir=/data/zookeeper-3.4.6/datadataLogDir=/data/zookeeper-3.4.6/logsclientPort=2181server.1=u04rtv01.yaya.corp:2888:3888server.2=u04rtv02.yaya.corp:2888:3888server.3=u04rtv03.yaya.corp:2888:3888 3.创建data和Log文件夹mkdir /data/zookeeper-3.4.6/datamkdir /data/zookeeper-3.4.6/logs 4、在zoo.cfg中的dataDir指定的目录下，新建myid文件。例如：$ZK_INSTALL/data下，新建myid。在myid文件中输入1。表示为server.1。如果为snapshot/d_2，则myid文件中的内容为 2，依此类推。 启动：在集群中的每台主机上执行如下命令bin/zkServer.sh start 查看状态，可以看到其中一台为主节点，其他两台为从节点：bin/zkServer.sh status 主节点：./zkServer.sh status 123JMX enabled by defaultUsing config: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: leader 从属节点：./zkServer.sh status 123JMX enabled by defaultUsing config: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: follower 停止：bin/zkServer.sh stop 连接：123bin/zkCli.sh -server zookeeper1:2181 bin/zkCli.sh -server zookeeper2:2181 bin/zkCli.sh -server zookeeper3:2181 报错：原因就是没有在dataDir目录下创建myid文件并且赋值(如1、2、3分别代表集群中的server1,server2,server3) 2016-08-22 17:55:16,145 [myid:] - INFO [main:QuorumPeerConfig@103] - Reading configuration from: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfg2016-08-22 17:55:16,150 [myid:] - INFO [main:QuorumPeerConfig@340] - Defaulting to majority quorums2016-08-22 17:55:16,150 [myid:] - ERROR [main:QuorumPeerMain@85] - Invalid config, exiting abnormallyorg.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfg at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:123) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:101) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)Caused by: java.lang.IllegalArgumentException: /data/yunva/zookeeper-3.4.6/data/myid file is missing at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parseProperties(QuorumPeerConfig.java:350) at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:119) … 2 moreInvalid config, exiting abnormally 单机部署——适用于开发测试tar -zxvf zookeeper-3.4.6.tar.gzcd zookeeper-3.4.6/confcp zoo_sample.cfg zoo.cfg 创建日志目录 12mkdir /data/yunva/zookeeper-3.4.6/datamkdir /data/yunva/zookeeper-3.4.6/logs 配置：conf/zoo.cfg 123456tickTime=2000 initLimit=10 syncLimit=5 dataDir=/data/yunva/zookeeper-3.4.6/logsdataLogDir=/data/yunva/zookeeper-3.4.6/logsclientPort=2181 #自动清除日志文件12autopurge.snapRetainCount=20autopurge.purgeInterval=48 启动： 1bin/zkServer.sh start 连接到Zookeeper： 12bin/zkCli.sh -server 127.0.0.1:2181 适用于Java开发 查看状态： 1234bin/zkServer.sh statusJMX enabled by defaultUsing config: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: standalone]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeepere</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark伪分布安装]]></title>
    <url>%2F2018%2F02%2F15%2Fspark%E4%BC%AA%E5%88%86%E5%B8%83%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[本文选用：centos6.5、zookeeper-3.4.11、hadoop2.8.0、jdk1.8.0、Scala2.12.1 前言.Spark简介和hadoop的区别 123456789101112Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。 1.架构不同。 Hadoop是对大数据集进行分布式计算的标准工具。提供了包括工具和技巧在内的丰富的生态系统，允许使用相对便宜的商业硬件集群进行超级计算机级别的计算。 Spark使用函数式编程范式扩展了MapReduce编程模型以支持更多计算类型，可以涵盖广泛的工作流。且需要一个第三方的分布式存储系统作为依赖。 2.处理对象不同 Spark处理数据的方式不一样，会比MapReduce快上很多。MapReduce是分步对数据进行处理的: 从集群中读取数据，进行一次处理，将结果写到集群，从集群中读取更新后的数据，进行下一次的处理，将结果写到集群,反观Spark，它会在内存中以接近“实时”的时间完成所有的数据分析：从集群中读取数据，完成所有必须的分析处理，将结果写回集群.所以Hadoop适合处理静态数据，而Spark适合对流数据进行分析。 3.速度 Spark基于内存：Spark使用内存缓存来提升性能，因此进行交互式分析也足够快速(就如同使用Python解释器，与集群进行交互一样)。缓存同时提升了迭代算法的性能，这使得Spark非常适合数据理论任务，特别是机器学习。 Hadoop基于磁盘：MapReduce要求每隔步骤之间的数据要序列化到磁盘，这意味着MapReduce作业的I/O成本很高，导致交互分析和迭代算法（iterative algorithms）开销很大。而事实是，几乎所有的最优化和机器学习都是迭代的。 4.灾难恢复 Hadoop将每次处理后的数据都写入到磁盘上，所以其天生就能很有弹性的对系统错误进行处理。 Spark的数据对象存储在分布于数据集群中的叫做弹性分布式数据集(RDD: Resilient Distributed Dataset)中。这些数据对象既可以放在内存，也可以放在磁盘，所以RDD同样也可以提供完成的灾难恢复功能。 spark安装模式 1234567891011121314spark有以下几种安装模式，每种安装模式都有自己不同的优点和长处。 local(本地模式)： 常用于本地开发测试，本地还分为local单线程和local-cluster多线程; standalone(集群模式)： 典型的Mater/slave模式，Master可能有单点故障的；Spark支持ZooKeeper来实现 HA。 on yarn(集群模式)： 运行在 yarn 资源管理器框架之上，由 yarn 负责资源管理，Spark 负责任务调度和计算。 on mesos(集群模式)： 运行在 mesos 资源管理器框架之上，由 mesos 负责资源管理，Spark 负责任务调度和计算。 on cloud(集群模式)： 比如 AWS 的 EC2，使用这个模式能很方便的访问 Amazon的 S3;Spark 支持多种分布式存储系统：HDFS 和 S3。 目前Apache Spark支持三种分布式部署方式，分别是standalone、Spark on mesos和 spark on YARN，其中，第一种类似于MapReduce 1.0所采用的模式，内部实现了容错性和资源管理，后两种则是未来发展的趋势，部分容错性和资源管理交由统一的资源管理系统完成：让Spark运行在一个通用的资源管理系统之上，这样可以与其他计算框架，比如MapReduce，公用一个集群资源，最大的好处是降低运维成本和提高资源利用率（资源按需分配）。 【步骤】 1.安装scala 123456上传scala包，解压缩 配置环境变量SCALA_HOME source /etc/profile使得生效 验证scala安装情况 scala -version 及上scala 安装完成 伪分布式Spark安装部署下载地址：http://saprk.apache.org解压12tar -zxf saprk*******cd saprk*******/conf 拷贝配置文件,修改为运行主机名1cp slaves.template slaves 12345678910cp spark-env.sh.template spark-env.sh vim spark-env.sh --------------------进行以下配置 export JAVA_HOME=/usr/local/jdk1.8.0 export SCALA_HOME=/usr/local/scala-2.12.1 export SPARK_WORKER_MEMORY=1G export HADOOP_HOME=/usr/local/hadoop2.8.0export HADOOP_CONF_DIR=/usr/local/hadoop2.8.0/etc/hadoop export SPARK_MASTER_IP=192.168.174.175 3.启动spark(1)先启动hadoop 环境 1/usr/local/hadoop/sbin# start-all.sh （2）启动spark环境1/usr/local/spark-2.0.1/sbin# ./start-all.sh [注] 如果使用start-all.sh时候会重复启动hadoop配置，需要./在当前工作目录下执行脚本文件。jps 观察进程 多出 worker 和 mater 两个进程 4、查看spark的web控制页面 1234http://192.168.137.133:8080/ ``` * 5.进入交互 /bin/spark-shell12* 6.查看sparl-shell web界面 http://192.168.137.133:4040/jobs/12* 7.Spark测试 1.在HDFS上建立目录hadoop fs -mkdir -p /datahadoop fs -ls /data/ 可以查看hadoop fs -mkdir -p /data/input 建立文件夹input将本地目录传送到HDFS上hadoop fs -put /home/santiago/data/spark/spark_test.txt /data/input123456```测试spark val text_in = sc.textFile(&quot;/data/input/hbase.txt&quot;)执行后打印：println(text_in.count())观察job网页]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos下sqoop环境搭建]]></title>
    <url>%2F2018%2F02%2F15%2Fcentos%E4%B8%8Bsqoop%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Sqoop是一个用来将Hadoop（Hive、HBase）和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库（例如：MySQL ,Oracle ,Postgres等）中的数据导入到Hadoop的HDFS中，也可以将HDFS的数据导入到关系型数据库中。 Sqoop安装 1.下载Sqoop安装包在Sqoop官网下载安装包，本次使用的是sqoop-1.4.6.binhadoop-2.0.4-alpha.tar.gz安装在/usr/local目录下，下载地址为http://apache.fayea.com/sqoop/1.4.6/sqoop-1.4.6.binhadoop-2.0.4-alpha.tar.gz 2.解压Sqoop安装包#进入sqoop安装目录[hadoop@BigData ~]$ cd /usr/local#解压sqoop安装包[hadoop@BigData ~]$ tar -zxvf sqoop-1.4.6.binhadoop-2.0.4-alpha.tar.gz#删除sqoop安装包[hadoop@BigData ~]$ rm -rf sqoop-1.4.6.binhadoop-2.0.4-alpha.tar.gz#重命名sqoop目录名[hadoop@BigData ~]$ mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop 3.配置Sqoop环境变量#配置Sqoop环境变量[root@BigData ~]# vi /etc/profileexport SQOOP_HOME=/usr/local/sqoopexport PATH=$PATH:$SQOOP_HOME/bin#保存之后记得source，使之前的配置生效source /etc/profile 4.将关系型数据库驱动包放到sqoop/lib目录下MySql：mysql-connector-java-5.1.30.jarOracle：ojdbc14.jar 5.修改Sqoop配置文件[hadoop@BigData ~]$ mv sqoop-env-template.sh sqoop-env.sh[hadoop@BigData ~]$ vi sqoop-env.sh12345678910#Set path to where bin/hadoop is availableexport HADOOP_COMMON_HOME=/usr/local/hadoop#Set path to where hadoop-*-core.jar is availableexport HADOOP_MAPRED_HOME=/usr/local/hadoop#set the path to where bin/hbase is availableexport HBASE_HOME=/usr/local/hbase#Set the path to where bin/hive is availableexport HIVE_HOME=/usr/local/hive#Set the path for where zookeper config dir isexport ZOOCFGDIR=/usr/local/zookeeper 到此，sqoop环境就已搭建成功！]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker安装]]></title>
    <url>%2F2018%2F02%2F05%2Fdocker%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1：关闭selinux临时关闭：setenforce 0永久关闭： 1. vi /etc/sysconfig/selinux插入/编辑以下内容1SELINUX=disabled #重启生效 2：在Fedora EPEL源中已经提供了docker-io包，下载安装epel：12345678rpm -ivh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpmsed -i &apos;s/^mirrorlist=https/mirrorlist=http/&apos; /etc/yum.repos.d/epel.repo（elpe.repo）[epel]name=epelmirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=epel-$releasever&amp;arch=$basearchenabled=1gpgcheck=0 3：安装docker1yum install docker-io 安装完成后 4：启动docker1service docker start 5：查看docker版本1docker vesion 6：查看docker日志1cat /var/log/docker docker安装完成 一：卸载docker列出你安装过的包12[root@localhost ~]# yum list installed | grep dockerdocker-io.x86_64 1.7.1-2.el6 @epel 删除软件包1yum -y remove docker-io.x86_64 删除镜像/容器等1rm -rf /var/lib/docker 二：升级docker版本为1.10.3升级之前停止docker服务,并将原有的docker服务进行备份.1mv /usr/bin/docker /usr/bin/docker.bak 1nohup wget -c https://get.docker.com/builds/Linux/x86_64/docker-1.10.3 -O /usr/bin/docker 给执行权限：chmod 755 /usr/bin/docker 然后重启服务，并查看版本. 报错：1234Starting cgconfig service: Error: cannot mount memory to /cgroup/memory: No such file or directory/sbin/cgconfigparser; error loading /etc/cgconfig.conf: Cgroup mounting failedFailed to parse /etc/cgconfig.conf [FAILED]Starting docker: [ OK ] 修改：/etc/cgconfig.conf文件12345678910mount &#123; cpuset = /cgroup/cpuset; cpu = /cgroup/cpu; cpuacct = /cgroup/cpuacct;# memory = /cgroup/memory; devices = /cgroup/devices; freezer = /cgroup/freezer; net_cls = /cgroup/net_cls; blkio = /cgroup/blkio;&#125; 重新启动docker 正常]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka编程模型]]></title>
    <url>%2F2018%2F01%2F13%2Fkafka%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Class KafkaProduce所有已实现的接口：1java.io.Closeable，java.lang.AutoCloseable，Producer&lt;K,V&gt; 12345678910111213141516171819public class KafkaProducer&lt;K,V&gt; extends java.lang.Object implements Producer&lt;K,V&gt;&#123; Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//指定kafka服务器 props.put(&quot;acks&quot;, &quot;all&quot;);//发送消息方式，此处采用异步发送,在acks配置控制下，其请求被认为是完整的标准。我们指定的“all”设置将导致阻止完整提交记录，这是最慢但最耐用的设置。如果请求失败，则生产者可以自动重试 props.put(&quot;retries&quot;, 0);//副本个数，一般设置为3 props.put(&quot;batch.size&quot;, 16384);//生产者为每个分区维护未发送记录的缓冲区。这些缓冲区的大小由batch.sizeconfig 指定。 props.put(&quot;linger.ms&quot;, 1);//默认情况下，即使缓冲区中有其他未使用的空间，也可以立即发送缓冲区。但是，如果您希望减少可以设置linger.ms为大于0 的请求数。这将指示生产者在发送请求之前等待该毫秒数，希望更多记录到达以填充同一批次。这类似于TCP中的Nagle算法。 /*buffer.memory控制提供给生产者用于缓冲总量的存储器。如果记录的发送速度快于传输到服务器的速度，则此缓冲区空间将耗尽。 当缓冲区空间耗尽时，额外的发送调用将被阻止。阻塞时间的阈值由max.block.ms之后确定它抛出TimeoutException。*/ props.put(&quot;buffer.memory&quot;, 33554432); props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);//序列化 props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);//序列化 Producer&lt;String,String&gt; produce = new Producer(props); for(int i=0; i &lt; 100; i++)&#123; Producer.send(new ProducerRecord&lt;String,String&gt;(&quot;my-topic&quot;,Integer(i),Integer(i))); &#125; Producer.close();&#125; 下面的示例说明了如何使用新API。每个消息都可以是单个事务的一部分。12345678910111213141516171819202122Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); props.put(&quot;transactional.id&quot;, &quot;my-transactional-id&quot;); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props, new StringSerializer(), new StringSerializer()); producer.initTransactions(); try &#123; /*在beginTransaction()和commitTransaction()调用之间发送的所有消息 都将是单个事务的一部分。 当 transactional.id指定时，由生产者发送的所有消息必须是事务的一部分。*/ producer.beginTransaction(); for (int i = 0; i &lt; 100; i++) producer.send(new ProducerRecord&lt;&gt;(&quot;my-topic&quot;, Integer.toString(i), Integer.toString(i))); producer.commitTransaction(); &#125; catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) &#123; // We can&apos;t recover from these exceptions, so our only option is to close the producer and exit. producer.close(); &#125; catch (KafkaException e) &#123; // For all other exceptions, just abort the transaction and try again. producer.abortTransaction(); &#125; producer.close(); consumer 12345Package org.apache.kafka.clients.consumerClass KafkaConsumer&lt;K,V&gt;-----All Implemented Interfaces:java.io.Closeable, java.lang.AutoCloseable, Consumer&lt;K,V&gt; 示例 自动抵消提交 1234567891011121314Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); props.put(&quot;group.id&quot;, &quot;test&quot;); props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);//设置enable.auto.commit意味着使用由配置控制的频率自动提交偏移auto.commit.interval.ms。 props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(&quot;foo&quot;, &quot;bar&quot;));//通过其中一个subscribeAPI 动态设置要订阅的主题列表 。 while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value()); &#125; 手动偏移控制 123456789101112131415161718192021Properties props = new Properties（）; props.put（“bootstrap.servers”，“localhost：9092”）; props.put（“group.id”，“test”）; props.put（“enable.auto.commit”，“false”）; props.put（“key.deserializer”，“org.apache.kafka.common.serialization.StringDeserializer”）; props.put（“value.deserializer”，“org.apache.kafka.common.serialization.StringDeserializer”）; KafkaConsumer &lt;String，String&gt; consumer = new KafkaConsumer &lt;&gt;（props）; consumer.subscribe（Arrays.asList（“foo”，“bar”））; final int minBatchSize = 200; List &lt;ConsumerRecord &lt;String，String &gt;&gt; buffer = new ArrayList &lt;&gt; （）; while（true）&#123; ConsumerRecords &lt;String，String&gt; records = consumer.poll（100）; for（ConsumerRecord &lt;String，String&gt; record：records）&#123; buffer.add（record）; &#125; if（buffer.size（）&gt; = minBatchSize）&#123; insertIntoDb（buffer）; consumer.commitSync（）; //commitSync()是一种安全机制，可确保只有组中的活动成员才能提交偏移量。 buffer.clear（）; &#125; &#125; 下面的示例中，我们在完成处理每个分区中的记录后提交偏移量。12345678910111213141516try &#123; while（running）&#123; ConsumerRecords &lt;String，String&gt; records = consumer.poll（Long.MAX_VALUE）; for（TopicPartition partition：records.partitions（））&#123; List &lt;ConsumerRecord &lt;String，String &gt;&gt; partitionRecords = records.records（partition）; for（ConsumerRecord &lt;String，String&gt; record：partitionRecords）&#123; System.out.println（record.offset（）+“：”+ record.value（））; &#125; long lastOffset = partitionRecords.get（partitionRecords.size（） - 1）.offset（）; consumer.commitSync（Collections.singletonMap（partition，new OffsetAndMetadata（lastOffset + 1）））; &#125; &#125; &#125;finally&#123; consumer.close（）; &#125; /*注意：提交的偏移量应始终是应用程序将读取的下一条消息的偏移量。 因此，在调用时，commitSync(offsets)您应该在处理的最后一条消息的偏移量中添加一个。*/ 手动分区分配 使用此模式，subscribe您只需使用要使用assign(Collection)的分区的完整列表进行调用 ，而不是使用主题订阅。1234String topic =“foo”; TopicPartition partition0 = new TopicPartition（topic，0）; TopicPartition partition1 = new TopicPartition（topic，1）; consumer.assign（Arrays.asList（partition0，partition1））; 多线程处理12345678910111213141516171819202122232425public class KafkaConsumerRunner implements Runnable &#123; private final AtomicBoolean closed = new AtomicBoolean(false); private final KafkaConsumer consumer; public void run() &#123; try &#123; consumer.subscribe(Arrays.asList(&quot;topic&quot;)); while (!closed.get()) &#123; ConsumerRecords records = consumer.poll(10000); // Handle new records &#125; &#125; catch (WakeupException e) &#123; // Ignore exception if closing if (!closed.get()) throw e; &#125; finally &#123; consumer.close(); &#125; &#125; // Shutdown hook which can be called from a separate thread public void shutdown() &#123; closed.set(true); consumer.wakeup(); &#125; &#125; 然后在单独的线程中，可以通过设置关闭标志并唤醒消费者来关闭消费者。12closed.set（true）; consumer.wakeup（）;]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[致橡树]]></title>
    <url>%2F2018%2F01%2F13%2F%E8%87%B4%E6%A9%A1%E6%A0%91%2F</url>
    <content type="text"><![CDATA[文学的力量往往是在于对人信仰的改变 我如果爱你——绝不像攀援的凌霄花，借你的高枝炫耀自己；我如果爱你——绝不学痴情的鸟儿，为绿荫重复单调的歌曲；也不止像泉源，常年送来清凉的慰藉；也不止像险峰，增加你的高度，衬托你的威仪。甚至日光，甚至春雨。 不，这些都还不够！我必须是你近旁的一株木棉，作为树的形象和你站在一起。根，紧握在地下；叶，相触在云里。每一阵风过，我们都互相致意，但没有人，听懂我们的言语。你有你的铜枝铁干，像刀，像剑，也像戟；我有我红硕的花朵，像沉重的叹息，又像英勇的火炬。 我们分担寒潮、风雷、霹雳；我们共享雾霭、流岚、虹霓。仿佛永远分离，却又终身相依。这才是伟大的爱情，坚贞就在这里：爱——不仅爱你伟岸的身躯，也爱你坚持的位置，足下的土地。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Centos7伪分布式安装Hadoop2.6和Hbase0.94]]></title>
    <url>%2F2017%2F12%2F01%2Fcentos7%2Bhadoop2.6%2Bhbase1.0.x%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑一、安装Jdk：首先需要卸载系统自带的openjava，查看系统的Java： rpm -qa|grep java。 卸载： yum -y remove java javaxxxxx(系统自带的Java版本) 安装jdk，将jdk.tar.gz文件复制到/usr/java中,终端进入/mnt/share ,cp jdk.tar.gz /usr/ava，进入/usr/java解压：tar xzvf jdk.targz 配置环境变量：vi /etc/profile 输入i编辑在尾部添加：export JAVA_HOME=/usr/java/jdkxxxxexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 保存并退出： wq使修改生效： source /etc/profile查看Java版本：java -version 二、Hadoop伪分布式安装 1、ssh无密码登陆 终端：ssh-keygen -t rsa (获得rsa公钥私钥,id_rsa和id_rsa.pub)cd .sshcp id_rsa.pub authorized_keys (将公钥复制给authorized_keys) &lt;分布式则要将所有节点id_rsa.pub相互复制&gt; 2、 /mnt/share cp hadoop2.x /usr.hadoop 解压tar xzvf hadoop 2.x 3、修改core-site.xml、hadoop-env.sh、hdfs-site.xml、mapred-site.xml 、yarn-site.xml(hadoop2.x版本的配置文件在/hadoop2.x/etc/hadoop下)12345①core-site.xml： fs.default.name hdfs://localhost:9000 12② hadoop-env.sh：export JAVA_HOME=/usr/java/jdkxxx (jdk路径) 123456789101112131415③ hdfs-site.xml： 先创建好数据节点和名称节点的存放路径 dfs.datanode.data.dir /user/hadoop/hadoop-2.5.1/data dfs.namenode.name.dir /user/hadoop/hadoop-2.5.1/name dfs.replication 1 1234567④mapred-site.xml: (注意：这个文件是将/hadoop2.x/etc/hadoop下的mapred-site.xml.template复制并重命名 ) mapreduce.framework.name yarn 12345⑤yarn-site.xml： yarn.nodemanager.aux-services mapreduce_shuffle 4、namenode格式化（一定要完成） 终端：cd /usr/hadoop/hadoop-2.5.1/bin ./hdfs namenode -format (输入./hadoop namenode -format也行) 5、运行hadoop 终端： cd /usr/hadoop/hadoop-2.5.1/sbin (2.x版本后启动/停止在sbin目录下)./start-hdfs.sh./start-yarn.sh(也可以只输入./start-all.sh) 输入jps查看启动项，当启动了NameNode、DataNode、SecondaryNameNode、ResourceManager、NodeManager即ok。 可进入Firefox中，输入端口号： localhost:50070 进入hadoop可视化页面。 三、Hbase0.94安装 1、/mnt/share cp hbase1.0.1 /usr.hbase 解压tar xzvf hbase1.0.1 2、修改hbase配置文件hbase-env.sh、hbase-site.xml1234hbase-env.sh:export JAVA_HOME=/usr/java/jdkxxxx (java路径)export HBASE_MANAGES_ZK=true (都得去掉前面#) 12345678910111213141516171819202122hbase-site.xml： hbase.rootdir hdfs://localhost:9000/hbase hbase.cluster.distributed true hbase.zookeeper.quorum localhost hbase.tmp.dir file:/usr/hbase/tmp hbase.zookeeper.property.dataDir file:/usr/hbase/zookeeperdata 3、运行hbase 运行前需先启动hadoop，再进入hbase的bin目录下输入指令 ./start-hbase.sh输入jps查看启动项，如有HMaster、HRegionServer、HQuormPeer,则说明hbase启动成功。输入./hbase Shell (进入shell指令，可通过shell指令建表)]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos 6.5 内核升级]]></title>
    <url>%2F2017%2F11%2F15%2Fcentos6.5%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[1.查看centos的内核版本rname -r 2.查看系统版本cat /etc/centos-release 3.安装软件编译安装新内核，依赖于开发环境和开发库123456789yum grouplist //查看已经安装的和未安装的软件包组，来判断我们是否安装了相应的开发环境和开发库；yum groupinstall &quot;Development Tools&quot; //一般是安装这两个软件包组，这样做会确定你拥有编译时所需的一切工具yum install ncurses-devel //你必须这样才能让 make *config 这个指令正确地执行yum install qt-devel //如果你没有 X 环境，这一条可以不用yum install hmaccalc zlib-devel binutils-devel elfutils-libelf-devel //创建 CentOS-6 内核时需要它们 4.编译内核Linux内核版本有两种：稳定版和开发版 ，Linux内核版本号由3个数字组成： r.x.y r: 主版本号 x: 次版本号，偶数表示稳定版本；奇数表示开发中版本。 y: 修订版本号 ， 表示修改的次数官网上有stable, longterm等版本，longterm是比stable更稳定的版本。1234`[root@sean ~]#curl -O -L https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.10.28.tar.xz[root@sean ~]# tar -xf linux-3.10.58.tar.xz -C /usr/src/[root@sean ~]# cd /usr/src/linux-3.10.58/[root@sean linux-3.10.58]# cp /boot/config-2.6.32-220.el6.x86_64 .config` 我们在系统原有的内核配置文件的基础上建立新的编译选项，所以复制一份到当前目录下，命名为.config。接下来继续配置：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@sean linux-3.10.58]# sh -c &apos;yes &quot;&quot; | make oldconfig&apos; HOSTCC scripts/basic/fixdep HOSTCC scripts/kconfig/conf.o SHIPPED scripts/kconfig/zconf.tab.c SHIPPED scripts/kconfig/zconf.lex.c SHIPPED scripts/kconfig/zconf.hash.c HOSTCC scripts/kconfig/zconf.tab.o HOSTLD scripts/kconfig/confscripts/kconfig/conf --oldconfig Kconfig.config:555:warning: symbol value &apos;m&apos; invalid for PCCARD_NONSTATIC.config:2567:warning: symbol value &apos;m&apos; invalid for MFD_WM8400.config:2568:warning: symbol value &apos;m&apos; invalid for MFD_WM831X.config:2569:warning: symbol value &apos;m&apos; invalid for MFD_WM8350.config:2582:warning: symbol value &apos;m&apos; invalid for MFD_WM8350_I2C.config:2584:warning: symbol value &apos;m&apos; invalid for AB3100_CORE.config:3502:warning: symbol value &apos;m&apos; invalid for MMC_RICOH_MMC** Restart config...*** General setup*......XZ decompressor tester (XZ_DEC_TEST) [N/m/y/?] (NEW)Averaging functions (AVERAGE) [Y/?] (NEW) yCORDIC algorithm (CORDIC) [N/m/y/?] (NEW) JEDEC DDR data (DDR) [N/y/?] (NEW) #configuration written to .config make oldconfig会读取当前目录下的.config文件，在.config文件里没有找到的选项则提示用户填写，然后备份.config文件为.config.old，并生成新的.config文件 5.开始编译[root@sean linux-3.10.58]# make -j4 bzImage //生成内核文件[root@sean linux-3.10.58]# make -j4 modules //编译模块[root@sean linux-3.10.58]# make -j4 modules_install //编译安装模块 -j后面的数字是线程数，用于加快编译速度，一般的经验是，逻辑CPU，就填写那个数字，例如有8核，则为-j8。（modules部分耗时30多分钟） 6.安装[root@sean linux-3.10.58]# make install实际运行到这一步时，出现ERROR: modinfo: could not find module vmware_balloon，但是不影响内核安装，是由于vsphere需要的模块没有编译，要避免这个问题，需要在make之前时修改.config文件，加入HYPERVISOR_GUEST=yCONFIG_VMWARE_BALLOON=m（这一部分比较容易出问题，参考下文异常部分） 7.修改grub引导，重启安装完成后，需要修改Grub引导顺序，让新安装的内核作为默认内核。编辑 grub.conf文件，vi /etc/grub.conf #boot=/dev/sdadefault=0timeout=5splashimage=(hd0,0)/grub/splash.xpm.gzhiddenmenutitle CentOS (3.10.58) root (hd0,0)… 数一下刚刚新安装的内核在哪个位置，从0开始，然后设置default为那个数字，一般新安装的内核在第一个位置，所以设置default=0。重启reboot now 8.确认当内核版本[root@sean ~]# uname -r3.10.58 升级内核成功!]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>operation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos6.5下搭建hadoop2.7单机伪分布环境]]></title>
    <url>%2F2017%2F10%2F01%2Fcentos6.5%2Bhadoop2.7%2F</url>
    <content type="text"><![CDATA[设置固定IP地址及网关 设置IP 1vi /etc/sysconfig/network-scripts/ifcfg-eth0 修改内容如下 123456789DEVICE=eth0HWADDR=08:00:27:BD:9D:B5 #不用改TYPE=EthernetUUID=53e4e4b6-9724-43ab-9da7-68792e611031 #不用改ONBOOT=yes #开机启动NM_CONTROLLED=yesBOOTPROTO=static #静态IPIPADDR=192.168.30.50 #IP地址NETMASK=255.255.255.0 #子网掩码 设置网关 1vi /etc/sysconfig/network 添加内容 12HOSTNAME=Hadoop.MasterGATEWAY=192.168.30.1 #网关 设置DNSvi /etc/resolv.conf 添加内容nameserver xxx.xxx.xxx.xxx #根据实际情况设置nameserver 114.114.114.114 #可以设置多个 重启网卡 1service network restart 设置主机名对应IP地址 1vi /etc/hosts #添加如下内容1192.168.30.50 Hadoop.Master 添加Hadoop用户 添加用户组 1groupadd hadoop 添加用户并分配用户组 1useradd -g hadoop hadoop 修改用户密码 1passwd hadoop 关闭服务 关闭防火墙 1234service iptables stop #关闭防火墙服务chkconfig iptables off #关闭防火墙开机启动service ip6tables stopchkconfig ip6tables off 关闭SELinux 1vi /etc/sysconfig/selinux #修改如下内容1SELINUX=enforcing -&gt; SELINUX=disabled #再执行如下命令12setenforce 0getenforce 关闭其他服务VSFTP安装与配置 检查是否安装 1chkconfig --list|grep vsftpd 安装vsftp 1yum -y install vsftpd 创建日志文件 1touch /var/log/vdftpd.log 配置vsftpd服务 1vi /etc/vsftpd/vsftpd.conf #修改如下内容123456anonymous_enable=NO #关闭匿名访问xferlog_file=/var/log/vsftpd.log #设置日志文件 -- 我们上一步所创建的文件idle_session_timeout=600 #会话超时时间async_abor_enable=YES #开启异步传输ascii_upload_enable=YES #开启ASCII上传ascii_download_enable=YES #开启ASCII下载 查看vsftp运行状态1service vsftpd status 启动vsftp123service vsftpd start#重启 service vsftpd restart#关闭 service vsftpd stop 查看vsftpd服务启动项 1chkconfig --list|grep vsftpd 设置vsftp开机启动 1chkconfig vsftpd on SSH无密码配置 查看ssh与rsync安装状态 12rpm -qa|grep opensshrpm -qa|grep rsync 安装ssh与rsync 12yum -y install sshyum -y install rsync 切换hadoop用户 1su - hadoop 生成ssh密码对 1ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa 将id_dsa.pub追加到授权的key中 1cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 设置授权key权限1chmod 600 ~/.ssh/authorized_keys #权限的设置非常重要，因为不安全的设置安全设置，会让你不能使用RSA功能 测试ssh连接1ssh localhost #如果不需要输入密码，则是成功 安装Java 下载地址1http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html 注：我这里使用的是：jdk-7u80-linux-i586.tar.gz 安装Java切换至root用户 1su root 创建/usr/java文件夹 1mkdir /usr/java 使用winscp工具上传至服务器 将压缩包上传至/home/hadoop目录 注：我这里使用的是winscp，使用hadoop用户连接 将压缩包解压至/usr/java 目录 1tar zxvf /home/hadoop/jdk-7u80-linux-i586.tar.gz -C /usr/java/ 设置环境变量 1vi /etc/profile #追加如下内容123export JAVA_HOME=/usr/java/jdk1.7.0_80export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/libexport PATH=$PATH:$JAVA_HOME/bin 使环境变量生效 1source /etc/profile 测试环境变量设置 1java -version Hadoop安装与配置下载地址1http://hadoop.apache.org/releases.html 注:我下载的是hadoop-2.7.1.tar.gz 安装Hadoop使用winscp工具上传至服务器将压缩包上传至/home/hadoop目录*将压缩包解压至/usr目录1tar zxvf /home/hadoop/hadoop-2.7.1.tar.gz -C /usr/ 修改文件夹名称 1mv /usr/hadoop-2.7.1/ /usr/hadoop 创建hadoop数据目录 1mkdir /usr/hadoop/tmp 将hadoop文件夹授权给hadoop用户 1chown -R hadoop:hadoop /usr/hadoop/ 设置环境变量 1vi /etc/profile #追加如下内容1234export HADOOP_HOME=/usr/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib 使环境变量生效 1source /etc/profile 测试环境变量设置 1hadoop version 配置HDFS 切换至Hadoop用户 1su - hadoop 修改hadoop-env.sh 1cd /usr/hadoop/etc/hadoop/ vi hadoop-env.sh #追加如下内容1export JAVA_HOME=/usr/java/jdk1.7.0_80 修改core-site.xmlvi core-site.xml#添加如下内容 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://Hadoop.Master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/hadoop/tmp/&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xmlvi hdfs-site.xml#添加如下内容 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 格式化hdfshdfs namenode -format注：出现Exiting with status 0即为成功 启动hdfsstart-dfs.sh#停止命令 stop-dfs.sh注：输出如下内容 123456789101115/09/21 18:09:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [Hadoop.Master]Hadoop.Master: starting namenode, logging to /usr/hadoop/logs/hadoop-hadoop-namenode-Hadoop.Master.outHadoop.Master: starting datanode, logging to /usr/hadoop/logs/hadoop-hadoop-datanode-Hadoop.Master.outStarting secondary namenodes [0.0.0.0]The authenticity of host &apos;0.0.0.0 (0.0.0.0)&apos; can&apos;t be established.RSA key fingerprint is b5:96:b2:68:e6:63:1a:3c:7d:08:67:4b:ae:80:e2:e3.Are you sure you want to continue connecting (yes/no)? yes0.0.0.0: Warning: Permanently added &apos;0.0.0.0&apos; (RSA) to the list of known hosts.0.0.0.0: starting secondarynamenode, logging to /usr/hadoop/logs/hadoop-hadoop-secondarynamenode-Hadoop.Master.out15/09/21 18:09:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicab 查看进程jps注：输出类似如下内容1763 NameNode1881 DataNode2146 Jps2040 SecondaryNameNode 使用web查看Hadoop运行状态http://你的服务器ip地址:50070/在HDFS上运行WordCount 创建HDFS用户目录hdfs dfs -mkdir /userhdfs dfs -mkdir /user/hadoop #根据自己的情况调整/user/ 复制输入文件（要处理的文件）到HDFS上hdfs dfs -put /usr/hadoop/etc/hadoop input 查看我们复制到HDFS上的文件hdfs dfs -ls input 运行单词检索（grep）程序hadoop jar /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input output ‘dfs[a-z.]+’#WordCount#hadoop jar /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount input output#说明：output文件夹如已经存在则需要删除或指定其他文件夹。 查看运行结果hdfs dfs -cat output/*配置YARN 修改mapred-site.xmlcd /usr/hadoop/etc/hadoop/cp mapred-site.xml.template mapred-site.xmlvi mapred-site.xml#添加如下内容 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改yarn-site.xmlvi yarn-site.xml#添加如下内容 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动YARNstart-yarn.sh#停止yarn stop-yarn.sh 查看当前java进程jsp#输出如下4918 ResourceManager1663 NameNode1950 SecondaryNameNode5010 NodeManager5218 Jps1759 DataNode 运行你的mapReduce程序 配置好如上配置再运行mapReduce程序时即是yarn中运行。 使用web查看Yarn运行状态http://你的服务器ip地址:8088/ HDFS常用命令 创建HDFS文件夹 在根目录创建input文件夹hdfs dfs -mkdir -p /input 在用户目录创建input文件夹说明：如果不指定“/目录”，则默认在用户目录创建文件夹hdfs dfs -mkdir -p input#等同于 hdfs dfs -mkdir -p /user/hadoop/input 查看HDFS文件夹 查看HDFS根文件夹hdfs dfs -ls / 查看HDFS用户目录文件夹hdfs dfs -ls 查看HDFS用户目录文件夹下input文件夹hdfs dfs -ls input#等同与 hdfs dfs -ls /user/hadoop/input 复制文件到HDFShdfs dfs -put /usr/hadoop/etc/hadoop input 删除文件夹hdfs dfs -rm -r input]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive参数优化]]></title>
    <url>%2F2017%2F08%2F29%2Fhive%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑 1.设置合理solt数mapred.tasktracker.map.tasks.maximum每个tasktracker可同时运行的最大map task数，默认值2。mapred.tasktracker.reduce.tasks.maximum每个tasktracker可同时运行的最大reduce task数，默认值1 2.配置磁盘块mapred.local.dirmap task中间结果写本地磁盘路径，默认值${hadoop.tmp.dir}/mapred/local。可配置多块磁盘缓解写压力。当存在多个可以磁盘时，Hadoop将采用轮询方式将不同的map task中间结果写到磁盘上。 3.配置RPC Handler数mapred.job.tracker.handler.countjobtracker可并发处理来自tasktracker的RPC请求数，默认值10。 4.配置HTTP线程数tasktracker.http.threadsHTTP服务器的工作线程数，用于获取map task的输出结果，默认值40。 5.启用批调度 6.选择合适的压缩算法Job输出结果是否压缩mapred.output.compress是否压缩，默认值false。mapred.output.compression.type压缩类型，有NONE, RECORD和BLOCK，默认值RECORD。mapred.output.compression.codec压缩算法，默认值org.apache.hadoop.io.compress.DefaultCodec。map task输出是否压缩mapred.compress.map.output是否压缩，默认值falsemapred.map.output.compression.codec压缩算法，默认值org.apache.hadoop.io.compress.DefaultCodec。 7.设置失败容忍度mapred.max.map.failures.percent例如：set mapred.max.map.failures.percent=30;作业最多允许失败的map task比例，默认值0。mapred.max.reduce.failures.percent作业最多允许失败的reduce task比例，默认值0。mapred.map.max.attempts一个map task的最多重试次数，默认值4。mapred.reduce.max.attempts一个reduce task的最多重试次数，默认值4。 8.设置跳过坏记录mapred.skip.attempts.to.start.skipping当任务失败次数达到该值时，启用跳过坏记录功能，默认值2。 mapred.skip.out.dir检测出的坏记录存放目录，默认值为输出目录的_logs/skip，设置为none表示不输出。mapred.skip.map.max.skip.recordsmap task最多允许的跳过记录数，默认值0。mapred.skip.reduce.max.skip.groupsreduce task最多允许的跳过记录数，默认值0。 9.配置jvm重用mapred.job.reuse.jvm.num.tasks一个jvm可连续启动多个同类型任务，默认值1，若为-1表示不受限制。 10.配置jvm参数mapred.child.java.opts任务启动的jvm参数，默认值-Xmx200m，建议值-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m -verbose:gc -Xloggc:/tmp/@taskid@.gc map task调优io.sort.mb默认值100Mio.sort.record.percent默认值0.05io.sort.spill.percent默认值0.80 12.reduce task调优io.sort.factor默认值10mapred.reduce.parallel.copies默认值5]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>Hhive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka参数优化]]></title>
    <url>%2F2017%2F08%2F25%2Fkafka%E4%BC%98%E5%8C%96%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑 非负整数，用于唯一标识brokerbroker.id=0 broker 服务监听端口port=9092 broker 发布给生产者消费者的hostname，会存储在zookeeper。配置好这个host可以实现内网外网同时访问。advertised.host.name=host1 broker 发布给生产者消费者的port，会存储在zookeeper。advertised.port=9092 处理网络请求的线程数量，一般默认配置就好num.network.threads=3 处理磁盘的线程数量，一般默认配置就好num.io.threads=8 socket server 发送数据缓冲区大小socket.send.buffer.bytes=102400 socket server 接受数据缓冲区大小socket.receive.buffer.bytes=102400 soket server 可接受最大消息大小，防止oomsocket.request.max.bytes=104857600 kafka存放消息的目录log.dirs=/home/data/kafka/kafka-logs 每个topic默认partition数量，根据消费者实际情况配置，配置过小会影响消费性能num.partitions=50 kafka启动恢复日志,关闭前日志刷盘的线程数num.recovery.threads.per.data.dir=1 日志保留时间log.retention.minutes=30 日志保留大小log.retention.bytes=53687091200 日志 segment file 大小. 超过这个大小创建新segment filelog.segment.bytes=67108864 日志 segment file 刷新时间. 超过这个时间创建新segment filelog.roll.hours=24 日志淘汰检查间隔时间log.retention.check.interval.ms=10000 Zookeeper host和portzookeeper.connect=localhost:2181 连接zookeeper超时时间zookeeper.connection.timeout.ms=6000 清除fetch purgatory 间隔消息条数fetch.purgatory.purge.interval.requests=100 清除producer purgatory 间隔消息条数producer .purgatory.purge.interval.requests=100 是否可以通过管理工具删除topic，默认是falsedelete.topic.enable=true 日志传输时候的压缩格式，可选择lz4, snappy, gzip,不压缩。建议打开压缩，可以提高传输性能，压缩格式的选择可以参考文章结尾的参考资料。compression.type=snappy 启用压缩的topic名称。若上面参数选择了一个压缩格式，那么压缩仅对本参数指定的topic有效，若本参数为空，则对所有topic有效。compressed.topics=topic1 用来从主partion同步数据的线程数，默认为1，建议适当调大，数据量大的时候一个同步线程可能不够用num.replica.fetchers=3 消息日志备份因子，默认是1default.replication.factor=2]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase手动优化]]></title>
    <url>%2F2017%2F08%2F20%2Fhbase%E6%89%8B%E5%8A%A8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑配置优化 zookeeper.session.timeout默认值：3分钟（180000ms）说明：RegionServer与Zookeeper间的连接超时时间。当超时时间到后，ReigonServer会被Zookeeper从RS集群清单中移除，HMaster收到移除通知后，会对这台server负责的regions重新balance，让其他存活的* RegionServer接管.调优：这个timeout决定了RegionServer是否能够及时的failover。设置成1分钟或更低，可以减少因等待超时而被延长的failover时间。不过需要注意的是，对于一些Online应用，RegionServer从宕机到恢复时间本身就很短的（网络闪断，crash等故障，运维可快速介入），如果调低timeout时间，反而会得不偿失。因为当ReigonServer被正式从RS集群中移除时，HMaster就开始做balance了（让其他RS根据故障机器记录的WAL日志进行恢复）。当故障的RS在人工介入恢复后，这个balance动作是毫无意义的，反而会使负载不均匀，给RS带来更多负担。特别是那些固定分配regions的场景。 Hbase.zookeeper.quorum默认值：localhost说明：hbase所依赖的zookeeper部署调优：部署的zookeeper越多，可靠性就越高，但是部署只能部署奇数个，主要为了便于选出leader。最好给每个zookeeper 1G的内存和独立的磁盘，可以确保高性能。hbase.zookeeper.property.dataDir可以修改zookeeper保存数据的路径。 hbase.regionserver.handler.count默认值：10说明：RegionServer的请求处理IO线程数。调优：这个参数的调优与内存息息相关。较少的IO线程，适用于处理单次请求内存消耗较高的Big PUT场景（大容量单次PUT或设置了较大cache的scan，均属于Big PUT）或ReigonServer的内存比较紧张的场景。较多的IO线程，适用于单次请求内存消耗低，TPS要求非常高的场景。设置该值的时候，以监控内存为主要参考。这里需要注意的是如果server的region数量很少，大量的请求都落在一个region上，因快速充满memstore触发flush导致的读写锁会影响全局TPS，不是IO线程数越高越好。压测时，开启Enabling RPC-level logging，可以同时监控每次请求的内存消耗和GC的状况，最后通过多次压测结果来合理调节IO线程数。这里是一个案例?Hadoop and HBase Optimization for Read Intensive Search Applications，作者在SSD的机器上设置IO线程数为100，仅供参考。 hbase.hregion.max.filesize默认值：256M说明：在当前ReigonServer上单个Reigon的最大存储空间，单个Region超过该值时，这个Region会被自动split成更小的region。调优：小region对split和compaction友好，因为拆分region或compact小region里的storefile速度很快，内存占用低。缺点是split和compaction会很频繁。特别是数量较多的小region不停地split, compaction，会导致集群响应时间波动很大，region数量太多不仅给管理上带来麻烦，甚至会引发一些Hbase的bug。一般512以下的都算小region。 大region，则不太适合经常split和compaction，因为做一次compact和split会产生较长时间的停顿，对应用的读写性能冲击非常大。此外，大region意味着较大的storefile，compaction时对内存也是一个挑战。当然，大region也有其用武之地。如果你的应用场景中，某个时间点的访问量较低，那么在此时做compact和split，既能顺利完成split和compaction，又能保证绝大多数时间平稳的读写性能。 既然split和compaction如此影响性能，有没有办法去掉？compaction是无法避免的，split倒是可以从自动调整为手动。只要通过将这个参数值调大到某个很难达到的值，比如100G，就可以间接禁用自动split（RegionServer不会对未到达100G的region做split）。再配合RegionSplitter这个工具，在需要split时，手动split。手动split在灵活性和稳定性上比起自动split要高很多，相反，管理成本增加不多，比较推荐online实时系统使用。 内存方面，小region在设置memstore的大小值上比较灵活，大region则过大过小都不行，过大会导致flush时app的IO wait增高，过小则因store file过多影响读性能。 hbase.regionserver.global.memstore.upperLimit/lowerLimit默认值：0.4/0.35upperlimit说明：hbase.hregion.memstore.flush.size 这个参数的作用是当单个Region内所有的memstore大小总和超过指定值时，flush该region的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模式来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。这个参数的作用是防止内存占用过大，当ReigonServer内所有region的memstores所占用内存总和达到heap的40%时，HBase会强制block所有的更新并flush这些region以释放所有memstore占用的内存。lowerLimit说明： 同upperLimit，只不过lowerLimit在所有region的memstores所占用内存达到Heap的35%时，不flush所有的memstore。它会找一个memstore内存占用最大的region，做个别flush，此时写更新还是会被block。lowerLimit算是一个在所有region强制flush导致性能降低前的补救措施。在日志中，表现为 “** Flush thread woke up with memory above low water.”调优：这是一个Heap内存保护参数，默认值已经能适用大多数场景。参数调整会影响读写，如果写的压力大导致经常超过这个阀值，则调小读缓存hfile.block.cache.size增大该阀值，或者Heap余量较多时，不修改读缓存大小。如果在高压情况下，也没超过这个阀值，那么建议你适当调小这个阀值再做压测，确保触发次数不要太多，然后还有较多Heap余量的时候，调大hfile.block.cache.size提高读性能。还有一种可能性是?hbase.hregion.memstore.flush.size保持不变，但RS维护了过多的region，要知道 region数量直接影响占用内存的大小。 hfile.block.cache.size 默认值：0.2说明：storefile的读缓存占用Heap的大小百分比，0.2表示20%。该值直接影响数据读的性能。调优：当然是越大越好，如果写比读少很多，开到0.4-0.5也没问题。如果读写较均衡，0.3左右。如果写比读多，果断默认吧。设置这个值的时候，你同时要参考?hbase.regionserver.global.memstore.upperLimit?，该值是memstore占heap的最大百分比，两个参数一个影响读，一个影响写。如果两值加起来超过80-90%，会有OOM的风险，谨慎设置。 hbase.hstore.blockingStoreFiles默认值：7说明：在flush时，当一个region中的Store（Coulmn Family）内有超过7个storefile时，则block所有的写请求进行compaction，以减少storefile数量。调优：block写请求会严重影响当前regionServer的响应时间，但过多的storefile也会影响读性能。从实际应用来看，为了获取较平滑的响应时间，可将值设为无限大。如果能容忍响应时间出现较大的波峰波谷，那么默认或根据自身场景调整即可。 hbase.hregion.memstore.block.multiplier默认值：2说明：当一个region里的memstore占用内存大小超过hbase.hregion.memstore.flush.size两倍的大小时，block该region的所有请求，进行flush，释放内存。虽然我们设置了region所占用的memstores总内存大小，比如64M，但想象一下，在最后63.9M的时候，我Put了一个200M的数据，此时memstore的大小会瞬间暴涨到超过预期的hbase.hregion.memstore.flush.size的几倍。这个参数的作用是当memstore的大小增至超过hbase.hregion.memstore.flush.size 2倍时，block所有请求，遏制风险进一步扩大。调优： 这个参数的默认值还是比较靠谱的。如果你预估你的正常应用场景（不包括异常）不会出现突发写或写的量可控，那么保持默认值即可。如果正常情况下，你的写请求量就会经常暴长到正常的几倍，那么你应该调大这个倍数并调整其他参数值，比如hfile.block.cache.size和hbase.regionserver.global.memstore.upperLimit/lowerLimit，以预留更多内存，防止HBase server OOM。 hbase.hregion.memstore.mslab.enabled默认值：true说明：减少因内存碎片导致的Full GC，提高整体性能。调优：详见 http://kenwublog.com/avoid-full-gc-in-hbase-using-arena-allocation hbase.client.scanner.caching默认值：1说明：scanner调用next方法一次获取的数据条数调优：少的RPC是提高hbase执行效率的一种方法，理论上一次性获取越多数据就会越少的RPC，也就越高效。但是内存是最大的障碍。设置这个值的时候要选择合适的大小，一面一次性获取过多数据占用过多内存，造成其他程序使用内存过少。或者造成程序超时等错误（这个超时与hbase.regionserver.lease.period相关）。 hbase.regionserver.lease.period默认值：60000说明：客户端租用HRegion server 期限，即超时阀值。调优：这个配合hbase.client.scanner.caching使用，如果内存够大，但是取出较多数据后计算过程较长，可能超过这个阈值，适当可设置较长的响应时间以防被认为宕机。]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce参数优化]]></title>
    <url>%2F2017%2F08%2F19%2FMapReduce%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑1 core-default.xml： hadoop.tmp.dir： 默认值： /tmp说明： 尽量手动配置这个选项，否则的话都默认存在了里系统的默认临时文件/tmp里。并且手动配置的时候，如果服务器是多磁盘的，每个磁盘都设置一个临时文件目录，这样便于mapreduce或者hdfs等使用的时候提高磁盘IO效率。 fs.trash.interval： 默认值： 0说明： 这个是开启hdfs文件删除自动转移到垃圾箱的选项，值为垃圾箱文件清除时间。一般开启这个会比较好，以防错误删除重要文件。单位是分钟。 io.file.buffer.size： 默认值：4096说明：SequenceFiles在读写中可以使用的缓存大小，可减少 I/O 次数。在大型的 Hadoop cluster，建议可设定为 65536 到 131072。 2 hdfs-default.xml： dfs.blocksize： 默认值：134217728说明： 这个就是hdfs里一个文件块的大小了，CDH5中默认128M。太大的话会有较少map同时计算，太小的话也浪费可用map个数资源，而且文件太小namenode就浪费内存多。根据需要进行设置。 dfs.namenode.handler.count： 默认值：10说明：设定 namenode server threads 的数量，这些 threads 會用 RPC 跟其他的 datanodes 沟通。当 datanodes 数量太多时会发現很容易出現 RPC timeout，解決方法是提升网络速度或提高这个值，但要注意的是 thread 数量多也表示 namenode 消耗的内存也随着增加 3 mapred-default.xml： mapred.reduce.tasks（mapreduce.job.reduces）： 默认值：1说明：默认启动的reduce数。通过该参数可以手动修改reduce的个数。 mapreduce.task.io.sort.factor： 默认值：10说明：Reduce Task中合并小文件时，一次合并的文件数据，每次合并的时候选择最小的前10进行合并。 mapreduce.task.io.sort.mb： 默认值：100说明： Map Task缓冲区所占内存大小。 mapred.child.java.opts： 默认值：-Xmx200m说明：jvm启动的子线程可以使用的最大内存。建议值-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m -verbose:gc -Xloggc:/tmp/@taskid@.gc mapreduce.jobtracker.handler.count： 默认值：10说明：JobTracker可以启动的线程数，一般为tasktracker节点的4%。 mapreduce.reduce.shuffle.parallelcopies： 默认值：5说明：reuduce shuffle阶段并行传输数据的数量。这里改为10。集群大可以增大。 mapreduce.tasktracker.http.threads： 默认值：40说明：map和reduce是通过http进行数据传输的，这个是设置传输的并行线程数。 mapreduce.map.output.compress： 默认值：false说明： map输出是否进行压缩，如果压缩就会多耗cpu，但是减少传输时间，如果不压缩，就需要较多的传输带宽。配合 mapreduce.map.output.compress.codec使用，默认是 org.apache.hadoop.io.compress.DefaultCodec，可以根据需要设定数据压缩方式。 mapreduce.reduce.shuffle.merge.percent： 默认值： 0.66说明：reduce归并接收map的输出数据可占用的内存配置百分比。类似mapreduce.reduce.shuffle.input.buffer.percen属性。 mapreduce.reduce.shuffle.memory.limit.percent： 默认值： 0.25说明：一个单一的shuffle的最大内存使用限制。 mapreduce.jobtracker.handler.count： 默认值： 10说明：可并发处理来自tasktracker的RPC请求数，默认值10。 mapred.job.reuse.jvm.num.tasks（mapreduce.job.jvm.numtasks）： 默认值： 1说明：一个jvm可连续启动多个同类型任务，默认值1，若为-1表示不受限制。 mapreduce.tasktracker.tasks.reduce.maximum： 默认值： 2说明：一个tasktracker并发执行的reduce数，建议为cpu核数]]></content>
      <categories>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[amabri卸载]]></title>
    <url>%2F2017%2F08%2F10%2Fambari%E5%8D%B8%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[1.删除hdp.repo和hdp-util.repo1234cd /etc/yum.repos.d/rm -rf hdp*rm -rf HDP*rm -rf ambari* 2.删除安装包用yum list installed | grep HDP来检查安装的ambari的包123456789101112131415161718192021222324252627282930313233343536373839404142yum remove -y sqoop.noarch yum remove -y lzo-devel.x86_64 yum remove -y hadoop-libhdfs.x86_64 yum remove -y rrdtool.x86_64 yum remove -y hbase.noarch yum remove -y pig.noarch yum remove -y lzo.x86_64 yum remove -y ambari-log4j.noarch yum remove -y oozie.noarch yum remove -y oozie-client.noarch yum remove -y gweb.noarch yum remove -y snappy-devel.x86_64 yum remove -y hcatalog.noarch yum remove -y python-rrdtool.x86_64 yum remove -y nagios.x86_64 yum remove -y webhcat-tar-pig.noarch yum remove -y snappy.x86_64 yum remove -y libconfuse.x86_64 yum remove -y webhcat-tar-hive.noarch yum remove -y ganglia-gmetad.x86_64 yum remove -y extjs.noarch yum remove -y hive.noarch yum remove -y hadoop-lzo.x86_64 yum remove -y hadoop-lzo-native.x86_64 yum remove -y hadoop-native.x86_64 yum remove -y hadoop-pipes.x86_64 yum remove -y nagios-plugins.x86_64 yum remove -y hadoop.x86_64 yum remove -y zookeeper.noarch yum remove -y hadoop-sbin.x86_64 yum remove -y ganglia-gmond.x86_64 yum remove -y libganglia.x86_64 yum remove -y perl-rrdtool.x86_64yum remove -y epel-release.noarchyum remove -y compat-readline5*yum remove -y fping.x86_64yum remove -y perl-Crypt-DES.x86_64yum remove -y exim.x86_64yum remove -y ganglia-web.noarchyum remove -y perl-Digest-HMAC.noarchyum remove -y perl-Digest-SHA1.x86_64yum remove -y bigtop-jsvc.x86_64 3.删除快捷方式12345678910111213cd /etc/alternativesrm -rf hadoop-etc rm -rf zookeeper-conf rm -rf hbase-conf rm -rf hadoop-log rm -rf hadoop-lib rm -rf hadoop-default rm -rf oozie-conf rm -rf hcatalog-conf rm -rf hive-conf rm -rf hadoop-man rm -rf sqoop-conf rm -rf hadoop-conf 4.删除用户1234567891011121314151617181920212223242526userdel nagios userdel hive userdel ambari-qa userdel hbase userdel oozie userdel hcat userdel mapred userdel hdfs userdel rrdcached userdel zookeeper #userdel mysql userdel sqoopuserdel puppetuserdel yarnuserdel tezuserdel hadoopuserdel knoxuserdel stormuserdel falconuserdel flumeuserdel nagiosuserdel adminuserdel postgresuserdel hdfsuserdel zookeeperuserdel hbase 5.删除文件夹1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677rm -rf /hadooprm -rf /etc/hadoop rm -rf /etc/hbase rm -rf /etc/hcatalog rm -rf /etc/hive rm -rf /etc/ganglia rm -rf /etc/nagios rm -rf /etc/oozie rm -rf /etc/sqoop rm -rf /etc/zookeeper rm -rf /var/run/hadoop rm -rf /var/run/hbase rm -rf /var/run/hive rm -rf /var/run/ganglia rm -rf /var/run/nagios rm -rf /var/run/oozierm -rf /var/run/zookeeperrm -rf /var/log/hadoop rm -rf /var/log/hbase rm -rf /var/log/hive rm -rf /var/log/nagios rm -rf /var/log/oozie rm -rf /var/log/zookeeper rm -rf /usr/lib/hadooprm -rf /usr/lib/hbase rm -rf /usr/lib/hcatalog rm -rf /usr/lib/hive rm -rf /usr/lib/oozie rm -rf /usr/lib/sqoop rm -rf /usr/lib/zookeeper rm -rf /var/lib/hive rm -rf /var/lib/ganglia rm -rf /var/lib/oozie rm -rf /var/lib/zookeeper rm -rf /var/tmp/oozie rm -rf /tmp/hive rm -rf /tmp/nagios rm -rf /tmp/ambari-qa rm -rf /tmp/sqoop-ambari-qa rm -rf /var/nagios rm -rf /hadoop/oozie rm -rf /hadoop/zookeeper rm -rf /hadoop/mapred rm -rf /hadoop/hdfs rm -rf /tmp/hadoop-hive rm -rf /tmp/hadoop-nagios rm -rf /tmp/hadoop-hcat rm -rf /tmp/hadoop-ambari-qa rm -rf /tmp/hsperfdata_hbase rm -rf /tmp/hsperfdata_hive rm -rf /tmp/hsperfdata_nagios rm -rf /tmp/hsperfdata_oozie rm -rf /tmp/hsperfdata_zookeeper rm -rf /tmp/hsperfdata_mapred rm -rf /tmp/hsperfdata_hdfs rm -rf /tmp/hsperfdata_hcat rm -rf /tmp/hsperfdata_ambari-qarm -rf /etc/flumerm -rf /etc/stormrm -rf /etc/hive-hcatalogrm -rf /etc/tezrm -rf /etc/falconrm -rf /var/run/flumerm -rf /var/run/stormrm -rf /var/run/webhcatrm -rf /var/run/hadoop-yarnrm -rf /var/run/hadoop-mapreducerm -rf /var/log/flumerm -rf /var/log/stormrm -rf /var/log/hadoop-yarnrm -rf /var/log/hadoop-mapreducerm -rf /usr/lib/nagiosrm -rf /var/lib/hdfsrm -rf /var/lib/hadoop-hdfsrm -rf /var/lib/hadoop-yarnrm -rf /var/lib/hadoop-mapreducerm -rf /tmp/hadoop-hdfs 5.重置数据库，删除ambari包#采用这句命令来检查1yum list installed | grep ambari 123456789ambari-server stopambari-agent stopambari-server resetyum remove -y ambari-*yum remove -y postgresqlrm -rf /etc/yum.repos.d/ambari*rm -rf /var/lib/ambari*rm -rf /var/log/ambari*rm -rf /etc/ambari*]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[错误及解决方案]]></title>
    <url>%2F2017%2F08%2F10%2Fambari%E6%90%AD%E5%BB%BA%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[12ERROR: Exiting with exit code 1. REASON: Database check failed to complete. Please check /var/log/ambari-server/ambari-server.log and /var/log/ambari-server/ambari-server-check-database.log for more information. 解决方法：查看日志，具体什么错.例如：数据库未开启、数据库配置问题 撤销已经赋予给 MySQL 用户权限的权限。revoke 跟 grant 的语法差不多，只需要把关键字 “to” 换成 “from” 即可：1revoke all on *.* from &apos;root&apos;@&apos;192.168.0.197&apos;; 12345678910[root@datanode01 ~]# yum makecacheLoaded plugins: fastestmirrorDetermining fastest mirrorsambari-2.4.1.0 | 2.9 kB 00:00 ambari-2.4.1.0/filelists_db | 139 kB 00:00 ambari-2.4.1.0/primary_db | 8.3 kB 00:00 ambari-2.4.1.0/other_db | 1.3 kB 00:00 file:///mnt/repodata/repomd.xml: [Errno 14] Could not open/read file:///mnt/repodata/repomd.xmlTrying other mirror.Error: Cannot retrieve repository metadata (repomd.xml) for repository: c6-media. Please verify its path and try again 如果有后面两个文件，处理方式mv CentOS-Media.repo CentOS-Media.repo.bak没有的话，源问题，换源repos tip:1、当提示要做如下操作的时候1Be?sure?you?have?run:ambari-server?setup?--jdbc-db=mysql?--jdbc-driver=/path/to/mysql/**** 需要下载mysql-connector-java-5.1.39.tar驱动，解压得到mysql-connector-java-5.1.39-bin.jar文件执行如下命令：ambari-server?setup?–jdbc-db=mysql?–jdbc-driver=/usr/lib/java/mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar同时，设置文件权限为644 2017.7.191.ERROR namenode.NameNode (NameNode.java:main(1759)) - Failed to start namenode.java.net.BindException: Port in use: datanode01:50070 不能获取映射地址，需要修改hosts中的映射ip为真实ip（ifconfig）2.错删自定义的service导致不能登录amnari UI（日志报出database问题） （百度方案无解，最后重装ambari-server(发现重装无法下手，百度后删除ambari相关组件以及数据库的完全卸载，完成)）3.使用ambari-server启动HDFS时，DataNode无法启动（出现port in use：localhost 0）。解决方式，在hosts文件中localhost 172.0.0.1被#；去除#后解决 一般故障排除Ambari服务器：检查/var/log/ambari-server/ambari-server.[log|out]是否存在错误。Ambari代理：检查/var/log/ambari-agent/ambari-agent.[log|out]是否存在错误。请注意，如果Ambari Agent在/var/log/ambari-agent/ambari-agent.out中有任何输出，则表明存在重大问题。 服务无法启动HDFS：检查/ var / log / hadoop / hdfs下的日志文件MapReduce：检查/ var / log / hadoop / mapred下的日志文件HBase：检查/ var / log / hbase下的日志文件Hive：检查/ var / log / hive下的日志文件Oozie：检查/ var / log / oozie下的日志文件ZooKeeper：检查/ var / log / zookeeper下的日志文件WebHCat：检查/ var / log / webhcat下的日志文件Nagios：检查/ var / log / nagios下的日志文件 2017.7.20Service ‘userhome’ check failed: java.io.FileNotFoundException: File does not exist: /user/admin解决方案：sudo -u hdfs hdfs dfs -mkdir /user/adminsudo -u hdfs hdfs dfs -chown admin:hadoop /user/admin 资源池问题FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed forblock pool Block pool BP-1480406410-192.168.1.181-1398701121586 (storage idDS-167510828-192.168.1.191-50010-1398750515421)原因：每次namenode format会重新创建一个namenodeId,而data目录包含了上次format时的id,namenode format清空了namenode下的数据,但是没有清空datanode下的数据,导致启动时失败,所要做的就是每次fotmat前,清空data下的所有目录.: d6 E2 t&amp; M” g7 a* q3 l, H解决办法：停掉集群，删除问题节点的data目录下的所有内容。即hdfs-site.xml文件中配置的dfs.data.dir目录。重新格式化namenode。 另一个更省事的办法：先停掉集群，然后将datanode节点目录/dfs/data/current/VERSION中的修改为与namenode一致即可。其实我没解决，直接重装 block missing问题会导致进入安全模式：处理方式，删除缺失包，在HDFS用户下退出安全模式 添加节点时，需要查看该节点root磁盘下的剩余大小，如果磁盘空间不足，则扩大磁盘再添加节点。不要问我为什么，说多了都是泪。集群就是这么崩盘的。]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ambari搭建]]></title>
    <url>%2F2017%2F08%2F09%2Fambari%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.物理条件：三台centos前期条件：修改每台centos的名称，分别为namenode、datanode01、datanode02 ssh免密码配置：namenode 可以免密码登录到datanode01、datanode02配置ssh 在namenode上生成密钥对：ssh-keygen –t rsa 在/root/.ssh中会有生成的密钥对 将namenode 的 id_rsa.pub写入datanode01的/root/.ssh authorized_keys文件中。再将datanode01的authorized_keys 写入namenode /root/.sshdatanode02同理操作scp .ssh/id_rsa.pub chenlb@192.168.1.181:/home/chenlb/id_rsa.pubcat id_rsa.pub &gt;&gt; .ssh/authorized_keys 将namenode中的三个机器的私钥 id_rsa提取到桌面，后续使用安装ntp服务yum install ntpservice ntpd startchkconfig ntpd on 2.更换源：将/etc/yum.repos.d/下的原centos.repos备份，然后添加本地源（我这是之前师傅在其他服务器布 置好了的源，你们可在网上查找相关源） 3.验证本机名Hostname -f 显示 namenode 其他两台一样 4.关闭防火墙Service iptables stop 禁用自启动 Chkconfig iptables off 5.禁用ipv6使用lsmod查看系统启动的模块：ipv6相关的模块是net-pf-10 ipv6 在vi /etc/modprobe.d/dist.conf 中最后添加 alias net-pf-10 off alias ipv6 off 重启后，再使用lsmod查看ipv6的相应模块还在不在 6.禁用SELinux配置selinuxvi /etc/sysconfig/selinux插入SELINUX=disabled暂时禁用 setenforce 0 7.配置禁用THP(每次重启机器后需要从新配置) 查看当前THP状态 cat /sys/kernel/mm/transparent_hugepage/enabled 如果为always madvise [never]则为禁用配置禁用1234echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defragecho never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/ transparent_hugepage/defragecho never &gt; /sys/kernel/mm/ transparent_hugepage/enabled 8.clean源 重新加载yum clean all yum makecache 9.配置mysql作为数据库（只在ambari-server安装机器上做） 查看当前安装数据库信息 rpm -qa | grep -i mysql 卸载方法 yum -y remove +name 安装MySQL 查看当前源中是否有提供的MySQL yum list | grep mysql 安装mysql-server安装 安装后mysql 只有一个用户root 且第一次开启时需要设置密码，我们这通过命令提前设置 mysqladmin -u root password ‘’ 设置为自启动 chkconfig mysqld on 登录到root用户 123456789101112mysql -u root -p create user ‘amabri’@’% ’ identified by ‘bigdata’;grant all privileges on *.* to ‘ambari’@’%’;create user ‘ambari’@’localhost’ identified by ‘bigdata’;grant all privileges on *.* to ‘ambari’@’localhost’;create user ‘ambari’@’namenode’ identified by ‘bigdata’;grant all privileges on *.* to ‘ambari’@’namenode’;登录ambari用户mysql -u ambari -pbigdatacreate database ambari;use ambari; 10.部署 （1） yum安装1yum -y install ambari-server (每台都要装) 后进入1234mysql -u ambari -pbigdatause ambari;source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql; （2） jdk的安装 下载jdk1.8.0_45安装包 放入/usr/lib/jvm/jdk1.8.0_45/ 解压 配置环境变量 /etc/profile 文件最后加上1234567#Java environment JAVA_HOME=/usr/java/jdk1.8.0_45PATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/export JAVA_HOMEexport PATHexport CLASSPATH 执行：123sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.8.0_45/bin/java 300 sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jdk1.8.0_45/bin/javac 300sudo update-alternatives --install /usr/bin/jar jar /usr/lib/jvm/jdk1.8.0_45/bin/jar 300 查看当前Java版本1java -version （3） 安装ambari-server (安装在一台centos上即可，部署为ambari-server的机器)1ambari-server setup -j /usr/lib/jvm/jdk1.8.0­_45/ 安装显示：1234567891011121314151617181920212223242526272829303132333435363738Using python /usr/bin/python Setup ambari‐server Checking SELinux... SELinux status is &apos;enabled&apos; SELinux mode is &apos;permissive&apos; WARNING: SELinux is set to &apos;permissive&apos; mode and temporarily disabled. OK to continue [y/n] (y)? Customize user account for ambari‐server daemon [y/n] (n)? Adjusting ambari‐server permissions and ownership... Checking firewall status... Checking JDK... WARNING: JAVA_HOME /usr/lib/jvm/jdk1.8.0_45 must be valid on ALL hosts WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Completing setup... Configuring database... Enter advanced database configuration [y/n] (n)? y Configuring database... ============================================================================= Choose one of the following options: [1] ‐ PostgreSQL (Embedded) [2] ‐ Oracle [3] ‐ MySQL / MariaDB [4] ‐ PostgreSQL [5] ‐ Microsoft SQL Server (Tech Preview) [6] ‐ SQL Anywhere [7] ‐ BDB ============================================================================= Enter choice (3): 3 Hostname (localhost): Port (3306): Database name (ambari): Username (ambari): Enter Database Password (bigdata): Configuring ambari database... Copying JDBC drivers to server resources... Configuring remote database connection properties...WARNING: Before starting Ambari Server, you must run the following DDL against the databa se to create the schema: /var/lib/ambari Proceed with configuring remote database connection properties [y/n] (y)? y Extracting system views............... Adjusting ambari‐server permissions and ownership... Ambari Server &apos;setup&apos; completed successfully. （4）启动ambari-server1ambari-server start 启动成功后通过：http：//namenode:8080访问 登录账号及密码：admin （6） 进程操作 1.查看ambari进程1ps -ef | grep ambari 2.停止ambari进程 1ambari-server stop 3.重启ambari进程 1ambari-server restart （7） 修改端口1vi /etc/ambari-server/conf/ambari.properties 插入或编辑以下内容 Client.api.port = &lt;port_number&gt; 11 安装ambari-agent(每台机器都要装)1.安装ambari-agent 1yum install ambari-agent 2.配置 1vi /etc/ambari-agent/conf/ambari-agent.ini 插入或更改以下内容 1hostname = namenode 最后使用谷歌浏览器登录：http://namenode:8080 账号：admin 密码：admin注意：选择操作系统时，应该选择当前机器的版本。如果是本地源，则需要修改HDP、HDP-UTILS的位置 If you are lucky enough, that I wish you success.Said too much, tears. 如果出现ip绑定问题，修改/etc/hosts/中的ip为每台机器的真实ip查看ip命令ifconfig出现mysql drive驱动问题1yum install mysql-connector-java]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos和Ubuntu本地源制作]]></title>
    <url>%2F2017%2F08%2F05%2Fcentos%E5%92%8CUbuntu%E5%88%B6%E4%BD%9C%E6%9C%AC%E5%9C%B0%E6%BA%90%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑CentOS系统 使用yum安装软件时，下载的* .rpm包缓存在/var/cache/yum/x86_64/7/； 创建目录 用于存放特定软件所需的软件包； 1mkdir -p /opt/packages 下载软件包 1yum install -y --downloadonly --downloaddir=/opt/packages python-openstackclient 生成repo文件 123yum install -y createrepocreaterepo /opt/packages 生成压缩包 1tar -zcf packages.tgz packages/ 配置本地源 123456789tar -zxf packages.tgz -C /optvim /etc/yum.repos.d/local.repo[Local]name=Local Yumbaseurl=file:////opt/packages/gpgcheck=0enabled=1 客户端安装软件： 1yum install python-openstackclient Ubuntu系统 使用apt命令安装软件时，下载的* .deb包缓存在/var/cache/apt/archives/； 创建目录 用于存放特定软件所需的软件包； 1mkdir -p /opt/packages 下载软件包 清除旧的缓存： 1rm -f /var/cache/apt/archives/* .deb 下载软件包： 1apt install -d -y --force-yes PACKAGE-NAME 拷贝软件包 1cp /var/cache/apt/archives/* .deb /opt/packages/ 生成Packages.gz包Packages.gz中包含软件包信息以及其依赖关系信息； 安装dpkg-dev以便使用dpkg-scanpackages命令生成Packages.gz文件； apt install -y dpkg-dev 单机版本地源 此处忽略一切的警告(warning)； cd /opt/dpkg-scanpackages packages/ /dev/null | gzip &gt; /opt/packages/Packages.gz -r 制作成压缩包，便于网络传输； cd /opt/tar -zcf packages.tgz packages/ 将压缩包拷贝到目标主机； 解压压缩包并配置软件源(目标主机)： 12tar -zxf packages.tgz -C /optecho &apos;deb file:///opt/ packages/&apos; &gt; /etc/apt/sources.list 临时的Web共享本地源 此处忽略一切的警告(warning)； 12cd /opt/packages/dpkg-scanpackages . /dev/null | gzip &gt; /opt/packages/Packages.gz -r 使用Python自带的SimpleHTTPServer会在当前目录启动一个简易的Web服务器； 12cd /opt/packages/python -m SimpleHTTPServer PORT 配置软件源(目标主机): 1echo &apos;deb http://IP:PORT/ /&apos; &gt; /etc/apt/sources.list 搭建Web服务共享本地源 搭建主流的Web服务：Apache服务或Nginx服务； 配置Web服务，根目录指向/opt/packages/； 配置软件源(目标主机): 1echo &apos;deb http://IP:PORT/ /&apos; &gt; /etc/apt/sources.list 安装软件(目标主机) 12apt updateapt install -y --force-yes PACKAGE-NAME]]></content>
      <categories>
        <category>本地源</category>
      </categories>
      <tags>
        <tag>本地源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS手动优化]]></title>
    <url>%2F2017%2F07%2F11%2FHDFS%E6%89%8B%E5%8A%A8%E4%BC%98%E5%8C%96%20%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑一 Linux文件系统参数调整（1） noatime 和 nodiratime属性文件挂载时设置这两个属性可以明显提高性能。。默认情况下，Linux ext2/ext3 文件系统在文件被访问、创建、修改时会记录下文件的时间戳，比如：文件创建时间、最近一次修改时间和最近一次访问时间。如果系统运行时要访问大量文件，关闭这些操作，可提升文件系统的性能。Linux 提供了 noatime 这个参数来禁止记录最近一次访问时间戳。（2） readahead buffer调整linux文件系统中预读缓冲区地大小，可以明显提高顺序读文件的性能。默认buffer大小为256 sectors，可以增大为1024或者2408 sectors（注意，并不是越大越好）。可使用blockdev命令进行调整。（3） 避免RAID和LVM操作避免在TaskTracker和DataNode的机器上执行RAID和LVM操作，这通常会降低性能。3.2.2 Hadoop通用参数调整（1） dfs.namenode.handler.count或mapred.job.tracker.handler.countamenode或者jobtracker中用于处理RPC的线程数，默认是10，较大集群，可调大些，比如64。（2） dfs.datanode.handler.countdatanode上用于处理RPC的线程数。默认为3，较大集群，可适当调大些，比如8。需要注意的是，每添加一个线程，需要的内存增加。（3） tasktracker.http.threadHTTP server上的线程数。运行在每个TaskTracker上，用于处理map task输出。大集群，可以将其设为40~50。 二 HDFS相关配置（1） dfs.replicatio文件副本数，通常设为3，不推荐修改。（2） dfs.block.sizeHDFS中数据block大小，默认为64M，对于较大集群，可设为128MB或者256MB。（也可以通过参数mapred.min.split.size配置）（3） mapred.local.dir和dfs.data.dir这两个参数mapred.local.dir和dfs.data.dir 配置的值应当是分布在各个磁盘上目录，这样可以充分利用节点的IO读写能力。运行 Linux sysstat包下的iostat -dx 5命令可以让每个磁盘都显示它的利用率。3.2.4 map/reduce 相关配置（1） {map/reduce}.tasks.maximum同时运行在TaskTracker上的最大map/reduce task数，一般设为(core_per_node)/2~2*（cores_per_node）。（2） io.sort.factor当一个map task执行完之后，本地磁盘上(mapred.local.dir)有若干个spill文件，map task最后做的一件事就是执行merge sort，把这些spill文件合成一个文件（partition）。执行merge sort的时候，每次同时打开多少个spill文件由该参数决定。打开的文件越多，不一定merge sort就越快，所以要根据数据情况适当的调整。（3） mapred.child.java.opt设置JVM堆的最大可用内存，需从应用程序角度进行配置。 三 map task相关配置（1） io.sort.mMap task的输出结果和元数据在内存中所占的buffer总大小。默认为100M，对于大集群，可设为200M。当buffer达到一定阈值，会启动一个后台线程来对buffer的内容进行排序，然后写入本地磁盘(一个spill文件)。（2） io.sort.spill.percent这个值就是上述buffer的阈值，默认是0.8，即80%，当buffer中的数据达到这个阈值，后台线程会起来对buffer中已有的数据进行排序，然后写入磁盘。（3） io.sort.recordIo.sort.mb中分配给元数据的内存百分比，默认是0.05。这个需要根据应用程序进行调整。（4） mapred.compress.map.output/ Mapred.output.compre中间结果和最终结果是否要进行压缩，如果是，指定压缩方式（Mapred.compress.map.output.codec/ Mapred.output.compress.codec）。推荐使用LZO压缩。Intel内部测试表明，相比未压缩，使用LZO压缩的TeraSort作业运行时间减少60%，且明显快于Zlib压缩。3.2.6 reduce task相关配置（1） Mapred.reduce.parallelReduce shuffle阶段copier线程数。默认是5，对于较大集群，可调整为16~25。 四 通过hadoop的参数进行调优(1):设置合理的槽位数目(具体配置 mapred.tasktracker.map.tasks.maximum | mapred.tasktracker.reduce.tasks.maximum | mapreduce.tasktracker.map.tasks.maximum | mapreduce.tasktracker.reduce.tasks.maximum)(2):调整心跳间隔,对于300台以下的集群 可以把心跳设置成300毫秒(默认是3秒),mapreduce.jobtracker.hearbeat.interval.min | mapred.hearbeats.in.second | mapreduce.jobtracker.heartbeats.scaling.factor(3):启用外心跳,为了减少任务分配延迟(比如我们的任务心跳设置为10秒钟,当有一个任务挂掉了之后,他就不能马上通知jobtracker),所以hadoop引入了外心跳,外心跳是任务运行结束或者任务运行失败的时候触发的,能够在出现空闲资源时第一时间通知jobtracker,以便他能够迅速为空闲资源分配新的任务外心跳的配置参数是 mapreduce.tasktracker.outofband.hearbeat (4):磁盘快的配置. map task会把中间结果放到本地磁盘中,所以对于I/O密集的任务来说这部分数据会对本地磁盘造成很大的压力,我们可以配置多块可用磁盘,hadoop将采用轮训的方式将不同的maptask的中间结果写到磁盘上 maptask中间结果的配置参数是mapred.local.dir | mapreduce.cluster.local.dir (5):配置RPC Handler的数量,jobracker需要冰法处理来自各个tasktracker的RPC请求,我们可以根据集群规模和服务器并发处理的情况调整RPC Handler的数目,以使jobtracker的服务能力最佳 配置参数是 mapred.job.tracker.handler.count | mapreduce.jobtracker.handler.count (默认是10) (6):配置HTTP线程数. 在shuffle阶段,reduce task 通过http请求从各个tasktracker上读取map task中间结果,而每个tasktracker通过jetty server处理这些http请求,所以可以适当配置调整jetty server的工作线程数 配置参数是 tasktracker.http.thread | mapreduce.tasktracker.http.threads (默认是40) (7):如果我们在运行作业的过程中发现某些机器被频繁地添加到黑名单里面,我们可以把此功能关闭 (8):使用合理调度器 (9):使用合适的压缩算法,在hadoop里面支持的压缩格式是: gzip,zip,bzip2,LZO,Snappy,LZO和Snappy的呀搜比和压缩效率都很优秀,Snappy是谷歌的开源数据压缩哭,他已经内置在hadoop1.0之后的版本,LZO得自己去编译 (10):开启预读机制. 预读机制可以有效提高磁盘I/O的读性能,目前标准版的apache hadoop不支持此功能,但是在cdh中是支持的 配置参数是: mapred.tasktracker.shuffle.fadvise=true (是否启用shuffle预读取机制) mapred.tasktracker.shuffle.readahead.bytes=4MB (shuffle预读取缓冲区大小) mapreduce.ifile.readahead = true (是否启用ifile预读取机制) mapreduce.ifile.readahead.bytes = 4MB (IFile预读取缓冲区大小) (11):启用推测执行机制 (12):map task调优: 合理调整io.sort.record.percent值,可减少中间文件数据,提高任务执行效率. (map task的输出结果将被暂时存放到一个环形缓冲区中,这个缓冲区的大小由参数”io.sort.mb”指定,单位MB,默认是100MB, 该缓冲区主要由两部分组成,索引和实际数据,默认情况下,索引占整个buffer的比例为io.sort.record.percent,默认是5%, 剩余空间存放数据,仅当满足以下任意一个条件时才会触发一次flush,生成一个临时文件,索引或者数据空间使用率达到比例为 io.sort.spill.percent的80%) 所以具体调优参数如下: io.sort.mb | io.sort.record.percent | io.sort.spill.percent (13):reduce task调优 reduce task会启动多个拷贝线程从每个map task上读取相应的中间结果,参数是”mapred.reduce.parallel.copies”(默认是5) 原理是这样的–&gt;对于每个待拷贝的文件,如果文件小于一定的阀值A,则将其放入到内存中,否则已文件的形式存放到磁盘上, 如果内存中文件满足一定条件D,则会将这些数据写入磁盘中,而当磁盘上文件数目达到io.sort.factor(默认是10)时, 所以如果中间结果非常大,可以适当地调节这个参数的值 (14):跳过坏记录 看具体参数说明,=号后面是默认值 mapred.skip.attempts.to.start.skipping=2 当任务失败次数达到该值时,才会进入到skip mode,即启用跳过坏记录gongnneg mapred.skip.map.max,skip.records=0 用户可通过该参数设置最多运行跳过的记录数目 mapred.skip.reduce.max.skip.groups=0 用户可通过设置该参数设置Reduce Task最多允许跳过的记录数目 mapred.skip.out.dir =${mapred.output.dir}/logs/ 检测出得坏记录存放到目录里面(一般为HDFS路径),hadoop将坏记录保存起来以便于用户调试和跟踪 (15):使用JVM重用 : mapred.job.reuse.jvm.aum.tasks | mapreduce.job.jvm.num.tasks = -1]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka伪分布安装]]></title>
    <url>%2F2017%2F05%2F20%2Fkafka%E4%BC%AA%E5%88%86%E5%B8%83%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1.版本centos6.5 hadoop2.8 zookeeper2.14 kafka2.10 2.下载kafka1https://kafka.apache.org 3.解压12tar -zxvf kafka*****mv kafka***** kafka 4.修改配置文件在config中1vi server.properties 1234567891011# The id of the broker. This must be set to a unique integer for each broker.broker.id=0-----------listeners=PLAINTEXT://:9092-----------# A comma seperated list of directories under which to store log fileslog.dirs=/tmp/server0/kafka-logs-----------# root directory for all kafka znodes.zookeeper.connect=localhost:2181----------- 修改以上几个主要配置文件 5.将server.properties重命名为server0.properties1mv server.properties server0.properties 6.复制server0.properties12cp server0.properties server1.propertiescp server0.properties server2.properties 7.修改server1.properties1234567# The id of the broker. This must be set to a unique integer for each broker.broker.id=1-----------listeners=PLAINTEXT://:9093-----------# A comma seperated list of directories under which to store log fileslog.dirs=/tmp/server1/kafka-logs 8.修改server2.properties1234567# The id of the broker. This must be set to a unique integer for each broker.broker.id=2-----------listeners=PLAINTEXT://:9094-----------# A comma seperated list of directories under which to store log fileslog.dirs=/tmp/server4/kafka-logs 9.在/tmp中创建新目录server0、server1、server2123mkdir /tmp/server0mkdir /tmp/server1mkdir /tmp/server2 10.启动kafka123bin/kafka-server-start config/server0.propertiesbin/kafka-server-start config/server1.propertiesbin/kafka-server-start config/server2.properties 11.创建topic1bin/kafka-topics.sh --create --zookeeper 192.168.174.175:2181 --replication-factor 1 --partitions 3 --topic test 12.查看所有主题1bin/kafka-topics.sh --list --zookeeper 192.168.174.175:2181 13.模拟客户端发送，接受消息123发送消息bin/kafka-console-producer.sh --topic topic_1 --broker-list 192.168.174.175:9092,192.168.174.175:9093,192.168.174.175:9094 123接收消息bin/kafka-console-consumer.sh --topic topic_1 --zookeeper 192.168.174.175:2181 --from-beginning 14.kill some broker1kill broker(id=0) 首先，我们根据前面的配置，得到broker(id=0)应该在9092监听,这样就能确定它的PID了。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java读取Excel表格中的数据]]></title>
    <url>%2F2017%2F05%2F20%2Fjava%E8%AF%BB%E5%8F%96Excel%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1.需求用java代码读取Excel.xls表格中的数据 2.java代码12345678910111213141516171819202122232425262728293031323334353637383940package com.test;import java.io.File;import jxl.*;public class ReadExcel&#123; public static void main(String[] args)&#123; ing i; Sheet sheet; Workbook book; Cell cell1,cell2,cell3,cell4,cell5,cell6,cell7; try&#123; //Excel.xls为要读取的excel文件 book = Workbook.getWorkbook(new File(&quot;文件物理地址&quot;)))； //获取第一个工作表对象（excel中sheet的编号从0开始，0,1,2,3，.....） sheet = book.getSheet(0); //获取左上角的单元格 cell1 = sheet.getCell(0,0);(行，列) System.out.println(&quot;标题：&quot;+cell1.getContents()); i = 1; while(true)&#123; //获取每一行单元格 cell1 = sheet.getCell(0,i); cell2 = sheet.getCell(1,i); cell3 = sheet.getCell(2,i); cell4 = sheet.getCell(3,i); cell5 = sheet.getCell(4,i); cell6 = sheet.getCell(5,i); cell7 = sheet.getCell(6,i); if(&quot;&quot;.equals(cell1.getContents()) == true) //如果读取的数据为空 break; System.out.println(cell1.getContents()+&quot;\t&quot;+cell2.getContents()+&quot;\t&quot;+cell3.getContents()+&quot;\t&quot;+cell4.getContents()+&quot;\t&quot;+cell5.getContents()+&quot;\t&quot;+cell6.getContents()+&quot;\t&quot;); i++; &#125; book.close(); &#125; catch(Exceprion e)&#123; &#125; &#125;&#125;]]></content>
      <categories>
        <category>study-Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase-写入方式]]></title>
    <url>%2F2017%2F05%2F08%2FHbase-5%E7%A7%8D%E5%86%99%E5%85%A5%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑HBase写入数据方式（参考：《HBase The Definitive Guide》） 1.直接使用HTable进行导入，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package hbase.curd;import java.io.IOException;import java.util.ArrayList;import java.util.List;impirt java.util.Random;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.util.Bytes;public class PutExample&#123; private HTable table = HTableUtil.getHTable(&quot;testtable&quot;); public static void main(String args[]) throws IOException&#123; PutExample pe = new PutExample(); pe.putRows(); &#125; public void putRows()&#123; List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;; for(int i=0; i&lt;10; i++)&#123; Put put = new Put(Bytes.toBytes(&quot;row_&quot;+i)); Rowdom random = new Random(); if(random.nextBoolean())&#123; put.add(Bytes.toBytes(&quot;colfam1&quot;), Bytes.toBytes(&quot;qual1&quot;), Bytes.toBytes(&quot;colfam1_qual1_value_&quot;+i)); &#125; if(random.nextBoolean())&#123; put.add(Bytes.toBytes(&quot;colfam1&quot;), Bytes.toBytes(&quot;qual2&quot;), Bytes.toBytes(&quot;colfam1_qual1_value_&quot;+i)); &#125; if(random.nextBoolean())&#123; put.add(Bytes.toBytes(&quot;colfam1&quot;), Bytes.toBytes(&quot;qual3&quot;), Bytes.toBytes(&quot;colfam1_qual1_value_&quot;+i)); &#125; if(random.nextBoolean())&#123; put.add(Bytes.toBytes(&quot;colfam1&quot;), Bytes.toBytes(&quot;qual4&quot;), Bytes.toBytes(&quot;colfam1_qual1_value_&quot;+i)); &#125; if(random.nextBoolean())&#123; put.add(Bytes.toBytes(&quot;colfam1&quot;), Bytes.toBytes(&quot;qual5&quot;), Bytes.toBytes(&quot;colfam1_qual1_value_&quot;+i)); &#125; puts.add(put); &#125; try&#123; table.put(puts); table.close(); &#125;catch(Exception e)&#123; e.printStackTrace(); return ; &#125; System.out.println(&quot;done put rows&quot;); &#125; &#125;&#125; 其中HTableUtil如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344package hbase.curd;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.util.Bytes;public class HTableUtil &#123; private static HTable table; private static Configuration conf; static&#123; conf =HBaseConfiguration.create(); conf.set(&quot;mapred.job.tracker&quot;, &quot;hbase:9001&quot;); conf.set(&quot;fs.default.name&quot;, &quot;hbase:9000&quot;); conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hbase&quot;); try &#123; table = new HTable(conf,&quot;testtable&quot;); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static Configuration getConf()&#123; return conf; &#125; public static HTable getHTable(String tablename)&#123; if(table==null)&#123; try &#123; table= new HTable(conf,tablename); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; return table; &#125; public static byte[] gB(String name)&#123; return Bytes.toBytes(name); &#125;&#125;]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础——String&StringBuffer]]></title>
    <url>%2F2017%2F03%2F14%2Fjava%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94String%26StringBuffer%2F</url>
    <content type="text"><![CDATA[创建字符串创建字符串最简单的方式如下:String greeting = “王小二客栈”;注意:String 类是不可改变的，所以你一旦创建了 String 对象，那它的值就无法改变了。如果需要对字符串做很多修改，那么应该选择使用 StringBuffer &amp; StringBuilder 类。 字符串长度用于获取有关对象的信息的方法称为访问器方法。String 类的一个访问器方法是 length() 方法，它返回字符串对象包含的字符数。 连接字符串String 类提供了连接两个字符串的方法：string1.concat(string2);更常用的是使用’+’操作符来连接字符串，如：“Hello,” + “ xiaoer” + “!” String 方法 char charAt(int index) 返回指定索引处的char值。 int compareTo（Object o） 把这个字符串和另一个对象比较。 int compareTo(String anotherString) 按字典顺序比较两个字符串。 int compareToIgnoreCase(String str) 按字典顺序比较两个字符串，不考虑大小写。 String concat(String str) 将指定字符串连接到此字符串的结尾。 boolean contentEquals(StringBuffer sb) 当且仅当字符串与指定的StringButter有相同顺序的字符时候返回真。 static String copyValueOf(char[] data) 返回指定数组中表示该字符序列的 String。 static String copyValueOf(char[] data, int offset, int count) 返回指定数组中表示该字符序列的 String。 boolean endsWith(String suffix) 测试此字符串是否以指定的后缀结束。 boolean equals(Object anObject) 将此字符串与指定的对象比较。 boolean equalsIgnoreCase(String anotherString) 将此 String 与另一个 String 比较，不考虑大小写。 byte[] getBytes() 使用平台的默认字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中。 byte[] getBytes(String charsetName) 使用指定的字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中。 void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin) 将字符从此字符串复制到目标字符数组。 int hashCode() 返回此字符串的哈希码。 int indexOf(int ch) 返回指定字符在此字符串中第一次出现处的索引。 int indexOf(int ch, int fromIndex) 返回在此字符串中第一次出现指定字符处的索引，从指定的索引开始搜索。 int indexOf(String str) 返回指定子字符串在此字符串中第一次出现处的索引。 int indexOf(String str, int fromIndex) 返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始。 String intern() 返回字符串对象的规范化表示形式。 int lastIndexOf(int ch) 返回指定字符在此字符串中最后一次出现处的索引。 int lastIndexOf(int ch, int fromIndex) 返回指定字符在此字符串中最后一次出现处的索引，从指定的索引处开始进行反向搜索。 int lastIndexOf(String str) 返回指定子字符串在此字符串中最右边出现处的索引。 int lastIndexOf(String str, int fromIndex) 返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索。 int length() 返回此字符串的长度。 boolean matches(String regex) 告知此字符串是否匹配给定的正则表达式。 boolean regionMatches(boolean ignoreCase, int toffset, String other, int ooffset, int len) 测试两个字符串区域是否相等。 boolean regionMatches(int toffset, String other, int ooffset, int len) 测试两个字符串区域是否相等。 String replace(char oldChar, char newChar) 返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的。 String replaceAll(String regex, String replacement) 使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串。 String[] split(String regex) 根据给定正则表达式的匹配拆分此字符串。 String[] split(String regex, int limit) 根据匹配给定的正则表达式来拆分此字符串。 boolean startsWith(String prefix) 测试此字符串是否以指定的前缀开始。 boolean startsWith(String prefix, int toffset) 测试此字符串从指定索引开始的子字符串是否以指定前缀开始。 CharSequence subSequence(int beginIndex, int endIndex) 返回一个新的字符序列，它是此序列的一个子序列。 String substring(int beginIndex) 返回一个新的字符串，它是此字符串的一个子字符串。 String substring(int beginIndex, int endIndex) 返回一个新字符串，它是此字符串的一个子字符串。 char[] toCharArray() 将此字符串转换为一个新的字符数组。 String toLowerCase() 使用默认语言环境的规则将此 String 中的所有字符都转换为小写。 String toLowerCase(Locale locale) 使用给定 Locale 的规则将此 String 中的所有字符都转换为小写。 String toString() 返回此对象本身（它已经是一个字符串！）。 String toUpperCase() 使用默认语言环境的规则将此 String 中的所有字符都转换为大写。 String toUpperCase(Locale locale) 使用给定 Locale 的规则将此 String 中的所有字符都转换为大写。 String trim() 返回字符串的副本，忽略前导空白和尾部空白。 static String valueOf(primitive data type x) 返回给定data type类型x参数的字符串表示形式。]]></content>
      <categories>
        <category>study-Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[王小二客栈——Java Number&Math]]></title>
    <url>%2F2017%2F03%2F13%2Fjava%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94Number%26Math%2F</url>
    <content type="text"><![CDATA[忆往昔峥嵘岁月，不堪入目 1.所有的包装类（Integer、Long、Byte、Double、Float、Short）都是抽象类Number的子类。 2.Java的Math包含了用于执行基本数学运算的属性和方法，如初等指数、对数、平方根和三角函数等。 Math的方法都被定义为static形式，通过Math类可以在主函数中直接调用。 下面列出Number &amp; Math类常用的一些方法： xxxValue() 将Number对象装换成xxx数据类型的值并返回。 compareTo（） 将number对象与参数比较。 equals() 判断number对象是否与参数相等。 valueOf() 返回一个Number对象制定的内置数据类型。 toString() 以字符串形式返回值。 parseInt() 将字符串解析为int类型。 abs() 返回参数的绝对值。 ceil（） 返回大于等于（&gt;=）给定参数的最小整数。 floor() 返回小雨等于（&lt;=）给定参数的最大整数。 rint（） 返回与参数最接近的整数，返回类型为double。 round() 他表示四舍五入。 min() 返回两个参数之间的最小值。 max() 返回两个参数之间的最大值。 exp（） 返回自然数底数e的参数次方。 log() 返回参数的自然数底数的对数值。 random（) 返回一个随机数。 Java Character 类 Character 类用于对单个字符进行操作。 Character 类在对象中包装一个基本类型 char 的值。转义序列转义序列 描述\t 在文中该处插入一个tab键\b 在文中该处插入一个后退键\n 在文中该处换行\r 在文中该处插入回车\f 在文中该处插入换页符\’ 在文中该处插入单引号\” 在文中该处插入双引号\ 在文中该处插入反斜杠 Character 方法isLetter() 是否是一个字母isDigit() 是否是一个数字字符isWhitespace() 是否为一个空格isUpperCase() 是否是大写字母isLowCase() 是否是小写字母toUpperCase() 制定之母的大写形式toLowCase() 制定之母的小写形式toString() 返回字符的字符串形式，字符串的长短仅为1。]]></content>
      <categories>
        <category>study-Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
</search>
