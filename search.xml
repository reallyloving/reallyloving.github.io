<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[nginx安装]]></title>
    <url>%2F2018%2F05%2F11%2Fnginx%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[** Nginx(“engine x”)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。 ** 在高连接并发的情况下，Nginx是Apache服务器不错的替代品。 Nginx 安装系统平台：CentOS release 6.6 (Final) 64位。 一、安装编译工具及库文件yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel 二、首先要安装 PCREPCRE 作用是让 Nginx 支持 Rewrite 功能。 1、下载 PCRE 安装包，下载地址： http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz [root@bogon src]# wget http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz 2、解压安装包: [root@bogon src]# tar zxvf pcre-8.35.tar.gz3、进入安装包目录 [root@bogon src]# cd pcre-8.354、编译安装 [root@bogon pcre-8.35]# ./configure[root@bogon pcre-8.35]# make &amp;&amp; make install5、查看pcre版本 [root@bogon pcre-8.35]# pcre-config –version 安装 Nginx1、下载 Nginx，下载地址：http://nginx.org/download/nginx-1.6.2.tar.gz [root@bogon src]# wget http://nginx.org/download/nginx-1.6.2.tar.gz2、解压安装包 [root@bogon src]# tar zxvf nginx-1.6.2.tar.gz3、进入安装包目录 [root@bogon src]# cd nginx-1.6.24、编译安装 [root@bogon nginx-1.6.2]# ./configure –prefix=/usr/local/webserver/nginx –with-http_stub_status_module –with-http_ssl_module –with-pcre=/usr/local/src/pcre-8.35[root@bogon nginx-1.6.2]# make[root@bogon nginx-1.6.2]# make install5、查看nginx版本 [root@bogon nginx-1.6.2]# /usr/local/webserver/nginx/sbin/nginx -v ** 到此，nginx安装完成。 Nginx 配置创建 Nginx 运行使用的用户 www： [root@bogon conf]# /usr/sbin/groupadd www[root@bogon conf]# /usr/sbin/useradd -g www www配置nginx.conf ，将/usr/local/webserver/nginx/conf/nginx.conf替换为以下内容 [root@bogon conf]# cat /usr/local/webserver/nginx/conf/nginx.conf user www www;worker_processes 2; #设置值和CPU核心数一致error_log /usr/local/webserver/nginx/logs/nginx_error.log crit; #日志位置和日志级别pid /usr/local/webserver/nginx/nginx.pid; #Specifies the value for maximum file descriptors that can be opened by this process.worker_rlimit_nofile 65535;events{ use epoll; worker_connections 65535;}http{ include mime.types; default_type application/octet-stream; log_format main ‘$remote_addr - $remote_user [$time_local] “$request” ‘ ‘$status $body_bytes_sent “$http_referer” ‘ ‘“$http_user_agent” $http_x_forwarded_for’; #charset gb2312; server_names_hash_bucket_size 128; client_header_buffer_size 32k; large_client_header_buffers 4 32k; client_max_body_size 8m; sendfile on; tcp_nopush on; keepalive_timeout 60; tcp_nodelay on; fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 2; gzip_types text/plain application/x-javascript text/css application/xml; gzip_vary on; #limit_zone crawler $binary_remote_addr 10m; #下面是server虚拟主机的配置 server { listen 80;#监听端口 server_name localhost;#域名 index index.html index.htm index.php; root /usr/local/webserver/nginx/html;#站点目录 location ~ .*.(php|php5)?$ { #fastcgi_pass unix:/tmp/php-cgi.sock; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; } location ~ .*\.(gif|jpg|jpeg|png|bmp|swf|ico)$ { expires 30d; #access_log off; } location ~ .*.(js|css)?$ { expires 15d; #access_log off; } access_log off; } }检查配置文件ngnix.conf的正确性命令： [root@bogon conf]# /usr/local/webserver/nginx/sbin/nginx -t 启动 NginxNginx 启动命令如下： [root@bogon conf]# /usr/local/webserver/nginx/sbin/nginx 访问站点从浏览器访问我们配置的站点ip： ** Nginx 其他命令以下包含了 Nginx 常用的几个命令： /usr/local/webserver/nginx/sbin/nginx -s reload # 重新载入配置文件/usr/local/webserver/nginx/sbin/nginx -s reopen # 重启 Nginx/usr/local/webserver/nginx/sbin/nginx -s stop # 停止 Nginx]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop架构——2.8.3]]></title>
    <url>%2F2018%2F05%2F04%2FHadoop%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑]]></content>
  </entry>
  <entry>
    <title><![CDATA[王小二客栈]]></title>
    <url>%2F2018%2F02%2F17%2Fmyblog-test%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑]]></content>
  </entry>
  <entry>
    <title><![CDATA[zookeeper搭建]]></title>
    <url>%2F2018%2F02%2F15%2Fzookeeper%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[本文选用：centos6.5、zookeeper-3.4.6集群部署【软件】准备好jdk环境，此次我们的环境是open_jdk1.8.0_101 zookeeper-3.4.6.tar.gz【步骤】 准备条件如果有内部dns或者外网有域名，则直接使用域名如果没有需要修改/etc/hosts文件，或者直接使用IP 集群规划 主机类型 IP地址 域名zookeeper1 192.168.1.1zookeeper1.chinasoft.comzookeeper2 192.168.1.2zookeeper2.chinasoft.comzookeeper3 192.168.1.3zookeeper3.chinasoft.com 注意：zookeeper因为有主节点和从节点的关系，所以部署的集群台数最好为奇数个，否则可能出现脑裂导致服务异常 安装下载地址：http://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/解压 tar -zxf zookeeper-3.4.6.tar.gzcd zookeeper-3.4.6 拷贝配置文件，修改完成后分发给其他节点cd /data/zookeeper-3.4.6/cp zoo_sample.cfg zoo.cfg cat zoo.cfg tickTime=2000initLimit=10syncLimit=5dataDir=/data/zookeeper-3.4.6/datadataLogDir=/data/zookeeper-3.4.6/logsclientPort=2181server.1=u04rtv01.yaya.corp:2888:3888server.2=u04rtv02.yaya.corp:2888:3888server.3=u04rtv03.yaya.corp:2888:3888 3.创建data和Log文件夹mkdir /data/zookeeper-3.4.6/datamkdir /data/zookeeper-3.4.6/logs 4、在zoo.cfg中的dataDir指定的目录下，新建myid文件。例如：$ZK_INSTALL/data下，新建myid。在myid文件中输入1。表示为server.1。如果为snapshot/d_2，则myid文件中的内容为 2，依此类推。 启动：在集群中的每台主机上执行如下命令bin/zkServer.sh start 查看状态，可以看到其中一台为主节点，其他两台为从节点：bin/zkServer.sh status 主节点：./zkServer.sh statusJMX enabled by defaultUsing config: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: leader 从属节点：./zkServer.sh statusJMX enabled by defaultUsing config: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: follower 停止：bin/zkServer.sh stop 连接：bin/zkCli.sh -server zookeeper1:2181bin/zkCli.sh -server zookeeper2:2181bin/zkCli.sh -server zookeeper3:2181 报错：原因就是没有在dataDir目录下创建myid文件并且赋值(如1、2、3分别代表集群中的server1,server2,server3) 2016-08-22 17:55:16,145 [myid:] - INFO [main:QuorumPeerConfig@103] - Reading configuration from: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfg2016-08-22 17:55:16,150 [myid:] - INFO [main:QuorumPeerConfig@340] - Defaulting to majority quorums2016-08-22 17:55:16,150 [myid:] - ERROR [main:QuorumPeerMain@85] - Invalid config, exiting abnormallyorg.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfg at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:123) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:101) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)Caused by: java.lang.IllegalArgumentException: /data/yunva/zookeeper-3.4.6/data/myid file is missing at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parseProperties(QuorumPeerConfig.java:350) at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:119) … 2 moreInvalid config, exiting abnormally 单机部署——适用于开发测试tar -zxvf zookeeper-3.4.6.tar.gzcd zookeeper-3.4.6/confcp zoo_sample.cfg zoo.cfg 创建日志目录mkdir /data/yunva/zookeeper-3.4.6/datamkdir /data/yunva/zookeeper-3.4.6/logs 配置：conf/zoo.cfg tickTime=2000initLimit=10syncLimit=5dataDir=/data/yunva/zookeeper-3.4.6/logsdataLogDir=/data/yunva/zookeeper-3.4.6/logsclientPort=2181 #自动清除日志文件autopurge.snapRetainCount=20autopurge.purgeInterval=48 启动： bin/zkServer.sh start 连接到Zookeeper： bin/zkCli.sh -server 127.0.0.1:2181 适用于Java开发 查看状态：bin/zkServer.sh statusJMX enabled by defaultUsing config: /data/yunva/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: standalone]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeepere</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos下sqoop环境搭建]]></title>
    <url>%2F2018%2F02%2F15%2Fcentos%E4%B8%8Bsqoop%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Sqoop是一个用来将Hadoop（Hive、HBase）和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库（例如：MySQL ,Oracle ,Postgres等）中的数据导入到Hadoop的HDFS中，也可以将HDFS的数据导入到关系型数据库中。 Sqoop安装 1.下载Sqoop安装包在Sqoop官网下载安装包，本次使用的是sqoop-1.4.6.binhadoop-2.0.4-alpha.tar.gz安装在/usr/local目录下，下载地址为http://apache.fayea.com/sqoop/1.4.6/sqoop-1.4.6.binhadoop-2.0.4-alpha.tar.gz 2.解压Sqoop安装包#进入sqoop安装目录[hadoop@BigData ~]$ cd /usr/local#解压sqoop安装包[hadoop@BigData ~]$ tar -zxvf sqoop-1.4.6.binhadoop-2.0.4-alpha.tar.gz#删除sqoop安装包[hadoop@BigData ~]$ rm -rf sqoop-1.4.6.binhadoop-2.0.4-alpha.tar.gz#重命名sqoop目录名[hadoop@BigData ~]$ mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop 3.配置Sqoop环境变量#配置Sqoop环境变量[root@BigData ~]# vi /etc/profileexport SQOOP_HOME=/usr/local/sqoopexport PATH=$PATH:$SQOOP_HOME/bin#保存之后记得source，使之前的配置生效source /etc/profile 4.将关系型数据库驱动包放到sqoop/lib目录下MySql：mysql-connector-java-5.1.30.jarOracle：ojdbc14.jar 5.修改Sqoop配置文件[hadoop@BigData ~]$ mv sqoop-env-template.sh sqoop-env.sh[hadoop@BigData ~]$ vi sqoop-env.sh#Set path to where bin/hadoop is availableexport HADOOP_COMMON_HOME=/usr/local/hadoop#Set path to where hadoop-*-core.jar is availableexport HADOOP_MAPRED_HOME=/usr/local/hadoop#set the path to where bin/hbase is availableexport HBASE_HOME=/usr/local/hbase#Set the path to where bin/hive is availableexport HIVE_HOME=/usr/local/hive#Set the path for where zookeper config dir isexport ZOOCFGDIR=/usr/local/zookeeper到此，sqoop环境就已搭建成功！]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker安装]]></title>
    <url>%2F2018%2F02%2F05%2Fdocker%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1：关闭selinux临时关闭：setenforce 0永久关闭： 1. vi /etc/sysconfig/selinux插入/编辑以下内容SELINUX=disabled #重启生效 2：在Fedora EPEL源中已经提供了docker-io包，下载安装epel：rpm -ivh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpmsed -i ‘s/^mirrorlist=https/mirrorlist=http/‘ /etc/yum.repos.d/epel.repo（elpe.repo）[epel]name=epelmirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=epel-$releasever&amp;arch=$basearchenabled=1gpgcheck=0 3：安装dockeryum install docker-io安装完成后 4：启动dockerservice docker start 5：查看docker版本docker vesion 6：查看docker日志cat /var/log/docker docker安装完成 一：卸载docker列出你安装过的包[root@localhost ~]# yum list installed | grep dockerdocker-io.x86_64 1.7.1-2.el6 @epel删除软件包yum -y remove docker-io.x86_64删除镜像/容器等rm -rf /var/lib/docker 二：升级docker版本为1.10.3升级之前停止docker服务,并将原有的docker服务进行备份. mv /usr/bin/docker /usr/bin/docker.bak nohup wget -c https://get.docker.com/builds/Linux/x86_64/docker-1.10.3 -O /usr/bin/docker给执行权限：chmod 755 /usr/bin/docker 然后重启服务，并查看版本. 报错：Starting cgconfig service: Error: cannot mount memory to /cgroup/memory: No such file or directory/sbin/cgconfigparser; error loading /etc/cgconfig.conf: Cgroup mounting failedFailed to parse /etc/cgconfig.conf [FAILED]Starting docker: [ OK ] 修改：/etc/cgconfig.conf文件mount { cpuset = /cgroup/cpuset; cpu = /cgroup/cpu; cpuacct = /cgroup/cpuacct; memory = /cgroup/memory;devices = /cgroup/devices; freezer = /cgroup/freezer; net_cls = /cgroup/net_cls; blkio = /cgroup/blkio; }重新启动docker 正常]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7伪分布式安装Hadoop2.6和Hbase0.94]]></title>
    <url>%2F2017%2F12%2F01%2Fcentos7%2Bhadoop2.6%2Bhbase1.0.x%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑一、安装Jdk：首先需要卸载系统自带的openjava，查看系统的Java： rpm -qa|grep java。 卸载： yum -y remove java javaxxxxx(系统自带的Java版本) 安装jdk，将jdk.tar.gz文件复制到/usr/java中,终端进入/mnt/share ,cp jdk.tar.gz /usr/ava，进入/usr/java解压：tar xzvf jdk.targz 配置环境变量：vi /etc/profile 输入i编辑在尾部添加：export JAVA_HOME=/usr/java/jdkxxxxexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 保存并退出： wq使修改生效： source /etc/profile查看Java版本：java -version 二、Hadoop伪分布式安装 1、ssh无密码登陆 终端：ssh-keygen -t rsa (获得rsa公钥私钥,id_rsa和id_rsa.pub)cd .sshcp id_rsa.pub authorized_keys (将公钥复制给authorized_keys) &lt;分布式则要将所有节点id_rsa.pub相互复制&gt; 2、 /mnt/share cp hadoop2.x /usr.hadoop 解压tar xzvf hadoop 2.x 3、修改core-site.xml、hadoop-env.sh、hdfs-site.xml、mapred-site.xml 、yarn-site.xml(hadoop2.x版本的配置文件在/hadoop2.x/etc/hadoop下) ①core-site.xml： fs.default.name hdfs://localhost:9000 ② hadoop-env.sh：export JAVA_HOME=/usr/java/jdkxxx (jdk路径) ③ hdfs-site.xml： 先创建好数据节点和名称节点的存放路径 dfs.datanode.data.dir /user/hadoop/hadoop-2.5.1/data dfs.namenode.name.dir /user/hadoop/hadoop-2.5.1/name dfs.replication 1 ④mapred-site.xml: (注意：这个文件是将/hadoop2.x/etc/hadoop下的mapred-site.xml.template复制并重命名 ) mapreduce.framework.name yarn ⑤yarn-site.xml： yarn.nodemanager.aux-services mapreduce_shuffle 4、namenode格式化（一定要完成） 终端：cd /usr/hadoop/hadoop-2.5.1/bin ./hdfs namenode -format (输入./hadoop namenode -format也行) 5、运行hadoop 终端： cd /usr/hadoop/hadoop-2.5.1/sbin (2.x版本后启动/停止在sbin目录下)./start-hdfs.sh./start-yarn.sh(也可以只输入./start-all.sh) 输入jps查看启动项，当启动了NameNode、DataNode、SecondaryNameNode、ResourceManager、NodeManager即ok。 可进入Firefox中，输入端口号： localhost:50070 进入hadoop可视化页面。 三、Hbase0.94安装 1、/mnt/share cp hbase1.0.1 /usr.hbase 解压tar xzvf hbase1.0.1 2、修改hbase配置文件hbase-env.sh、hbase-site.xml hbase-env.sh: export JAVA_HOME=/usr/java/jdkxxxx (java路径)export HBASE_MANAGES_ZK=true (都得去掉前面#) hbase-site.xml： hbase.rootdir hdfs://localhost:9000/hbase hbase.cluster.distributed true hbase.zookeeper.quorum localhost hbase.tmp.dir file:/usr/hbase/tmp hbase.zookeeper.property.dataDir file:/usr/hbase/zookeeperdata 3、运行hbase 运行前需先启动hadoop，再进入hbase的bin目录下输入指令 ./start-hbase.sh输入jps查看启动项，如有HMaster、HRegionServer、HQuormPeer,则说明hbase启动成功。输入./hbase Shell (进入shell指令，可通过shell指令建表)]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos 6.5 内核升级]]></title>
    <url>%2F2017%2F11%2F15%2Fcentos6.5%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[1.查看centos的内核版本rname -r 2.查看系统版本cat /etc/centos-release 3.安装软件编译安装新内核，依赖于开发环境和开发库yum grouplist //查看已经安装的和未安装的软件包组，来判断我们是否安装了相应的开发环境和开发库； yum groupinstall “Development Tools” //一般是安装这两个软件包组，这样做会确定你拥有编译时所需的一切工具 yum install ncurses-devel //你必须这样才能让 make *config 这个指令正确地执行 yum install qt-devel //如果你没有 X 环境，这一条可以不用 yum install hmaccalc zlib-devel binutils-devel elfutils-libelf-devel //创建 CentOS-6 内核时需要它们 4.编译内核Linux内核版本有两种：稳定版和开发版 ，Linux内核版本号由3个数字组成： r.x.y r: 主版本号 x: 次版本号，偶数表示稳定版本；奇数表示开发中版本。 y: 修订版本号 ， 表示修改的次数官网上有stable, longterm等版本，longterm是比stable更稳定的版本。[root@sean ~]#curl -O -L https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.10.28.tar.xz [root@sean ~]# tar -xf linux-3.10.58.tar.xz -C /usr/src/ [root@sean ~]# cd /usr/src/linux-3.10.58/ [root@sean linux-3.10.58]# cp /boot/config-2.6.32-220.el6.x86_64 .config 我们在系统原有的内核配置文件的基础上建立新的编译选项，所以复制一份到当前目录下，命名为.config。接下来继续配置：`[root@sean linux-3.10.58]# sh -c ‘yes “” | make oldconfig’ HOSTCC scripts/basic/fixdep HOSTCC scripts/kconfig/conf.o SHIPPED scripts/kconfig/zconf.tab.c SHIPPED scripts/kconfig/zconf.lex.c SHIPPED scripts/kconfig/zconf.hash.c HOSTCC scripts/kconfig/zconf.tab.o HOSTLD scripts/kconfig/conf scripts/kconfig/conf –oldconfig Kconfig .config:555:warning: symbol value ‘m’ invalid for PCCARD_NONSTATIC.config:2567:warning: symbol value ‘m’ invalid for MFD_WM8400.config:2568:warning: symbol value ‘m’ invalid for MFD_WM831X.config:2569:warning: symbol value ‘m’ invalid for MFD_WM8350.config:2582:warning: symbol value ‘m’ invalid for MFD_WM8350_I2C.config:2584:warning: symbol value ‘m’ invalid for AB3100_CORE.config:3502:warning: symbol value ‘m’ invalid for MMC_RICOH_MMC * Restart config… * * General setup * …… XZ decompressor tester (XZ_DEC_TEST) [N/m/y/?] (NEW) Averaging functions (AVERAGE) [Y/?] (NEW)yCORDIC algorithm (CORDIC) [N/m/y/?] (NEW) JEDEC DDR data (DDR) [N/y/?] (NEW) # configuration written to .config`make oldconfig会读取当前目录下的.config文件，在.config文件里没有找到的选项则提示用户填写，然后备份.config文件为.config.old，并生成新的.config文件 5.开始编译[root@sean linux-3.10.58]# make -j4 bzImage //生成内核文件[root@sean linux-3.10.58]# make -j4 modules //编译模块[root@sean linux-3.10.58]# make -j4 modules_install //编译安装模块 -j后面的数字是线程数，用于加快编译速度，一般的经验是，逻辑CPU，就填写那个数字，例如有8核，则为-j8。（modules部分耗时30多分钟） 6.安装[root@sean linux-3.10.58]# make install实际运行到这一步时，出现ERROR: modinfo: could not find module vmware_balloon，但是不影响内核安装，是由于vsphere需要的模块没有编译，要避免这个问题，需要在make之前时修改.config文件，加入HYPERVISOR_GUEST=yCONFIG_VMWARE_BALLOON=m（这一部分比较容易出问题，参考下文异常部分） 7.修改grub引导，重启安装完成后，需要修改Grub引导顺序，让新安装的内核作为默认内核。编辑 grub.conf文件，vi /etc/grub.conf #boot=/dev/sdadefault=0timeout=5splashimage=(hd0,0)/grub/splash.xpm.gzhiddenmenutitle CentOS (3.10.58) root (hd0,0)… 数一下刚刚新安装的内核在哪个位置，从0开始，然后设置default为那个数字，一般新安装的内核在第一个位置，所以设置default=0。重启reboot now 8.确认当内核版本[root@sean ~]# uname -r3.10.58 升级内核成功!]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>operation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos6.5下搭建hadoop2.7单机伪分布环境]]></title>
    <url>%2F2017%2F10%2F01%2Fcentos6.5%2Bhadoop2.7%2F</url>
    <content type="text"><![CDATA[设置固定IP地址及网关 设置IPvi /etc/sysconfig/network-scripts/ifcfg-eth0 修改内容如下DEVICE=eth0HWADDR=08:00:27:BD:9D:B5 #不用改TYPE=EthernetUUID=53e4e4b6-9724-43ab-9da7-68792e611031 #不用改ONBOOT=yes #开机启动NM_CONTROLLED=yesBOOTPROTO=static #静态IPIPADDR=192.168.30.50 #IP地址NETMASK=255.255.255.0 #子网掩码 设置网关vi /etc/sysconfig/network 添加内容NETWORKING=yesHOSTNAME=Hadoop.MasterGATEWAY=192.168.30.1 #网关 设置DNSvi /etc/resolv.conf 添加内容nameserver xxx.xxx.xxx.xxx #根据实际情况设置nameserver 114.114.114.114 #可以设置多个 重启网卡service network restart 设置主机名对应IP地址vi /etc/hosts#添加如下内容192.168.30.50 Hadoop.Master添加Hadoop用户 添加用户组groupadd hadoop 添加用户并分配用户组useradd -g hadoop hadoop 修改用户密码passwd hadoop关闭服务 关闭防火墙service iptables stop #关闭防火墙服务chkconfig iptables off #关闭防火墙开机启动service ip6tables stopchkconfig ip6tables off 关闭SELinuxvi /etc/sysconfig/selinux#修改如下内容SELINUX=enforcing -&gt; SELINUX=disabled#再执行如下命令setenforce 0getenforce 关闭其他服务VSFTP安装与配置 检查是否安装chkconfig –list|grep vsftpd 安装vsftpyum -y install vsftpd 创建日志文件touch /var/log/vdftpd.log 配置vsftpd服务vi /etc/vsftpd/vsftpd.conf#修改如下内容anonymous_enable=NO #关闭匿名访问xferlog_file=/var/log/vsftpd.log #设置日志文件 – 我们上一步所创建的文件idle_session_timeout=600 #会话超时时间async_abor_enable=YES #开启异步传输ascii_upload_enable=YES #开启ASCII上传ascii_download_enable=YES #开启ASCII下载 查看vsftp运行状态service vsftpd status启动vsftpservice vsftpd start#重启 service vsftpd restart#关闭 service vsftpd stop 查看vsftpd服务启动项chkconfig –list|grep vsftpd 设置vsftp开机启动chkconfig vsftpd onSSH无密码配置 查看ssh与rsync安装状态rpm -qa|grep opensshrpm -qa|grep rsync 安装ssh与rsyncyum -y install sshyum -y install rsync 切换hadoop用户su - hadoop 生成ssh密码对ssh-keygen -t rsa -P ‘’ -f ~/.ssh/id_rsa 将id_dsa.pub追加到授权的key中cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys设置授权key权限chmod 600 ~/.ssh/authorized_keys#权限的设置非常重要，因为不安全的设置安全设置，会让你不能使用RSA功能测试ssh连接ssh localhost#如果不需要输入密码，则是成功安装Java 下载地址http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html 注：我这里使用的是：jdk-7u80-linux-i586.tar.gz 安装Java切换至root用户su root 创建/usr/java文件夹mkdir /usr/java 使用winscp工具上传至服务器 将压缩包上传至/home/hadoop目录 注：我这里使用的是winscp，使用hadoop用户连接 将压缩包解压至/usr/java 目录tar zxvf /home/hadoop/jdk-7u80-linux-i586.tar.gz -C /usr/java/ 设置环境变量vi /etc/profile#追加如下内容export JAVA_HOME=/usr/java/jdk1.7.0_80export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/libexport PATH=$PATH:$JAVA_HOME/bin 使环境变量生效source /etc/profile 测试环境变量设置java -versionHadoop安装与配置下载地址http://hadoop.apache.org/releases.html 注:我下载的是hadoop-2.7.1.tar.gz 安装Hadoop使用winscp工具上传至服务器将压缩包上传至/home/hadoop目录*将压缩包解压至/usr目录tar zxvf /home/hadoop/hadoop-2.7.1.tar.gz -C /usr/ 修改文件夹名称mv /usr/hadoop-2.7.1/ /usr/hadoop 创建hadoop数据目录mkdir /usr/hadoop/tmp 将hadoop文件夹授权给hadoop用户chown -R hadoop:hadoop /usr/hadoop/ 设置环境变量vi /etc/profile#追加如下内容export HADOOP_HOME=/usr/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=”-Djava.library.path=$HADOOP_HOME/lib 使环境变量生效source /etc/profile 测试环境变量设置hadoop version配置HDFS 切换至Hadoop用户su - hadoop 修改hadoop-env.shcd /usr/hadoop/etc/hadoop/vi hadoop-env.sh#追加如下内容export JAVA_HOME=/usr/java/jdk1.7.0_80 修改core-site.xmlvi core-site.xml#添加如下内容 fs.defaultFS hdfs://Hadoop.Master:9000 hadoop.tmp.dir /usr/hadoop/tmp/ A base for other temporary directories. 修改hdfs-site.xmlvi hdfs-site.xml#添加如下内容 dfs.replication 1 格式化hdfshdfs namenode -format注：出现Exiting with status 0即为成功 启动hdfsstart-dfs.sh#停止命令 stop-dfs.sh注：输出如下内容15/09/21 18:09:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicableStarting namenodes on [Hadoop.Master]Hadoop.Master: starting namenode, logging to /usr/hadoop/logs/hadoop-hadoop-namenode-Hadoop.Master.outHadoop.Master: starting datanode, logging to /usr/hadoop/logs/hadoop-hadoop-datanode-Hadoop.Master.outStarting secondary namenodes [0.0.0.0]The authenticity of host ‘0.0.0.0 (0.0.0.0)’ can’t be established.RSA key fingerprint is b5:96:b2:68:e6:63:1a:3c:7d:08:67:4b:ae:80:e2:e3.Are you sure you want to continue connecting (yes/no)? yes0.0.0.0: Warning: Permanently added ‘0.0.0.0’ (RSA) to the list of known hosts.0.0.0.0: starting secondarynamenode, logging to /usr/hadoop/logs/hadoop-hadoop-secondarynamenode-Hadoop.Master.out15/09/21 18:09:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicab 查看进程jps注：输出类似如下内容1763 NameNode1881 DataNode2146 Jps2040 SecondaryNameNode 使用web查看Hadoop运行状态http://你的服务器ip地址:50070/在HDFS上运行WordCount 创建HDFS用户目录hdfs dfs -mkdir /userhdfs dfs -mkdir /user/hadoop #根据自己的情况调整/user/ 复制输入文件（要处理的文件）到HDFS上hdfs dfs -put /usr/hadoop/etc/hadoop input 查看我们复制到HDFS上的文件hdfs dfs -ls input 运行单词检索（grep）程序hadoop jar /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input output ‘dfs[a-z.]+’#WordCount#hadoop jar /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount input output#说明：output文件夹如已经存在则需要删除或指定其他文件夹。 查看运行结果hdfs dfs -cat output/*配置YARN 修改mapred-site.xmlcd /usr/hadoop/etc/hadoop/cp mapred-site.xml.template mapred-site.xmlvi mapred-site.xml#添加如下内容 mapreduce.framework.name yarn 修改yarn-site.xmlvi yarn-site.xml#添加如下内容 yarn.nodemanager.aux-services mapreduce_shuffle 启动YARNstart-yarn.sh#停止yarn stop-yarn.sh 查看当前java进程jsp#输出如下4918 ResourceManager1663 NameNode1950 SecondaryNameNode5010 NodeManager5218 Jps1759 DataNode 运行你的mapReduce程序 配置好如上配置再运行mapReduce程序时即是yarn中运行。 使用web查看Yarn运行状态http://你的服务器ip地址:8088/ HDFS常用命令 创建HDFS文件夹 在根目录创建input文件夹hdfs dfs -mkdir -p /input 在用户目录创建input文件夹说明：如果不指定“/目录”，则默认在用户目录创建文件夹hdfs dfs -mkdir -p input#等同于 hdfs dfs -mkdir -p /user/hadoop/input 查看HDFS文件夹 查看HDFS根文件夹hdfs dfs -ls / 查看HDFS用户目录文件夹hdfs dfs -ls 查看HDFS用户目录文件夹下input文件夹hdfs dfs -ls input#等同与 hdfs dfs -ls /user/hadoop/input 复制文件到HDFShdfs dfs -put /usr/hadoop/etc/hadoop input 删除文件夹hdfs dfs -rm -r input]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive参数优化]]></title>
    <url>%2F2017%2F08%2F29%2Fhive%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑 1.设置合理solt数mapred.tasktracker.map.tasks.maximum每个tasktracker可同时运行的最大map task数，默认值2。mapred.tasktracker.reduce.tasks.maximum每个tasktracker可同时运行的最大reduce task数，默认值1 2.配置磁盘块mapred.local.dirmap task中间结果写本地磁盘路径，默认值${hadoop.tmp.dir}/mapred/local。可配置多块磁盘缓解写压力。当存在多个可以磁盘时，Hadoop将采用轮询方式将不同的map task中间结果写到磁盘上。 3.配置RPC Handler数mapred.job.tracker.handler.countjobtracker可并发处理来自tasktracker的RPC请求数，默认值10。 4.配置HTTP线程数tasktracker.http.threadsHTTP服务器的工作线程数，用于获取map task的输出结果，默认值40。 5.启用批调度 6.选择合适的压缩算法Job输出结果是否压缩mapred.output.compress是否压缩，默认值false。mapred.output.compression.type压缩类型，有NONE, RECORD和BLOCK，默认值RECORD。mapred.output.compression.codec压缩算法，默认值org.apache.hadoop.io.compress.DefaultCodec。map task输出是否压缩mapred.compress.map.output是否压缩，默认值falsemapred.map.output.compression.codec压缩算法，默认值org.apache.hadoop.io.compress.DefaultCodec。 7.设置失败容忍度mapred.max.map.failures.percent例如：set mapred.max.map.failures.percent=30;作业最多允许失败的map task比例，默认值0。mapred.max.reduce.failures.percent作业最多允许失败的reduce task比例，默认值0。mapred.map.max.attempts一个map task的最多重试次数，默认值4。mapred.reduce.max.attempts一个reduce task的最多重试次数，默认值4。 8.设置跳过坏记录mapred.skip.attempts.to.start.skipping当任务失败次数达到该值时，启用跳过坏记录功能，默认值2。 mapred.skip.out.dir检测出的坏记录存放目录，默认值为输出目录的_logs/skip，设置为none表示不输出。mapred.skip.map.max.skip.recordsmap task最多允许的跳过记录数，默认值0。mapred.skip.reduce.max.skip.groupsreduce task最多允许的跳过记录数，默认值0。 9.配置jvm重用mapred.job.reuse.jvm.num.tasks一个jvm可连续启动多个同类型任务，默认值1，若为-1表示不受限制。 10.配置jvm参数mapred.child.java.opts任务启动的jvm参数，默认值-Xmx200m，建议值-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m -verbose:gc -Xloggc:/tmp/@taskid@.gc map task调优io.sort.mb默认值100Mio.sort.record.percent默认值0.05io.sort.spill.percent默认值0.80 12.reduce task调优io.sort.factor默认值10mapred.reduce.parallel.copies默认值5]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>Hhive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka参数优化]]></title>
    <url>%2F2017%2F08%2F25%2Fkafka%E4%BC%98%E5%8C%96%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑 非负整数，用于唯一标识brokerbroker.id=0 broker 服务监听端口port=9092 broker 发布给生产者消费者的hostname，会存储在zookeeper。配置好这个host可以实现内网外网同时访问。advertised.host.name=host1 broker 发布给生产者消费者的port，会存储在zookeeper。advertised.port=9092 处理网络请求的线程数量，一般默认配置就好num.network.threads=3 处理磁盘的线程数量，一般默认配置就好num.io.threads=8 socket server 发送数据缓冲区大小socket.send.buffer.bytes=102400 socket server 接受数据缓冲区大小socket.receive.buffer.bytes=102400 soket server 可接受最大消息大小，防止oomsocket.request.max.bytes=104857600 kafka存放消息的目录log.dirs=/home/data/kafka/kafka-logs 每个topic默认partition数量，根据消费者实际情况配置，配置过小会影响消费性能num.partitions=50 kafka启动恢复日志,关闭前日志刷盘的线程数num.recovery.threads.per.data.dir=1 日志保留时间log.retention.minutes=30 日志保留大小log.retention.bytes=53687091200 日志 segment file 大小. 超过这个大小创建新segment filelog.segment.bytes=67108864 日志 segment file 刷新时间. 超过这个时间创建新segment filelog.roll.hours=24 日志淘汰检查间隔时间log.retention.check.interval.ms=10000 Zookeeper host和portzookeeper.connect=localhost:2181 连接zookeeper超时时间zookeeper.connection.timeout.ms=6000 清除fetch purgatory 间隔消息条数fetch.purgatory.purge.interval.requests=100 清除producer purgatory 间隔消息条数producer .purgatory.purge.interval.requests=100 是否可以通过管理工具删除topic，默认是falsedelete.topic.enable=true 日志传输时候的压缩格式，可选择lz4, snappy, gzip,不压缩。建议打开压缩，可以提高传输性能，压缩格式的选择可以参考文章结尾的参考资料。compression.type=snappy 启用压缩的topic名称。若上面参数选择了一个压缩格式，那么压缩仅对本参数指定的topic有效，若本参数为空，则对所有topic有效。compressed.topics=topic1 用来从主partion同步数据的线程数，默认为1，建议适当调大，数据量大的时候一个同步线程可能不够用num.replica.fetchers=3 消息日志备份因子，默认是1default.replication.factor=2]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase手动优化]]></title>
    <url>%2F2017%2F08%2F20%2Fhbase%E6%89%8B%E5%8A%A8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑配置优化 zookeeper.session.timeout默认值：3分钟（180000ms）说明：RegionServer与Zookeeper间的连接超时时间。当超时时间到后，ReigonServer会被Zookeeper从RS集群清单中移除，HMaster收到移除通知后，会对这台server负责的regions重新balance，让其他存活的* RegionServer接管.调优：这个timeout决定了RegionServer是否能够及时的failover。设置成1分钟或更低，可以减少因等待超时而被延长的failover时间。不过需要注意的是，对于一些Online应用，RegionServer从宕机到恢复时间本身就很短的（网络闪断，crash等故障，运维可快速介入），如果调低timeout时间，反而会得不偿失。因为当ReigonServer被正式从RS集群中移除时，HMaster就开始做balance了（让其他RS根据故障机器记录的WAL日志进行恢复）。当故障的RS在人工介入恢复后，这个balance动作是毫无意义的，反而会使负载不均匀，给RS带来更多负担。特别是那些固定分配regions的场景。 Hbase.zookeeper.quorum默认值：localhost说明：hbase所依赖的zookeeper部署调优：部署的zookeeper越多，可靠性就越高，但是部署只能部署奇数个，主要为了便于选出leader。最好给每个zookeeper 1G的内存和独立的磁盘，可以确保高性能。hbase.zookeeper.property.dataDir可以修改zookeeper保存数据的路径。 hbase.regionserver.handler.count默认值：10说明：RegionServer的请求处理IO线程数。调优：这个参数的调优与内存息息相关。较少的IO线程，适用于处理单次请求内存消耗较高的Big PUT场景（大容量单次PUT或设置了较大cache的scan，均属于Big PUT）或ReigonServer的内存比较紧张的场景。较多的IO线程，适用于单次请求内存消耗低，TPS要求非常高的场景。设置该值的时候，以监控内存为主要参考。这里需要注意的是如果server的region数量很少，大量的请求都落在一个region上，因快速充满memstore触发flush导致的读写锁会影响全局TPS，不是IO线程数越高越好。压测时，开启Enabling RPC-level logging，可以同时监控每次请求的内存消耗和GC的状况，最后通过多次压测结果来合理调节IO线程数。这里是一个案例?Hadoop and HBase Optimization for Read Intensive Search Applications，作者在SSD的机器上设置IO线程数为100，仅供参考。 hbase.hregion.max.filesize默认值：256M说明：在当前ReigonServer上单个Reigon的最大存储空间，单个Region超过该值时，这个Region会被自动split成更小的region。调优：小region对split和compaction友好，因为拆分region或compact小region里的storefile速度很快，内存占用低。缺点是split和compaction会很频繁。特别是数量较多的小region不停地split, compaction，会导致集群响应时间波动很大，region数量太多不仅给管理上带来麻烦，甚至会引发一些Hbase的bug。一般512以下的都算小region。 大region，则不太适合经常split和compaction，因为做一次compact和split会产生较长时间的停顿，对应用的读写性能冲击非常大。此外，大region意味着较大的storefile，compaction时对内存也是一个挑战。当然，大region也有其用武之地。如果你的应用场景中，某个时间点的访问量较低，那么在此时做compact和split，既能顺利完成split和compaction，又能保证绝大多数时间平稳的读写性能。 既然split和compaction如此影响性能，有没有办法去掉？compaction是无法避免的，split倒是可以从自动调整为手动。只要通过将这个参数值调大到某个很难达到的值，比如100G，就可以间接禁用自动split（RegionServer不会对未到达100G的region做split）。再配合RegionSplitter这个工具，在需要split时，手动split。手动split在灵活性和稳定性上比起自动split要高很多，相反，管理成本增加不多，比较推荐online实时系统使用。 内存方面，小region在设置memstore的大小值上比较灵活，大region则过大过小都不行，过大会导致flush时app的IO wait增高，过小则因store file过多影响读性能。 hbase.regionserver.global.memstore.upperLimit/lowerLimit默认值：0.4/0.35upperlimit说明：hbase.hregion.memstore.flush.size 这个参数的作用是当单个Region内所有的memstore大小总和超过指定值时，flush该region的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模式来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。这个参数的作用是防止内存占用过大，当ReigonServer内所有region的memstores所占用内存总和达到heap的40%时，HBase会强制block所有的更新并flush这些region以释放所有memstore占用的内存。lowerLimit说明： 同upperLimit，只不过lowerLimit在所有region的memstores所占用内存达到Heap的35%时，不flush所有的memstore。它会找一个memstore内存占用最大的region，做个别flush，此时写更新还是会被block。lowerLimit算是一个在所有region强制flush导致性能降低前的补救措施。在日志中，表现为 “** Flush thread woke up with memory above low water.”调优：这是一个Heap内存保护参数，默认值已经能适用大多数场景。参数调整会影响读写，如果写的压力大导致经常超过这个阀值，则调小读缓存hfile.block.cache.size增大该阀值，或者Heap余量较多时，不修改读缓存大小。如果在高压情况下，也没超过这个阀值，那么建议你适当调小这个阀值再做压测，确保触发次数不要太多，然后还有较多Heap余量的时候，调大hfile.block.cache.size提高读性能。还有一种可能性是?hbase.hregion.memstore.flush.size保持不变，但RS维护了过多的region，要知道 region数量直接影响占用内存的大小。 hfile.block.cache.size 默认值：0.2说明：storefile的读缓存占用Heap的大小百分比，0.2表示20%。该值直接影响数据读的性能。调优：当然是越大越好，如果写比读少很多，开到0.4-0.5也没问题。如果读写较均衡，0.3左右。如果写比读多，果断默认吧。设置这个值的时候，你同时要参考?hbase.regionserver.global.memstore.upperLimit?，该值是memstore占heap的最大百分比，两个参数一个影响读，一个影响写。如果两值加起来超过80-90%，会有OOM的风险，谨慎设置。 hbase.hstore.blockingStoreFiles默认值：7说明：在flush时，当一个region中的Store（Coulmn Family）内有超过7个storefile时，则block所有的写请求进行compaction，以减少storefile数量。调优：block写请求会严重影响当前regionServer的响应时间，但过多的storefile也会影响读性能。从实际应用来看，为了获取较平滑的响应时间，可将值设为无限大。如果能容忍响应时间出现较大的波峰波谷，那么默认或根据自身场景调整即可。 hbase.hregion.memstore.block.multiplier默认值：2说明：当一个region里的memstore占用内存大小超过hbase.hregion.memstore.flush.size两倍的大小时，block该region的所有请求，进行flush，释放内存。虽然我们设置了region所占用的memstores总内存大小，比如64M，但想象一下，在最后63.9M的时候，我Put了一个200M的数据，此时memstore的大小会瞬间暴涨到超过预期的hbase.hregion.memstore.flush.size的几倍。这个参数的作用是当memstore的大小增至超过hbase.hregion.memstore.flush.size 2倍时，block所有请求，遏制风险进一步扩大。调优： 这个参数的默认值还是比较靠谱的。如果你预估你的正常应用场景（不包括异常）不会出现突发写或写的量可控，那么保持默认值即可。如果正常情况下，你的写请求量就会经常暴长到正常的几倍，那么你应该调大这个倍数并调整其他参数值，比如hfile.block.cache.size和hbase.regionserver.global.memstore.upperLimit/lowerLimit，以预留更多内存，防止HBase server OOM。 hbase.hregion.memstore.mslab.enabled默认值：true说明：减少因内存碎片导致的Full GC，提高整体性能。调优：详见 http://kenwublog.com/avoid-full-gc-in-hbase-using-arena-allocation hbase.client.scanner.caching默认值：1说明：scanner调用next方法一次获取的数据条数调优：少的RPC是提高hbase执行效率的一种方法，理论上一次性获取越多数据就会越少的RPC，也就越高效。但是内存是最大的障碍。设置这个值的时候要选择合适的大小，一面一次性获取过多数据占用过多内存，造成其他程序使用内存过少。或者造成程序超时等错误（这个超时与hbase.regionserver.lease.period相关）。 hbase.regionserver.lease.period默认值：60000说明：客户端租用HRegion server 期限，即超时阀值。调优：这个配合hbase.client.scanner.caching使用，如果内存够大，但是取出较多数据后计算过程较长，可能超过这个阈值，适当可设置较长的响应时间以防被认为宕机。]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce参数优化]]></title>
    <url>%2F2017%2F08%2F19%2FMapReduce%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑1 core-default.xml： hadoop.tmp.dir： 默认值： /tmp说明： 尽量手动配置这个选项，否则的话都默认存在了里系统的默认临时文件/tmp里。并且手动配置的时候，如果服务器是多磁盘的，每个磁盘都设置一个临时文件目录，这样便于mapreduce或者hdfs等使用的时候提高磁盘IO效率。 fs.trash.interval： 默认值： 0说明： 这个是开启hdfs文件删除自动转移到垃圾箱的选项，值为垃圾箱文件清除时间。一般开启这个会比较好，以防错误删除重要文件。单位是分钟。 io.file.buffer.size： 默认值：4096说明：SequenceFiles在读写中可以使用的缓存大小，可减少 I/O 次数。在大型的 Hadoop cluster，建议可设定为 65536 到 131072。 2 hdfs-default.xml： dfs.blocksize： 默认值：134217728说明： 这个就是hdfs里一个文件块的大小了，CDH5中默认128M。太大的话会有较少map同时计算，太小的话也浪费可用map个数资源，而且文件太小namenode就浪费内存多。根据需要进行设置。 dfs.namenode.handler.count： 默认值：10说明：设定 namenode server threads 的数量，这些 threads 會用 RPC 跟其他的 datanodes 沟通。当 datanodes 数量太多时会发現很容易出現 RPC timeout，解決方法是提升网络速度或提高这个值，但要注意的是 thread 数量多也表示 namenode 消耗的内存也随着增加 3 mapred-default.xml： mapred.reduce.tasks（mapreduce.job.reduces）： 默认值：1说明：默认启动的reduce数。通过该参数可以手动修改reduce的个数。 mapreduce.task.io.sort.factor： 默认值：10说明：Reduce Task中合并小文件时，一次合并的文件数据，每次合并的时候选择最小的前10进行合并。 mapreduce.task.io.sort.mb： 默认值：100说明： Map Task缓冲区所占内存大小。 mapred.child.java.opts： 默认值：-Xmx200m说明：jvm启动的子线程可以使用的最大内存。建议值-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m -verbose:gc -Xloggc:/tmp/@taskid@.gc mapreduce.jobtracker.handler.count： 默认值：10说明：JobTracker可以启动的线程数，一般为tasktracker节点的4%。 mapreduce.reduce.shuffle.parallelcopies： 默认值：5说明：reuduce shuffle阶段并行传输数据的数量。这里改为10。集群大可以增大。 mapreduce.tasktracker.http.threads： 默认值：40说明：map和reduce是通过http进行数据传输的，这个是设置传输的并行线程数。 mapreduce.map.output.compress： 默认值：false说明： map输出是否进行压缩，如果压缩就会多耗cpu，但是减少传输时间，如果不压缩，就需要较多的传输带宽。配合 mapreduce.map.output.compress.codec使用，默认是 org.apache.hadoop.io.compress.DefaultCodec，可以根据需要设定数据压缩方式。 mapreduce.reduce.shuffle.merge.percent： 默认值： 0.66说明：reduce归并接收map的输出数据可占用的内存配置百分比。类似mapreduce.reduce.shuffle.input.buffer.percen属性。 mapreduce.reduce.shuffle.memory.limit.percent： 默认值： 0.25说明：一个单一的shuffle的最大内存使用限制。 mapreduce.jobtracker.handler.count： 默认值： 10说明：可并发处理来自tasktracker的RPC请求数，默认值10。 mapred.job.reuse.jvm.num.tasks（mapreduce.job.jvm.numtasks）： 默认值： 1说明：一个jvm可连续启动多个同类型任务，默认值1，若为-1表示不受限制。 mapreduce.tasktracker.tasks.reduce.maximum： 默认值： 2说明：一个tasktracker并发执行的reduce数，建议为cpu核数]]></content>
      <categories>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[错误及解决方案]]></title>
    <url>%2F2017%2F08%2F10%2Fambari%E6%90%AD%E5%BB%BA%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[ERROR: Exiting with exit code 1.REASON: Database check failed to complete. Please check /var/log/ambari-server/ambari-server.log and /var/log/ambari-server/ambari-server-check-database.log for more information.解决方法：查看日志，具体什么错.例如：数据库未开启、数据库配置问题 撤销已经赋予给 MySQL 用户权限的权限。revoke 跟 grant 的语法差不多，只需要把关键字 “to” 换成 “from” 即可：revoke all on . from ‘root’@’192.168.0.197’; [root@datanode01 ~]# yum makecacheLoaded plugins: fastestmirrorDetermining fastest mirrorsambari-2.4.1.0 | 2.9 kB 00:00ambari-2.4.1.0/filelists_db | 139 kB 00:00ambari-2.4.1.0/primary_db | 8.3 kB 00:00ambari-2.4.1.0/other_db | 1.3 kB 00:00file:///mnt/repodata/repomd.xml: [Errno 14] Could not open/read file:///mnt/repodata/repomd.xmlTrying other mirror.Error: Cannot retrieve repository metadata (repomd.xml) for repository: c6-media. Please verify its path and try again 如果有后面两个文件，处理方式mv CentOS-Media.repo CentOS-Media.repo.bak没有的话，源问题，换源repos tip:1、当提示要做如下操作的时候Be?sure?you?have?run:ambari-server?setup?–jdbc-db=mysql?–jdbc-driver=/path/to/mysql/**需要下载mysql-connector-java-5.1.39.tar驱动，解压得到mysql-connector-java-5.1.39-bin.jar文件执行如下命令：ambari-server?setup?–jdbc-db=mysql?–jdbc-driver=/usr/lib/java/mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar同时，设置文件权限为644 2017.7.191.ERROR namenode.NameNode (NameNode.java:main(1759)) - Failed to start namenode.java.net.BindException: Port in use: datanode01:50070 不能获取映射地址，需要修改hosts中的映射ip为真实ip（ifconfig）2.错删自定义的service导致不能登录amnari UI（日志报出database问题） （百度方案无解，最后重装ambari-server(发现重装无法下手，百度后删除ambari相关组件以及数据库的完全卸载，完成)）3.使用ambari-server启动HDFS时，DataNode无法启动（出现port in use：localhost 0）。解决方式，在hosts文件中localhost 172.0.0.1被#；去除#后解决 一般故障排除Ambari服务器：检查/var/log/ambari-server/ambari-server.[log|out]是否存在错误。Ambari代理：检查/var/log/ambari-agent/ambari-agent.[log|out]是否存在错误。请注意，如果Ambari Agent在/var/log/ambari-agent/ambari-agent.out中有任何输出，则表明存在重大问题。 服务无法启动HDFS：检查/ var / log / hadoop / hdfs下的日志文件MapReduce：检查/ var / log / hadoop / mapred下的日志文件HBase：检查/ var / log / hbase下的日志文件Hive：检查/ var / log / hive下的日志文件Oozie：检查/ var / log / oozie下的日志文件ZooKeeper：检查/ var / log / zookeeper下的日志文件WebHCat：检查/ var / log / webhcat下的日志文件Nagios：检查/ var / log / nagios下的日志文件 2017.7.20Service ‘userhome’ check failed: java.io.FileNotFoundException: File does not exist: /user/admin解决方案：sudo -u hdfs hdfs dfs -mkdir /user/adminsudo -u hdfs hdfs dfs -chown admin:hadoop /user/admin 资源池问题FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed forblock pool Block pool BP-1480406410-192.168.1.181-1398701121586 (storage idDS-167510828-192.168.1.191-50010-1398750515421)原因：每次namenode format会重新创建一个namenodeId,而data目录包含了上次format时的id,namenode format清空了namenode下的数据,但是没有清空datanode下的数据,导致启动时失败,所要做的就是每次fotmat前,清空data下的所有目录.: d6 E2 t&amp; M” g7 a* q3 l, H解决办法：停掉集群，删除问题节点的data目录下的所有内容。即hdfs-site.xml文件中配置的dfs.data.dir目录。重新格式化namenode。 另一个更省事的办法：先停掉集群，然后将datanode节点目录/dfs/data/current/VERSION中的修改为与namenode一致即可。其实我没解决，直接重装 block missing问题会导致进入安全模式：处理方式，删除缺失包，在HDFS用户下退出安全模式 添加节点时，需要查看该节点root磁盘下的剩余大小，如果磁盘空间不足，则扩大磁盘再添加节点。不要问我为什么，说多了都是泪。集群就是这么崩盘的。]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[amabri卸载]]></title>
    <url>%2F2017%2F08%2F10%2Fambari%E5%8D%B8%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[1.删除hdp.repo和hdp-util.repocd /etc/yum.repos.d/rm -rf hdprm -rf HDPrm -rf ambari* 2.删除安装包用yum list installed | grep HDP来检查安装的ambari的包yum remove -y sqoop.noarchyum remove -y lzo-devel.x86_64yum remove -y hadoop-libhdfs.x86_64yum remove -y rrdtool.x86_64yum remove -y hbase.noarchyum remove -y pig.noarchyum remove -y lzo.x86_64yum remove -y ambari-log4j.noarchyum remove -y oozie.noarchyum remove -y oozie-client.noarchyum remove -y gweb.noarchyum remove -y snappy-devel.x86_64yum remove -y hcatalog.noarchyum remove -y python-rrdtool.x86_64yum remove -y nagios.x86_64yum remove -y webhcat-tar-pig.noarchyum remove -y snappy.x86_64yum remove -y libconfuse.x86_64yum remove -y webhcat-tar-hive.noarchyum remove -y ganglia-gmetad.x86_64yum remove -y extjs.noarchyum remove -y hive.noarchyum remove -y hadoop-lzo.x86_64yum remove -y hadoop-lzo-native.x86_64yum remove -y hadoop-native.x86_64yum remove -y hadoop-pipes.x86_64yum remove -y nagios-plugins.x86_64yum remove -y hadoop.x86_64yum remove -y zookeeper.noarchyum remove -y hadoop-sbin.x86_64yum remove -y ganglia-gmond.x86_64yum remove -y libganglia.x86_64yum remove -y perl-rrdtool.x86_64yum remove -y epel-release.noarchyum remove -y compat-readline5*yum remove -y fping.x86_64yum remove -y perl-Crypt-DES.x86_64yum remove -y exim.x86_64yum remove -y ganglia-web.noarchyum remove -y perl-Digest-HMAC.noarchyum remove -y perl-Digest-SHA1.x86_64yum remove -y bigtop-jsvc.x86_64 3.删除快捷方式cd /etc/alternativesrm -rf hadoop-etcrm -rf zookeeper-confrm -rf hbase-confrm -rf hadoop-logrm -rf hadoop-librm -rf hadoop-defaultrm -rf oozie-confrm -rf hcatalog-confrm -rf hive-confrm -rf hadoop-manrm -rf sqoop-confrm -rf hadoop-conf 4.删除用户userdel nagiosuserdel hiveuserdel ambari-qauserdel hbaseuserdel oozieuserdel hcatuserdel mapreduserdel hdfsuserdel rrdcacheduserdel zookeeper #userdel mysqluserdel sqoopuserdel puppetuserdel yarnuserdel tezuserdel hadoopuserdel knoxuserdel stormuserdel falconuserdel flumeuserdel nagiosuserdel adminuserdel postgresuserdel hdfsuserdel zookeeperuserdel hbase 5.删除文件夹rm -rf /hadooprm -rf /etc/hadooprm -rf /etc/hbaserm -rf /etc/hcatalogrm -rf /etc/hiverm -rf /etc/gangliarm -rf /etc/nagiosrm -rf /etc/oozierm -rf /etc/sqooprm -rf /etc/zookeeperrm -rf /var/run/hadooprm -rf /var/run/hbaserm -rf /var/run/hiverm -rf /var/run/gangliarm -rf /var/run/nagiosrm -rf /var/run/oozierm -rf /var/run/zookeeperrm -rf /var/log/hadooprm -rf /var/log/hbaserm -rf /var/log/hiverm -rf /var/log/nagiosrm -rf /var/log/oozierm -rf /var/log/zookeeperrm -rf /usr/lib/hadooprm -rf /usr/lib/hbaserm -rf /usr/lib/hcatalogrm -rf /usr/lib/hiverm -rf /usr/lib/oozierm -rf /usr/lib/sqooprm -rf /usr/lib/zookeeperrm -rf /var/lib/hiverm -rf /var/lib/gangliarm -rf /var/lib/oozierm -rf /var/lib/zookeeperrm -rf /var/tmp/oozierm -rf /tmp/hiverm -rf /tmp/nagiosrm -rf /tmp/ambari-qarm -rf /tmp/sqoop-ambari-qarm -rf /var/nagiosrm -rf /hadoop/oozierm -rf /hadoop/zookeeperrm -rf /hadoop/mapredrm -rf /hadoop/hdfsrm -rf /tmp/hadoop-hiverm -rf /tmp/hadoop-nagiosrm -rf /tmp/hadoop-hcatrm -rf /tmp/hadoop-ambari-qarm -rf /tmp/hsperfdata_hbaserm -rf /tmp/hsperfdata_hiverm -rf /tmp/hsperfdata_nagiosrm -rf /tmp/hsperfdata_oozierm -rf /tmp/hsperfdata_zookeeperrm -rf /tmp/hsperfdata_mapredrm -rf /tmp/hsperfdata_hdfsrm -rf /tmp/hsperfdata_hcatrm -rf /tmp/hsperfdata_ambari-qarm -rf /etc/flumerm -rf /etc/stormrm -rf /etc/hive-hcatalogrm -rf /etc/tezrm -rf /etc/falconrm -rf /var/run/flumerm -rf /var/run/stormrm -rf /var/run/webhcatrm -rf /var/run/hadoop-yarnrm -rf /var/run/hadoop-mapreducerm -rf /var/log/flumerm -rf /var/log/stormrm -rf /var/log/hadoop-yarnrm -rf /var/log/hadoop-mapreducerm -rf /usr/lib/nagiosrm -rf /var/lib/hdfsrm -rf /var/lib/hadoop-hdfsrm -rf /var/lib/hadoop-yarnrm -rf /var/lib/hadoop-mapreducerm -rf /tmp/hadoop-hdfs 5.重置数据库，删除ambari包#采用这句命令来检查yum list installed | grep ambariambari-server stopambari-agent stopambari-server resetyum remove -y ambari-yum remove -y postgresqlrm -rf /etc/yum.repos.d/ambarirm -rf /var/lib/ambarirm -rf /var/log/ambarirm -rf /etc/ambari*]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ambari搭建]]></title>
    <url>%2F2017%2F08%2F09%2Fambari%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.物理条件：三台centos前期条件：修改每台centos的名称，分别为namenode、datanode01、datanode02 ssh免密码配置：namenode 可以免密码登录到datanode01、datanode02配置ssh 在namenode上生成密钥对：ssh-keygen –t rsa 在/root/.ssh中会有生成的密钥对 将namenode 的 id_rsa.pub写入datanode01的/root/.ssh authorized_keys文件中。再将datanode01的authorized_keys 写入namenode /root/.sshdatanode02同理操作scp .ssh/id_rsa.pub chenlb@192.168.1.181:/home/chenlb/id_rsa.pubcat id_rsa.pub &gt;&gt; .ssh/authorized_keys 将namenode中的三个机器的私钥 id_rsa提取到桌面，后续使用安装ntp服务yum install ntpservice ntpd startchkconfig ntpd on 2.更换源：将/etc/yum.repos.d/下的原centos.repos备份，然后添加本地源（我这是之前师傅在其他服务器布 置好了的源，你们可在网上查找相关源） 3.验证本机名Hostname -f 显示 namenode 其他两台一样 4.关闭防火墙Service iptables stop 禁用自启动 Chkconfig iptables off 5.禁用ipv6使用lsmod查看系统启动的模块：ipv6相关的模块是net-pf-10 ipv6 在vi /etc/modprobe.d/dist.conf 中最后添加 alias net-pf-10 off alias ipv6 off 重启后，再使用lsmod查看ipv6的相应模块还在不在 6.禁用SELinux配置selinuxvi /etc/sysconfig/selinux插入SELINUX=disabled暂时禁用 setenforce 0 7.配置禁用THP(每次重启机器后需要从新配置) 查看当前THP状态 cat /sys/kernel/mm/transparent_hugepage/enabled 如果为always madvise [never]则为禁用配置禁用echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defragecho never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/ transparent_hugepage/defragecho never &gt; /sys/kernel/mm/ transparent_hugepage/enabled 8.clean源 重新加载yum clean all yum makecache 9.配置mysql作为数据库（只在ambari-server安装机器上做） 查看当前安装数据库信息 rpm -qa | grep -i mysql 卸载方法 yum -y remove +name 安装MySQL 查看当前源中是否有提供的MySQL yum list | grep mysql 安装mysql-server安装 安装后mysql 只有一个用户root 且第一次开启时需要设置密码，我们这通过命令提前设置 mysqladmin -u root password ‘’ 设置为自启动 chkconfig mysqld on 登录到root用户 mysql -u root -p create user ‘amabri’@’% ’ identified by ‘bigdata’; grant all privileges on *.* to ‘ambari’@’%’; create user ‘ambari’@’localhost’ identified by ‘bigdata’; grant all privileges on *.* to ‘ambari’@’localhost’; create user ‘ambari’@’namenode’ identified by ‘bigdata’; grant all privileges on *.* to ‘ambari’@’namenode’; 登录ambari用户 mysql -u ambari -p bigdata create database ambari; use ambari; 10.部署 （1） yum安装 yum -y install ambari-server (每台都要装)后进入mysql -u ambari -pbigdatause ambari;source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql; （2） jdk的安装 下载jdk1.8.0_45安装包 放入/usr/lib/jvm/jdk1.8.0_45/ 解压 配置环境变量 /etc/profile 文件最后加上 #Java environment JAVA_HOME=/usr/java/jdk1.8.0_45PATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/export JAVA_HOMEexport PATHexport CLASSPATH执行：sudo update-alternatives –install /usr/bin/java java /usr/lib/jvm/jdk1.8.0_45/bin/java 300sudo update-alternatives –install /usr/bin/javac javac /usr/lib/jvm/jdk1.8.0_45/bin/javac 300sudo update-alternatives –install /usr/bin/jar jar /usr/lib/jvm/jdk1.8.0_45/bin/jar 300查看当前Java版本java -version （3） 安装ambari-server (安装在一台centos上即可，部署为ambari-server的机器)ambari-server setup -j /usr/lib/jvm/jdk1.8.0­_45/安装显示：Using python /usr/bin/python Setup ambari‐server Checking SELinux… SELinux status is ‘enabled’ SELinux mode is ‘permissive’ WARNING: SELinux is set to ‘permissive’ mode and temporarily disabled. OK to continue [y/n] (y)? Customize user account for ambari‐server daemon [y/n] (n)? Adjusting ambari‐server permissions and ownership… Checking firewall status… Checking JDK… WARNING: JAVA_HOME /usr/lib/jvm/jdk1.8.0_45 must be valid on ALL hosts WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Completing setup… Configuring database… Enter advanced database configuration [y/n] (n)? y Configuring database… ============================================================================= Choose one of the following options: [1] ‐ PostgreSQL (Embedded) [2] ‐ Oracle [3] ‐ MySQL / MariaDB [4] ‐ PostgreSQL [5] ‐ Microsoft SQL Server (Tech Preview) [6] ‐ SQL Anywhere [7] ‐ BDB ============================================================================= Enter choice (3): 3 Hostname (localhost): Port (3306): Database name (ambari): Username (ambari): Enter Database Password (bigdata): Configuring ambari database… Copying JDBC drivers to server resources… Configuring remote database connection properties…WARNING: Before starting Ambari Server, you must run the following DDL against the databa se to create the schema: /var/lib/ambari Proceed with configuring remote database connection properties [y/n] (y)? y Extracting system views…………… Adjusting ambari‐server permissions and ownership… Ambari Server ‘setup’ completed successfully. （4）启动ambari-server ambari-server start 启动成功后通过：http：//namenode:8080访问 登录账号及密码：admin （6） 进程操作 1.查看ambari进程 ps -ef | grep ambari 2.停止ambari进程 ambari-server stop 3.重启ambari进程 ambari-server restart （7） 修改端口 vi /etc/ambari-server/conf/ambari.properties 插入或编辑以下内容 Client.api.port = 11 安装ambari-agent(每台机器都要装)1.安装ambari-agent yum install ambari-agent 2.配置 vi /etc/ambari-agent/conf/ambari-agent.ini 插入或更改以下内容 hostname = namenode最后使用谷歌浏览器登录：http://namenode:8080 账号：admin 密码：admin注意：选择操作系统时，应该选择当前机器的版本。如果是本地源，则需要修改HDP、HDP-UTILS的位置 If you are lucky enough, that I wish you success.Said too much, tears. 如果出现ip绑定问题，修改/etc/hosts/中的ip为每台机器的真实ip查看ip命令ifconfig出现mysql drive驱动问题 yum install mysql-connector-java]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos和Ubuntu本地源制作]]></title>
    <url>%2F2017%2F08%2F05%2Fcentos%E5%92%8CUbuntu%E5%88%B6%E4%BD%9C%E6%9C%AC%E5%9C%B0%E6%BA%90%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑CentOS系统 使用yum安装软件时，下载的* .rpm包缓存在/var/cache/yum/x86_64/7/； 创建目录 用于存放特定软件所需的软件包； mkdir -p /opt/packages 下载软件包yum install -y –downloadonly –downloaddir=/opt/packages python-openstackclient 生成repo文件yum install -y createrepo createrepo /opt/packages 生成压缩包 tar -zcf packages.tgz packages/ 配置本地源 tar -zxf packages.tgz -C /optvim /etc/yum.repos.d/local.repo [Local]name=Local Yumbaseurl=file:////opt/packages/gpgcheck=0enabled=1 客户端安装软件：yum install python-openstackclientUbuntu系统 使用apt命令安装软件时，下载的* .deb包缓存在/var/cache/apt/archives/； 创建目录 用于存放特定软件所需的软件包； mkdir -p /opt/packages 下载软件包 清除旧的缓存： rm -f /var/cache/apt/archives/* .deb 下载软件包： apt install -d -y –force-yes PACKAGE-NAME 拷贝软件包 cp /var/cache/apt/archives/* .deb /opt/packages/ 生成Packages.gz包Packages.gz中包含软件包信息以及其依赖关系信息； 安装dpkg-dev以便使用dpkg-scanpackages命令生成Packages.gz文件； apt install -y dpkg-dev 单机版本地源 此处忽略一切的警告(warning)； cd /opt/dpkg-scanpackages packages/ /dev/null | gzip &gt; /opt/packages/Packages.gz -r 制作成压缩包，便于网络传输； cd /opt/tar -zcf packages.tgz packages/ 将压缩包拷贝到目标主机； 解压压缩包并配置软件源(目标主机)： tar -zxf packages.tgz -C /optecho ‘deb file:///opt/ packages/‘ &gt; /etc/apt/sources.list 临时的Web共享本地源 此处忽略一切的警告(warning)； cd /opt/packages/dpkg-scanpackages . /dev/null | gzip &gt; /opt/packages/Packages.gz -r 使用Python自带的SimpleHTTPServer会在当前目录启动一个简易的Web服务器； cd /opt/packages/python -m SimpleHTTPServer PORT 配置软件源(目标主机):echo ‘deb http://IP:PORT/ /‘ &gt; /etc/apt/sources.list 搭建Web服务共享本地源 搭建主流的Web服务：Apache服务或Nginx服务； 配置Web服务，根目录指向/opt/packages/； 配置软件源(目标主机):echo ‘deb http://IP:PORT/ /‘ &gt; /etc/apt/sources.list 安装软件(目标主机)apt updateapt install -y –force-yes PACKAGE-NAME]]></content>
      <categories>
        <category>本地源</category>
      </categories>
      <tags>
        <tag>本地源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS手动优化]]></title>
    <url>%2F2017%2F07%2F11%2FHDFS%E6%89%8B%E5%8A%A8%E4%BC%98%E5%8C%96%20%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑一 Linux文件系统参数调整（1） noatime 和 nodiratime属性文件挂载时设置这两个属性可以明显提高性能。。默认情况下，Linux ext2/ext3 文件系统在文件被访问、创建、修改时会记录下文件的时间戳，比如：文件创建时间、最近一次修改时间和最近一次访问时间。如果系统运行时要访问大量文件，关闭这些操作，可提升文件系统的性能。Linux 提供了 noatime 这个参数来禁止记录最近一次访问时间戳。（2） readahead buffer调整linux文件系统中预读缓冲区地大小，可以明显提高顺序读文件的性能。默认buffer大小为256 sectors，可以增大为1024或者2408 sectors（注意，并不是越大越好）。可使用blockdev命令进行调整。（3） 避免RAID和LVM操作避免在TaskTracker和DataNode的机器上执行RAID和LVM操作，这通常会降低性能。3.2.2 Hadoop通用参数调整（1） dfs.namenode.handler.count或mapred.job.tracker.handler.countamenode或者jobtracker中用于处理RPC的线程数，默认是10，较大集群，可调大些，比如64。（2） dfs.datanode.handler.countdatanode上用于处理RPC的线程数。默认为3，较大集群，可适当调大些，比如8。需要注意的是，每添加一个线程，需要的内存增加。（3） tasktracker.http.threadHTTP server上的线程数。运行在每个TaskTracker上，用于处理map task输出。大集群，可以将其设为40~50。 二 HDFS相关配置（1） dfs.replicatio文件副本数，通常设为3，不推荐修改。（2） dfs.block.sizeHDFS中数据block大小，默认为64M，对于较大集群，可设为128MB或者256MB。（也可以通过参数mapred.min.split.size配置）（3） mapred.local.dir和dfs.data.dir这两个参数mapred.local.dir和dfs.data.dir 配置的值应当是分布在各个磁盘上目录，这样可以充分利用节点的IO读写能力。运行 Linux sysstat包下的iostat -dx 5命令可以让每个磁盘都显示它的利用率。3.2.4 map/reduce 相关配置（1） {map/reduce}.tasks.maximum同时运行在TaskTracker上的最大map/reduce task数，一般设为(core_per_node)/2~2*（cores_per_node）。（2） io.sort.factor当一个map task执行完之后，本地磁盘上(mapred.local.dir)有若干个spill文件，map task最后做的一件事就是执行merge sort，把这些spill文件合成一个文件（partition）。执行merge sort的时候，每次同时打开多少个spill文件由该参数决定。打开的文件越多，不一定merge sort就越快，所以要根据数据情况适当的调整。（3） mapred.child.java.opt设置JVM堆的最大可用内存，需从应用程序角度进行配置。 三 map task相关配置（1） io.sort.mMap task的输出结果和元数据在内存中所占的buffer总大小。默认为100M，对于大集群，可设为200M。当buffer达到一定阈值，会启动一个后台线程来对buffer的内容进行排序，然后写入本地磁盘(一个spill文件)。（2） io.sort.spill.percent这个值就是上述buffer的阈值，默认是0.8，即80%，当buffer中的数据达到这个阈值，后台线程会起来对buffer中已有的数据进行排序，然后写入磁盘。（3） io.sort.recordIo.sort.mb中分配给元数据的内存百分比，默认是0.05。这个需要根据应用程序进行调整。（4） mapred.compress.map.output/ Mapred.output.compre中间结果和最终结果是否要进行压缩，如果是，指定压缩方式（Mapred.compress.map.output.codec/ Mapred.output.compress.codec）。推荐使用LZO压缩。Intel内部测试表明，相比未压缩，使用LZO压缩的TeraSort作业运行时间减少60%，且明显快于Zlib压缩。3.2.6 reduce task相关配置（1） Mapred.reduce.parallelReduce shuffle阶段copier线程数。默认是5，对于较大集群，可调整为16~25。 四 通过hadoop的参数进行调优(1):设置合理的槽位数目(具体配置 mapred.tasktracker.map.tasks.maximum | mapred.tasktracker.reduce.tasks.maximum | mapreduce.tasktracker.map.tasks.maximum | mapreduce.tasktracker.reduce.tasks.maximum)(2):调整心跳间隔,对于300台以下的集群 可以把心跳设置成300毫秒(默认是3秒),mapreduce.jobtracker.hearbeat.interval.min | mapred.hearbeats.in.second | mapreduce.jobtracker.heartbeats.scaling.factor(3):启用外心跳,为了减少任务分配延迟(比如我们的任务心跳设置为10秒钟,当有一个任务挂掉了之后,他就不能马上通知jobtracker),所以hadoop引入了外心跳,外心跳是任务运行结束或者任务运行失败的时候触发的,能够在出现空闲资源时第一时间通知jobtracker,以便他能够迅速为空闲资源分配新的任务外心跳的配置参数是 mapreduce.tasktracker.outofband.hearbeat (4):磁盘快的配置. map task会把中间结果放到本地磁盘中,所以对于I/O密集的任务来说这部分数据会对本地磁盘造成很大的压力,我们可以配置多块可用磁盘,hadoop将采用轮训的方式将不同的maptask的中间结果写到磁盘上 maptask中间结果的配置参数是mapred.local.dir | mapreduce.cluster.local.dir (5):配置RPC Handler的数量,jobracker需要冰法处理来自各个tasktracker的RPC请求,我们可以根据集群规模和服务器并发处理的情况调整RPC Handler的数目,以使jobtracker的服务能力最佳 配置参数是 mapred.job.tracker.handler.count | mapreduce.jobtracker.handler.count (默认是10) (6):配置HTTP线程数. 在shuffle阶段,reduce task 通过http请求从各个tasktracker上读取map task中间结果,而每个tasktracker通过jetty server处理这些http请求,所以可以适当配置调整jetty server的工作线程数 配置参数是 tasktracker.http.thread | mapreduce.tasktracker.http.threads (默认是40) (7):如果我们在运行作业的过程中发现某些机器被频繁地添加到黑名单里面,我们可以把此功能关闭 (8):使用合理调度器 (9):使用合适的压缩算法,在hadoop里面支持的压缩格式是: gzip,zip,bzip2,LZO,Snappy,LZO和Snappy的呀搜比和压缩效率都很优秀,Snappy是谷歌的开源数据压缩哭,他已经内置在hadoop1.0之后的版本,LZO得自己去编译 (10):开启预读机制. 预读机制可以有效提高磁盘I/O的读性能,目前标准版的apache hadoop不支持此功能,但是在cdh中是支持的 配置参数是: mapred.tasktracker.shuffle.fadvise=true (是否启用shuffle预读取机制) mapred.tasktracker.shuffle.readahead.bytes=4MB (shuffle预读取缓冲区大小) mapreduce.ifile.readahead = true (是否启用ifile预读取机制) mapreduce.ifile.readahead.bytes = 4MB (IFile预读取缓冲区大小) (11):启用推测执行机制 (12):map task调优: 合理调整io.sort.record.percent值,可减少中间文件数据,提高任务执行效率. (map task的输出结果将被暂时存放到一个环形缓冲区中,这个缓冲区的大小由参数”io.sort.mb”指定,单位MB,默认是100MB, 该缓冲区主要由两部分组成,索引和实际数据,默认情况下,索引占整个buffer的比例为io.sort.record.percent,默认是5%, 剩余空间存放数据,仅当满足以下任意一个条件时才会触发一次flush,生成一个临时文件,索引或者数据空间使用率达到比例为 io.sort.spill.percent的80%) 所以具体调优参数如下: io.sort.mb | io.sort.record.percent | io.sort.spill.percent (13):reduce task调优 reduce task会启动多个拷贝线程从每个map task上读取相应的中间结果,参数是”mapred.reduce.parallel.copies”(默认是5) 原理是这样的–&gt;对于每个待拷贝的文件,如果文件小于一定的阀值A,则将其放入到内存中,否则已文件的形式存放到磁盘上, 如果内存中文件满足一定条件D,则会将这些数据写入磁盘中,而当磁盘上文件数目达到io.sort.factor(默认是10)时, 所以如果中间结果非常大,可以适当地调节这个参数的值 (14):跳过坏记录 看具体参数说明,=号后面是默认值 mapred.skip.attempts.to.start.skipping=2 当任务失败次数达到该值时,才会进入到skip mode,即启用跳过坏记录gongnneg mapred.skip.map.max,skip.records=0 用户可通过该参数设置最多运行跳过的记录数目 mapred.skip.reduce.max.skip.groups=0 用户可通过设置该参数设置Reduce Task最多允许跳过的记录数目 mapred.skip.out.dir =${mapred.output.dir}/logs/ 检测出得坏记录存放到目录里面(一般为HDFS路径),hadoop将坏记录保存起来以便于用户调试和跟踪 (15):使用JVM重用 : mapred.job.reuse.jvm.aum.tasks | mapreduce.job.jvm.num.tasks = -1]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java读取Excel表格中的数据]]></title>
    <url>%2F2017%2F05%2F20%2Fjava%E8%AF%BB%E5%8F%96Excel%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1.需求用java代码读取Excel.xls表格中的数据 2.java代码package com.test;import java.io.File;import jxl.*;public class ReadExcel{ public static void main(String[] args){ ing i; Sheet sheet; Workbook book; Cell cell1,cell2,cell3,cell4,cell5,cell6,cell7; try{ //Excel.xls为要读取的excel文件 book = Workbook.getWorkbook(new File(&quot;文件物理地址&quot;)))； //获取第一个工作表对象（excel中sheet的编号从0开始，0,1,2,3，.....） sheet = book.getSheet(0); //获取左上角的单元格 cell1 = sheet.getCell(0,0);(行，列) System.out.println(&quot;标题：&quot;+cell1.getContents()); i = 1; while(true){ //获取每一行单元格 cell1 = sheet.getCell(0,i); cell2 = sheet.getCell(1,i); cell3 = sheet.getCell(2,i); cell4 = sheet.getCell(3,i); cell5 = sheet.getCell(4,i); cell6 = sheet.getCell(5,i); cell7 = sheet.getCell(6,i); if(&quot;&quot;.equals(cell1.getContents()) == true) //如果读取的数据为空 break; System.out.println(cell1.getContents()+&quot;\t&quot;+cell2.getContents()+&quot;\t&quot;+cell3.getContents()+&quot;\t&quot;+cell4.getContents()+&quot;\t&quot;+cell5.getContents()+&quot;\t&quot;+cell6.getContents()+&quot;\t&quot;); i++; } book.close(); } catch(Exceprion e){ } } }]]></content>
      <categories>
        <category>study-Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase-写入方式]]></title>
    <url>%2F2017%2F05%2F08%2FHbase-5%E7%A7%8D%E5%86%99%E5%85%A5%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[只记录那二年踏过的坑HBase写入数据方式（参考：《HBase The Definitive Guide》） 1.直接使用HTable进行导入，代码如下： package hbase.curd;import java.io.IOException;import java.util.ArrayList;import java.util.List;impirt java.util.Random;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.util.Bytes; public class PutExample{ private HTable table = HTableUtil.getHTable(&quot;testtable&quot;); public static void main(String args[]) throws IOException{ PutExample pe = new PutExample(); pe.putRows(); } public void putRows(){ List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;; for(int i=0; i&lt;10; i++){ Put put = new Put(Bytes.toBytes(&quot;row_&quot;+i)); Rowdom random = new Random(); if(random.nextBoolean()){ put.add(Bytes.toBytes(&quot;colfam1&quot;), Bytes.toBytes(&quot;qual1&quot;), Bytes.toBytes(&quot;colfam1_qual1_value_&quot;+i)); } if(random.nextBoolean()){ put.add(Bytes.toBytes(&quot;colfam1&quot;), Bytes.toBytes(&quot;qual2&quot;), Bytes.toBytes(&quot;colfam1_qual1_value_&quot;+i)); } if(random.nextBoolean()){ put.add(Bytes.toBytes(&quot;colfam1&quot;), Bytes.toBytes(&quot;qual3&quot;), Bytes.toBytes(&quot;colfam1_qual1_value_&quot;+i)); } if(random.nextBoolean()){ put.add(Bytes.toBytes(&quot;colfam1&quot;), Bytes.toBytes(&quot;qual4&quot;), Bytes.toBytes(&quot;colfam1_qual1_value_&quot;+i)); } if(random.nextBoolean()){ put.add(Bytes.toBytes(&quot;colfam1&quot;), Bytes.toBytes(&quot;qual5&quot;), Bytes.toBytes(&quot;colfam1_qual1_value_&quot;+i)); } puts.add(put); } try{ table.put(puts); table.close(); }catch(Exception e){ e.printStackTrace(); return ; } System.out.println(&quot;done put rows&quot;); } } } 其中HTableUtil如下：package hbase.curd; import java.io.IOException; import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.util.Bytes; public class HTableUtil { private static HTable table; private static Configuration conf; static{ conf =HBaseConfiguration.create(); conf.set(&quot;mapred.job.tracker&quot;, &quot;hbase:9001&quot;); conf.set(&quot;fs.default.name&quot;, &quot;hbase:9000&quot;); conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hbase&quot;); try { table = new HTable(conf,&quot;testtable&quot;); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } } public static Configuration getConf(){ return conf; } public static HTable getHTable(String tablename){ if(table==null){ try { table= new HTable(conf,tablename); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } } return table; } public static byte[] gB(String name){ return Bytes.toBytes(name); } }]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础——String&StringBuffer]]></title>
    <url>%2F2017%2F03%2F14%2Fjava%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94String%26StringBuffer%2F</url>
    <content type="text"><![CDATA[创建字符串创建字符串最简单的方式如下:String greeting = “王小二客栈”;注意:String 类是不可改变的，所以你一旦创建了 String 对象，那它的值就无法改变了。如果需要对字符串做很多修改，那么应该选择使用 StringBuffer &amp; StringBuilder 类。 字符串长度用于获取有关对象的信息的方法称为访问器方法。String 类的一个访问器方法是 length() 方法，它返回字符串对象包含的字符数。 连接字符串String 类提供了连接两个字符串的方法：string1.concat(string2);更常用的是使用’+’操作符来连接字符串，如：“Hello,” + “ xiaoer” + “!” String 方法 char charAt(int index) 返回指定索引处的char值。 int compareTo（Object o） 把这个字符串和另一个对象比较。 int compareTo(String anotherString) 按字典顺序比较两个字符串。 int compareToIgnoreCase(String str) 按字典顺序比较两个字符串，不考虑大小写。 String concat(String str) 将指定字符串连接到此字符串的结尾。 boolean contentEquals(StringBuffer sb) 当且仅当字符串与指定的StringButter有相同顺序的字符时候返回真。 static String copyValueOf(char[] data) 返回指定数组中表示该字符序列的 String。 static String copyValueOf(char[] data, int offset, int count) 返回指定数组中表示该字符序列的 String。 boolean endsWith(String suffix) 测试此字符串是否以指定的后缀结束。 boolean equals(Object anObject) 将此字符串与指定的对象比较。 boolean equalsIgnoreCase(String anotherString) 将此 String 与另一个 String 比较，不考虑大小写。 byte[] getBytes() 使用平台的默认字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中。 byte[] getBytes(String charsetName) 使用指定的字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中。 void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin) 将字符从此字符串复制到目标字符数组。 int hashCode() 返回此字符串的哈希码。 int indexOf(int ch) 返回指定字符在此字符串中第一次出现处的索引。 int indexOf(int ch, int fromIndex) 返回在此字符串中第一次出现指定字符处的索引，从指定的索引开始搜索。 int indexOf(String str) 返回指定子字符串在此字符串中第一次出现处的索引。 int indexOf(String str, int fromIndex) 返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始。 String intern() 返回字符串对象的规范化表示形式。 int lastIndexOf(int ch) 返回指定字符在此字符串中最后一次出现处的索引。 int lastIndexOf(int ch, int fromIndex) 返回指定字符在此字符串中最后一次出现处的索引，从指定的索引处开始进行反向搜索。 int lastIndexOf(String str) 返回指定子字符串在此字符串中最右边出现处的索引。 int lastIndexOf(String str, int fromIndex) 返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索。 int length() 返回此字符串的长度。 boolean matches(String regex) 告知此字符串是否匹配给定的正则表达式。 boolean regionMatches(boolean ignoreCase, int toffset, String other, int ooffset, int len) 测试两个字符串区域是否相等。 boolean regionMatches(int toffset, String other, int ooffset, int len) 测试两个字符串区域是否相等。 String replace(char oldChar, char newChar) 返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的。 String replaceAll(String regex, String replacement) 使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串。 String[] split(String regex) 根据给定正则表达式的匹配拆分此字符串。 String[] split(String regex, int limit) 根据匹配给定的正则表达式来拆分此字符串。 boolean startsWith(String prefix) 测试此字符串是否以指定的前缀开始。 boolean startsWith(String prefix, int toffset) 测试此字符串从指定索引开始的子字符串是否以指定前缀开始。 CharSequence subSequence(int beginIndex, int endIndex) 返回一个新的字符序列，它是此序列的一个子序列。 String substring(int beginIndex) 返回一个新的字符串，它是此字符串的一个子字符串。 String substring(int beginIndex, int endIndex) 返回一个新字符串，它是此字符串的一个子字符串。 char[] toCharArray() 将此字符串转换为一个新的字符数组。 String toLowerCase() 使用默认语言环境的规则将此 String 中的所有字符都转换为小写。 String toLowerCase(Locale locale) 使用给定 Locale 的规则将此 String 中的所有字符都转换为小写。 String toString() 返回此对象本身（它已经是一个字符串！）。 String toUpperCase() 使用默认语言环境的规则将此 String 中的所有字符都转换为大写。 String toUpperCase(Locale locale) 使用给定 Locale 的规则将此 String 中的所有字符都转换为大写。 String trim() 返回字符串的副本，忽略前导空白和尾部空白。 static String valueOf(primitive data type x) 返回给定data type类型x参数的字符串表示形式。]]></content>
      <categories>
        <category>study-Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[王小二客栈——Java Number&Math]]></title>
    <url>%2F2017%2F03%2F13%2Fjava%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94Number%26Math%2F</url>
    <content type="text"><![CDATA[忆往昔峥嵘岁月，不堪入目 1.所有的包装类（Integer、Long、Byte、Double、Float、Short）都是抽象类Number的子类。 2.Java的Math包含了用于执行基本数学运算的属性和方法，如初等指数、对数、平方根和三角函数等。 Math的方法都被定义为static形式，通过Math类可以在主函数中直接调用。 下面列出Number &amp; Math类常用的一些方法： xxxValue() 将Number对象装换成xxx数据类型的值并返回。 compareTo（） 将number对象与参数比较。 equals() 判断number对象是否与参数相等。 valueOf() 返回一个Number对象制定的内置数据类型。 toString() 以字符串形式返回值。 parseInt() 将字符串解析为int类型。 abs() 返回参数的绝对值。 ceil（） 返回大于等于（&gt;=）给定参数的最小整数。 floor() 返回小雨等于（&lt;=）给定参数的最大整数。 rint（） 返回与参数最接近的整数，返回类型为double。 round() 他表示四舍五入。 min() 返回两个参数之间的最小值。 max() 返回两个参数之间的最大值。 exp（） 返回自然数底数e的参数次方。 log() 返回参数的自然数底数的对数值。 random（) 返回一个随机数。 Java Character 类 Character 类用于对单个字符进行操作。 Character 类在对象中包装一个基本类型 char 的值。转义序列转义序列 描述\t 在文中该处插入一个tab键\b 在文中该处插入一个后退键\n 在文中该处换行\r 在文中该处插入回车\f 在文中该处插入换页符\’ 在文中该处插入单引号\” 在文中该处插入双引号\ 在文中该处插入反斜杠 Character 方法isLetter() 是否是一个字母isDigit() 是否是一个数字字符isWhitespace() 是否为一个空格isUpperCase() 是否是大写字母isLowCase() 是否是小写字母toUpperCase() 制定之母的大写形式toLowCase() 制定之母的小写形式toString() 返回字符的字符串形式，字符串的长短仅为1。]]></content>
      <categories>
        <category>study-Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
</search>
